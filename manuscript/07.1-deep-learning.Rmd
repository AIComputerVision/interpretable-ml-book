## Neural Networks

This chapter looks at a few tools that are specifically for interpreting neural networks.

There are three main themes: How can we visualize the abstract features that a neural networks learned, how can we explain predictions and how can we simplify a network?
There are of course more techniques but these three receive a lot of attention and are important.

What makes neural networks different from other models and what is special about the interpretation methods?
First, the intrinsically learned features are a specialty of neural networks.
Also gradient based methods, which many interpretation methods make use of and are thus more efficient as there model-agnostic sibling methods.

See also Visual Interpretability for Deep Learning: a Survey

There is a lot going on.
A good portion of it is for understanding neural networks better, but are not day-to-day tools that will help build more interpretable models.


### Feature Visualization / Neural Activation

Convolutional Neural Networks (CNNs) are known to "learn" features in it's layers.
Starting from edge detectors to detectors for faces.
This chapter explains how we can interpret what each neuron learned to detect.
Visualizing the images that maximize a neuron activation or channel activation are called "Feature Visualization", because 
Chapter based on https://distill.pub/2017/feature-visualization/
This chapter will not bother with all the details. 
Go to the blog post if you need those.
I just summarize here and criticially discuss the method.

The information in a neural network can be seen to become more and more refined until it is the classification outcome.
In each step, more abstract concepts are learned.
From simple edge detectors, over more complex textures to dog noses up to the dog classification.

These images look intepretable, we may detect that one of them shows a certain concept, like a tennis ball. 
But then again, we interpret it to be a tennis ball, the same as we interpret art.
It could be something else, and actually you might see something different on the image as well.
A blend of concepts.
And let me remind you that we only look at the image that maximizes the activation.
If we go the other direction and minimize activation, a completely different blend of concepts might occur.
There can be multiple neurons or layers that respond (activate) for the same patterns or images.

Feature attribution might also be called saliency maps.

This chapter assumes knowledge about CNNs, so I only say two sentences about CNNs here:
Convolutional Neural Networks have an architecture where first convolutional and pooling layers are connected, then fully connected layers.
The inputs are pixels.
While an image can be understood by a human, each single pixel does not hold much meaning.
By passing the pixels through the layers, the trained neural networks neurons get activated, based on the content of the picture.
The idea is to find for each neuron an image that maximizes that neurons activation.

TODO, use here first image of https://distill.pub/2017/feature-visualization/

So understanding a neural network is turned into an optimization problem:
For an optimization problem, we have to choose the objective we want to optimize and the search space.
Both things define what we interpret.


Search space: 
- from training data
- artificially created images

Training data has the problem that elements on the image can be correlated and we don't get to see what the neural network is looking for.
If all the images that have a high activation of a neuron / channel show a dog and a tennis ball, we don't know whether the neural network looks at the dog, the tennis ball or maybe at both.


TODO: Draw the different things by marking them in CNN drawing.
Also you have more options for what you maximize
- Neuron activiation. Makes no sense because you have millions of neurons in architecturs like GoogleNet.
- A whole channel. Still a lot of elements. = convolutional filter
- A whole (convolutional) layer 
- A neuron in the fully connected layer (pre-softmax)
- The final class probability (e.g. dog)





```{r feature-viz-units, fig.cap="Features can be visualized for different units. The most common is the vissualization for the individual channels. Here, n is the index of the layer, z the channel index, k the class index, x and y the spatial positions. Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/.", out.width=800}
knitr::include_graphics("images/feature-visualization-units.png")
```

For multiple artificial neurons, we maximize the mean activation.

TODO: Write about how many of each thing to optimize there are in some representative architectures (AlexNet, GoogleNet, ...)
Goal: Shows how feasible each is for looking at all of those images.
Keep in mind that images can be unstable and it is recommend to look at multiple images per unit that is looked at.

See for example how many images you have to look at for Google Net here:
https://distill.pub/2017/feature-visualization/appendix/

To not get these images with repetitive patterns (which are more like adversarial examples than real life images), many researchers applied some regularization or some constraint.
Three options for regularization:
- Frequency penalization
- transformation robustness
- learned priors (e.g. GANs)

Google DeepDream takes an image, extracts the activation from one and adds it to the image.
Start with an image, feed it to a pretrained nn, choose a layer.
During the forward pass until the chosen layer, extract the gradients.
We want to maximize for the given image the activation of the chosen layer, so we add the gradient to the image.
A few techniques like Gaussian blurring are applied to improve the resulting images.


See here: https://distill.pub/2017/feature-visualization/
Appendix of post: https://distill.pub/2017/feature-visualization/appendix/

"Because InceptionV1 is a convolutional network, there is not just one activation vector per layer per image. This means that the same neurons are run on each patch of the previous layer. Thus, when we pass an entire image through the network, each neuron will be evaluated hundreds of times, once for each overlapping patch of the image. We can consider the vectors of how much each neuron fired for each patch separately. "
See also: https://distill.pub/2019/activation-atlas/

Positive and negative images are not in some natural or logical way opposites.
Networks learn a very alien view of the world.
This also restricts the usefulness for day to day ML.
But helps to build a general understanding.

Deep connection with adversarial examples, because there you try also to find an image that maximizes something (e.g. class probability of an adversarial class).
TODO: Link chapter
Adversarial examples also maximally activate units, in this case the a class probability.
Also it starts with some given image, for which the adversarial example should be produced, instead of some random noise image.
Also the optimization has an additional constraint: Don't change too much from the initial image.


CAVs, concept activation vectors.


List of notebooks:
https://github.com/tensorflow/lucid
#### Advantages
- Really great to learn this. makes clearer how nns work. For example it showed that CNNs look more on the texture, less on the shapes: I MAGE N ET - TRAINED CNN S ARE BIASED TOWARDS TEXTURE; INCREASING SHAPE BIAS IMPROVES  ACCURACY AND ROBUSTNESS
- Great Desktop Wallpaper or for T-Shirts
- Can be combined with feature attribution to get a better tool, see here: https://distill.pub/2018/building-blocks/.
Feature attributions are explained here: TODO: LINK CHAPTER

#### Disadvantages
- not practical for daily work
- difficult to get it to work.
- Maximum activation only small part of the space. 
  Negative activation exists as well, what about slightly postive activation? 
  We would have to describe the whole spectrum to describe that neuron.
- too many neurons
- what does it tell you in the end? only sense of understanding, but final prediction is still a difficult interaction between all those neurons, and these images only show the maximal activation, but neurons might only be have activated for some specific prediction, and it is unclear if that image really helps you then
- like understanding modern art, always trying to interpret what artist intended

More a tool for getting a general, better understanding of cnns, but not for daily job.



#### Software
- Blog Post with some code: https://github.com/fg91/visualizing-cnn-feature-maps/blob/master/filter_visualizer.ipynb
- Tensorflow: https://github.com/InFoCusp/tf_cnnvis
- Keras: https://github.com/jacobgil/keras-filter-visualization
- DeepViz Toolbox - https://github.com/yosinski/deep-visualization-toolbox
TODO: How does DeepDream fit in here?
- CAffee and with GANS https://github.com/Evolving-AI-Lab/synthesizing



### Feature Attribution

Feature attribution explains individual predictions by attributing each input feature by how much it changed the prediction (negative or positive).
Be it input pixels, tabular data or words.

Attribution is also sometimes called "relevance" or "contribution".

There are lots and lots of approaches to to this, all very similar.
We will only look at the general idea, and point out to some of the approaches.

Good overview: TOWARDS BETTER UNDERSTANDING OF GRADIENT - BASED ATTRIBUTION METHODS FOR DEEP NEURAL NETWORKS.
This paper also served to structure this article. 

Here we consider neural networks that output as prediction a vector of length $C$, which includes regression where $C=1$.
Output of DNN is called $S(x)=[S_1(x),\ldots,S_p(x)]$.
Formally all those methods take in input $x\in\mathbb{R}^p$ (can be image pixels, tabular data, words, ...) and outputs an explanation $R^c=[R_1^c,\ldots,R_p^c]$, one relevance value for each of the p input features.
The c indicicates the relevance for the c-th output.

In multi-class classification you have to decide for which classification to look at the relevance of the inputs.
This can be the correct class of that example, at least that's an interesting case to look at.
But also if DNN was wrong, it might be interesting to look at the input relevance for the wrong class and also for the relevance for the correct class for debugging.

The word attribution maps means that, for images, we visualize the pixels with red if they positively contributed, blue if negatively.
Of course, you are free to choose any color you like.
There are not rules.




All of the approaches return, as an explanation of an individual prediction a weighted sum:
TODO: Formula for attribution (linear sum). 


Main classification: 
- perturbation based methods: compute attribution of features by removing/masking/altering these features, and doing the prediction again. marginal effect of feature is estimated. depends on the  number of features that is perturbed together (for images you would perturb the nearby pixels together).
- gradient based methods
- 


Shapley Value only approximated usually, read more in LINK CHAPTER.

Some of those methods have the property of Completeness, meaning that the sum of the relevance values for all input features sum up to the difference between the prediction of the image/data point minus the prediction of a reference point (e.g. all grey image).
Integrated Gradient and SHAP have this property.

*List of approaches*

VISUALIZING DEEP NEURAL NETWORK DECISIONS: PREDICTION DIFFERENCE ANALYSIS:
- Based on Shapley Value for explaining individual predictions
- Relevance of feature is estimated by measuring how the prediction changes if feature is unknown, by simulating that the feature is unknown.
- what this paper does differently: instead of simulating 
- Implementation: https://github.com/lmzintgraf/DeepVis-PredDiff



- DeepSHAP (gradient-based)
  - version called DeepExplainer. There is a connection between SHAP and DeepLift
  - version called GradientExplainer. Connection between SHAP and Gradient Input algorithm.
- (epsilon) Layer-Wise Relevance Propagation (gradient-based)
- DeepLift (gradient-based)
- Deep Taylor Decomposition (gradient-based)
- Saliency Map (gradient-based) https://arxiv.org/abs/1312.6034. Saliency maps are a local method, because only gradient aroudn local is considered, but not the absolut global contributions. Most other methods are global. Probably Taylor decomp is also local.
- Gradient * Input (gradient-based, surprise!) https://arxiv.org/abs/1605.01713
- Occlusion (perturbation based) https://arxiv.org/abs/1311.2901
- Integrated Gradients (gradient based) https://arxiv.org/abs/1703.01365
- Shapley Value Sampling (perturbation based)
- LIME (perturbation based)
- Grad-CAM (gradient-based)
- Deconvolutional Network
- Guided Backpropagagion

Deep Lift and $\epsilon$-LRP can both be re-formulated as computing backpropagation for modified gradient function
Ancona et. al 2018.

These gradient based methods are all different for different activation functions, since when the chain rule for derivation is applied, they replace the non-linear activations with a function $g(x)$ is different in different methods.


TODO: Create example with different relevance methods.


Implementations:

https://github.com/oracle/Skater/blob/master/skater/core/local_interpretation/dnni/deep_interpreter.py

DeepLift Implementation https://github.com/kundajelab/deeplift
DeepVisualization ToolBox https://github.com/yosinski/deep-visualization-toolbox
Integrated Gradietns https://github.com/ankurtaly/Integrated-Gradients
SHAP https://github.com/slundberg/shap
#### Advantages 
- better than model-agnostic variants (e.g. shapley) since they rely on the gradient and can be computed faster.

#### Disadvantages



A lot of the methods are implemented in the DeepExplaiin Toolbox: https://github.com/marcoancona/DeepExplain

### TCAV

Useful to analyze a neural network with regard to specfic concepts, e.g. to check for gender bias.


### Rule Extraction and Model Extraction

Some methods, which are all only tested on single hidden layer networks.
- KT method: Extract for each neuron a rule
- Extract decision tree (Hinton)
- CRED (2001, Sato and Tsukimoto)
- Trepan

DeepRed 


LIME

Types of rule extraction: Decompositional, Pedagogical, Eclectic
Pedagogical is simply model-agnostic.

You can also look at different levels:
Approximate the rules of a single neuron
Approximate the whole network classification.

TODO: Link overview paper.


### Built-In Interpretability

based on Explaining Explanations: An Overview of
Interpretability of Machine Learning

- Attention mechanisms
- Disentangled representations
- Genereated Explanations

### Stuffs that's good to know about neural networks

Stuff that came out of interpretability research. 
Not really tools but a few things for better understanding models.
Better understand when and why they might fail.

- CNNs
 - learn mostly textures, shapes not so much
 - concepts can transform when learning, e.g. dog into waterfall
 - The thing that maximally activates a neuron or channel can be completely unrelated from the patterns that minimize it
 - Nets learn similar things. 
 - 

### Advantages
- Gradient allows many fast analysis
- For CNN nice pictures

### Disadvantages
- Many methods, difficult to get overview
- Feature Visualization: Not sure how useful it is
- 

