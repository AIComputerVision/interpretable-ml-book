## Neural Networks

This chapter looks at interpretation methods for neural networks:
These methods help visualizing the features and concepts learned by the network, explaining individual preditions and simplifying a network.

Neural networks have specific interpretation methods for two reasons:
Neural networks learn feature in their hidden layers and there is the gradient which can be utilized to implement interpretation methods that are more computationally efficient compared to their model-agnostic counterparts.

This chapter is based on a few great overview papers:
Visual Interpretability for Deep Learning: a Survey by Zhang et. al (2018) [^visual-survey]

The chapter assumes familiarity with deep learning, including convolutional neural networks.
We don't go in deep, there would just be too much ground to cover.
Rather, an overview so you know where to look.

### Feature Visualization

<!-- Background: Why feature visualization -->
Deep neural networks "learn" features from the raw input.
For example in computer vision before deep learning, you would have extracted features like edges or shapes with manually specifically designed extractor functions.
With (convolutional) neural networks, the image is passed into the network in its raw form, transformed multiple times through the convolution layers and fully connected layers and the network spits out the predictions.
When a CNN is trained, it learns new features in its layers.
In the first layers, it learns edge detectors.
In the later layers it distinguishes between different textures.
In the higher layers it learns more complex concepts like detecting dog snouts or tennis balls.

Naturally, one way of interpreting deep neural networks is by making explicit what features the neural network learned at each step.
For images this means visualizing what activates each unit in a neural network the most.
For tabular data or text data you can also describe which input would excite the network the most.
TODO: Search for papers or blog post that do this for tabular or for text.
For text analysis with recurrent neural networks, you might want to 

<!-- Feature Visualization explained-->
The approach of making the learned features explicit is called "Feature Visualization".
Feature visualization for a unit of the neural network is done by finding the input that activates the unit the most.
A unit here refers to either individual neurons, whole feature maps in convolutional neural networks, whole layers, or it could be even the final class probability in classification.
Chapter based on https://distill.pub/2017/feature-visualization/
Appendix of post: https://distill.pub/2017/feature-visualization/appendix/


```{r trippy, fig.cap="Optimized images for GoogleNet (channels mixed3a, mixed4c, mixed4d and mixed5a). Images are maximized for a random direction of the activations. Figure from Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/.", out.width=800}
knitr::include_graphics("images/trippy.png")
```
<!--Why not visualize the weights?-->
An alternative would be to simply visualize the weights, e.g. the weights that are learned in convolutional layer by CNNs.
This could also tell us whether it looks for edges or not.
But for more complex concepts it does not help us and it is more helpful to look at images that maximize some activation.
I just explained feature visualization via looking at the activation of units of the network for certain inputs.
The other option would be to visualize the weights directly.
Why don't we simply print out or visualize the learned weights?
Well, there are often millions of them, interacting in non-linear ways with each other.
It does not scale.
Also it is not on the same space as the feature input.
It is more difficult what the weights really mean and more intuitive to understand what input maximizes the activation of certain neurons in the neural network.


<!-- How to create feature visualizations? For activation map and with generated image--> 
Convolutional Neural Networks have an architecture where first convolutional and pooling layers are connected, then fully connected layers.
The inputs are pixels.
While an image can be understood by a human, each single pixel does not hold much meaning.
By passing the pixels through the layers, the trained neural networks neurons get activated, based on the content of the picture.
The idea is to find for each neuron an image that maximizes that neurons activation.


```{r activation-optim, fig.cap="Features can be visualized for different units. The most common is the vissualization for the individual channels. Here, n is the index of the layer, z the channel index, k the class index, x and y the spatial positions. Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/.", out.width=800}
knitr::include_graphics("images/activation-optim.png")
```

<!-- Choice for Visualization -->
We have more choice than to visualize the layer or work with random images.
So understanding a neural network is turned into an optimization problem:
For an optimization problem, we have to choose the objective we want to optimize and the search space.
Both things define what we interpret.

Search space: 
- from training data
- artificially created images

Training data has the problem that elements on the image can be correlated and we don't get to see what the neural network is looking for.
If all the images that have a high activation of a neuron / channel show a dog and a tennis ball, we don't know whether the neural network looks at the dog, the tennis ball or maybe at both.


Also you have more options for what you maximize
- Neuron activiation. Makes no sense because you have millions of neurons in architecturs like GoogleNet.
- A whole channel. Still a lot of elements. = convolutional filter
- A whole (convolutional) layer 
- A neuron in the fully connected layer (pre-softmax)
- The final class probability (e.g. dog)


For multiple artificial neurons, we maximize the mean activation or a random direction.

TODO: Write about how many of each thing to optimize there are in some representative architectures (AlexNet, GoogleNet, ...)

```{r feature-viz-units, fig.cap="Features can be visualized for different units. The most common is the vissualization for the individual channels. Here, n is the index of the layer, z the channel index, k the class index, x and y the spatial positions. Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/.", out.width=800}
knitr::include_graphics("images/feature-visualization-units.png")
```

<!-- How useful are they? -->
These images look intepretable, we may detect that one of them shows a certain concept, like a tennis ball. 
But then again, we interpret it to be a tennis ball, the same as we interpret art.
It could be something else, and actually you might see something different on the image as well.
A blend of concepts.
And let me remind you that we only look at the image that maximizes the activation.
If we go the other direction and minimize activation, a completely different blend of concepts might occur.
There can be multiple neurons or layers that respond (activate) for the same patterns or images.
See for example how many images you have to look at for Google Net here.
And these are only for optimizing the channels, not the individual neurons.
https://distill.pub/2017/feature-visualization/appendix/
Keep in mind that images can be unstable and it is recommend to look at multiple images per unit that is looked at.

To not get these images with repetitive patterns (which are more like adversarial examples than real life images), many researchers applied some regularization or some constraint.
Three options for regularization frequency penalization, transformation robustness, learned priors (e.g. GANs).
Or, of course, you only search through available, real images.

<!-- Google DeepDream --> 
Google DeepDream takes an image, extracts the activation from one and adds it to the image.
Start with an image, feed it to a pretrained nn, choose a layer.
During the forward pass until the chosen layer, extract the gradients.
We want to maximize for the given image the activation of the chosen layer, so we add the gradient to the image.
A few techniques like Gaussian blurring are applied to improve the resulting images.

<!-- Connection to Adversarial Examples -->
Deep connection with adversarial examples, because there you try also to find an image that maximizes something (e.g. class probability of an adversarial class).
TODO: Link chapter
Adversarial examples also maximally activate units, in this case the a class probability.
Also it starts with some given image, for which the adversarial example should be produced, instead of some random noise image.
Also the optimization has an additional constraint: Don't change too much from the initial image.

### Detecting Concepts 

Let's go deeper.
Visualizing features is nice and all, but can we maybe more concretely attach concepts to individual units of the neural network and individual predictions (?).

We will look at two approaches: Network Dissection and Concept Activation Vectors

Source for Network Dissection
http://netdissect.csail.mit.edu/

**TCAV: Testing with concept activation vectors.**

TCAV by Kim et al. (2019)[^tcav] explain a prediction by showing the importance of more high level concepts (e.g. texture, gender, color) for the prediction or classification.

You have to learn the concepts from data.
That means if you want to understand whether the network uses the concept of "female" for the classification of e.g. images, you have to provide some examples of "female" (could be images with women in it), and non-female (images without women in it).

You send all those images through the network 

Good thing is that TCAV does not require to change the network you are using, but you can use the network that you already have.




TCAV uses directional derivatives to quantify the importance of a concept for the classification or prediction.
The concept is defined by the user and must be defined via some positive and negative data examples.
For example for the image classification of a zebra, the concept might be stripes.
The concept is defined byselecting images of stripes and some randomly sampled images without stripes.

```{r tcav, fig.cap="Figure from TCAV paper, Been Kim et. al (2018) ", out.width=800}
knitr::include_graphics("images/tcav.png")
```


Code for TCAV: https://github.com/tensorflow/tcav

TODO: CONTINUE DESCRIBING TCAV

Good things about TCAV:
The concepts are not required to be known at training time.
Really any concept can be analyzed, as long as you find some positive and negative examples.

<!-- Feature Visualization for RNNs -->
For RNNs: https://medium.com/@plusepsilon/visualizations-of-recurrent-neural-networks-c18f07779d56



List of notebooks:
https://github.com/tensorflow/lucid
More a tool for getting a general, better understanding of cnns, but not for daily job.



**Word Embeddings**
As another way to visualize concepts that were learned are word embeddings.
An embedding maps a discrete feature (e.g. a word) to a m-dimensional vector.
A word embedding is the vector in some embedding space a word is mapped onto.
The embedding space is learned by the neural network.
The directions in that space often correlate to concepts.
This means that words with similar vectors have some similarity, e.g. cat and dog.
This also has the nice effect that we can do arithmetics in that space.
e.g.

$$embedding(king)-embedding(queen)=embedding(man)-embedding(woman)$$

The embeddings are high-dimensional vectors.
For visualization, they are often mapped to 2 Dimensions (e.g. with tSNE) TODO: CITE

What can you do with embeddings?
You can visualize the concepts that were learned.
Embedding let us analyze what the neural network learned.
For example, did it learn some kind of bias?
How do we get word embeddings?
Other use cases include to use these embeddings as feature transformations before the e.g. text is used in a machine learning model.

How are they created?
It's a mapping from categorical features (e.g. words) to some vectors.
They can be initialized with random weights and the embeddings are learned along with the thing you are trying to predict, e.g. with a recurrent neural network.
An alternativ is to use a pre-trained embedding like word2vec, GloVe or fasttext.
Those are trained over huge corpuses of text to predict words from their neighboring words.


**Software**

- Blog Post with some code: https://github.com/fg91/visualizing-cnn-feature-maps/blob/master/filter_visualizer.ipynb
- Tensorflow: https://github.com/InFoCusp/tf_cnnvis
- Keras: https://github.com/jacobgil/keras-filter-visualization
- DeepViz Toolbox - https://github.com/yosinski/deep-visualization-toolbox
TODO: How does DeepDream fit in here?
- CAffee and with GANS https://github.com/Evolving-AI-Lab/synthesizing



### Feature Attribution

Feature attribution explains individual predictions by attributing each input feature by how much it changed the prediction (negative or positive).
Be it input pixels, tabular data or words.

Attribution is also sometimes called "relevance" or "contribution".

Feature attribution might also be called saliency maps.
There are lots and lots of approaches to to this, all very similar.
We will only look at the general idea, and point out to some of the approaches.

Good overview: TOWARDS BETTER UNDERSTANDING OF GRADIENT - BASED ATTRIBUTION METHODS FOR DEEP NEURAL NETWORKS.
This paper also served to structure this article. 

Here we consider neural networks that output as prediction a vector of length $C$, which includes regression where $C=1$.
Output of DNN is called $S(x)=[S_1(x),\ldots,S_p(x)]$.
Formally all those methods take in input $x\in\mathbb{R}^p$ (can be image pixels, tabular data, words, ...) and outputs an explanation $R^c=[R_1^c,\ldots,R_p^c]$, one relevance value for each of the p input features.
The c indicicates the relevance for the c-th output.

In multi-class classification you have to decide for which classification to look at the relevance of the inputs.
This can be the correct class of that example, at least that's an interesting case to look at.
But also if DNN was wrong, it might be interesting to look at the input relevance for the wrong class and also for the relevance for the correct class for debugging.

The word attribution maps means that, for images, we visualize the pixels with red if they positively contributed, blue if negatively.
Of course, you are free to choose any color you like.
There are not rules.




All of the approaches return, as an explanation of an individual prediction a weighted sum:
TODO: Formula for attribution (linear sum). 


Main classification: 
- perturbation based methods: compute attribution of features by removing/masking/altering these features, and doing the prediction again. marginal effect of feature is estimated. depends on the  number of features that is perturbed together (for images you would perturb the nearby pixels together).
- gradient based methods
- 


Shapley Value only approximated usually, read more in LINK CHAPTER.

Some of those methods have the property of Completeness, meaning that the sum of the relevance values for all input features sum up to the difference between the prediction of the image/data point minus the prediction of a reference point (e.g. all grey image).
Integrated Gradient and SHAP have this property.

*List of approaches*

VISUALIZING DEEP NEURAL NETWORK DECISIONS: PREDICTION DIFFERENCE ANALYSIS:
- Based on Shapley Value for explaining individual predictions
- Relevance of feature is estimated by measuring how the prediction changes if feature is unknown, by simulating that the feature is unknown.
- what this paper does differently: instead of simulating 
- Implementation: https://github.com/lmzintgraf/DeepVis-PredDiff



- DeepSHAP (gradient-based)
  - version called DeepExplainer. There is a connection between SHAP and DeepLift
  - version called GradientExplainer. Connection between SHAP and Gradient Input algorithm.
- (epsilon) Layer-Wise Relevance Propagation (gradient-based)
- DeepLift (gradient-based)
- Deep Taylor Decomposition (gradient-based)
- Saliency Map (gradient-based) https://arxiv.org/abs/1312.6034. Saliency maps are a local method, because only gradient aroudn local is considered, but not the absolut global contributions. Most other methods are global. Probably Taylor decomp is also local.
- Gradient * Input (gradient-based, surprise!) https://arxiv.org/abs/1605.01713
- Occlusion (perturbation based) https://arxiv.org/abs/1311.2901
- Integrated Gradients (gradient based) https://arxiv.org/abs/1703.01365
- Shapley Value Sampling (perturbation based)
- LIME (perturbation based)
- Grad-CAM (gradient-based)
- Deconvolutional Network
- Guided Backpropagagion

Deep Lift and $\epsilon$-LRP can both be re-formulated as computing backpropagation for modified gradient function
Ancona et. al 2018.

These gradient based methods are all different for different activation functions, since when the chain rule for derivation is applied, they replace the non-linear activations with a function $g(x)$ is different in different methods.


TODO: Create example with different relevance methods.


Implementations:

https://github.com/oracle/Skater/blob/master/skater/core/local_interpretation/dnni/deep_interpreter.py

DeepLift Implementation https://github.com/kundajelab/deeplift
DeepVisualization ToolBox https://github.com/yosinski/deep-visualization-toolbox
Integrated Gradietns https://github.com/ankurtaly/Integrated-Gradients
SHAP https://github.com/slundberg/shap
#### Advantages 
- better than model-agnostic variants (e.g. shapley) since they rely on the gradient and can be computed faster.

#### Disadvantages



A lot of the methods are implemented in the DeepExplaiin Toolbox: https://github.com/marcoancona/DeepExplain

### TCAV

Useful to analyze a neural network with regard to specfic concepts, e.g. to check for gender bias.


### Rule Extraction and Model Extraction

Some methods, which are all only tested on single hidden layer networks.
- KT method: Extract for each neuron a rule
- Extract decision tree (Hinton)
- CRED (2001, Sato and Tsukimoto)
- Trepan

DeepRed 


LIME

Types of rule extraction: Decompositional, Pedagogical, Eclectic
Pedagogical is simply model-agnostic.

You can also look at different levels:
Approximate the rules of a single neuron
Approximate the whole network classification.

TODO: Link overview paper.


### Built-In Interpretability

based on Explaining Explanations: An Overview of
Interpretability of Machine Learning

- Attention mechanisms
- Disentangled representations
- Genereated Explanations

### Stuffs that's good to know about neural networks

Stuff that came out of interpretability research. 
Not really tools but a few things for better understanding models.
Better understand when and why they might fail.

- CNNs
 - learn mostly textures, shapes not so much
 - concepts can transform when learning, e.g. dog into waterfall
 - The thing that maximally activates a neuron or channel can be completely unrelated from the patterns that minimize it
 - Nets learn similar things. 
 - 




### Advantages
- Gradient allows many fast analysis
- For CNN nice pictures
- Really great to learn about feature visulization. makes clearer how nns work. For example it showed that CNNs look more on the texture, less on the shapes: I MAGE N ET - TRAINED CNN S ARE BIASED TOWARDS TEXTURE; INCREASING SHAPE BIAS IMPROVES  ACCURACY AND ROBUSTNESS
- Feature visualization are Great Desktop Wallpaper or for T-Shirts
- Can be combined with feature attribution to get a better tool, see here: https://distill.pub/2018/building-blocks/.
Feature attributions are explained here: TODO: LINK CHAPTER


### Disadvantages
- Many methods, difficult to get overview
- Feature Visualization: Not sure how useful it is
- feature visualization are not practical for daily work and difficult to get it to work.
- feature visualization: Maximum activation only small part of the space. 
  Negative activation exists as well, what about slightly postive activation? 
  We would have to describe the whole spectrum to describe that neuron.
- too many neurons to look at for feature visualization
- what does it tell you in the end? only sense of understanding, but final prediction is still a difficult interaction between all those neurons, and these images only show the maximal activation, but neurons might only be have activated for some specific prediction, and it is unclear if that image really helps you then
- feature visualizations are like understanding modern art, always trying to interpret what artist intended

 
[^visual-survey]: Zhang, Quan-shi, and Song-Chun Zhu. "Visual interpretability for deep learning: a survey." Frontiers of Information Technology & Electronic Engineering 19.1 (2018): 27-39.

[^TCAV]: Kim, Been, et al. "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)." arXiv preprint arXiv:1711.11279 (2017).

