```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```

## Shapley Values {#shapley}

Predictions can be explained by assuming that each feature is a 'player' in a game where the prediction is the payout.
The Shapley value - a method from coalitional game theory - tells us how to fairly distribute the 'payout' among the features.


### General Idea

Assume the following scenario: 

You trained a machine learning model to predict apartment prices.
For a certain apartment it predicts 300,000 € and you need to explain this prediction.
The apartment has a size of 50 m^2^, is located on the 2nd floor, with a park nearby and cats are forbidden:

```{r shapley-instance, fig.cap = "The predicted price for a 50 m^2^ apartment in the 2nd floor with a nearby park and cats forbidden is 300,000€. Our goal is to explain how each of these features values contributed to the prediction.", out.width=500}
knitr::include_graphics("images/shapley-instance.png")
```

The average prediction for all apartments is 310,000€.
How much did each feature value contribute to the prediction compared to the average prediction?


The answer is easy for linear regression models:
The effect of each feature is the weight of the feature times the feature value:
This works only because of the linearity of the model.
For more complex model we need a different solution.
For example [LIME](#lime) suggests local models to estimate effects.

A different solution comes from cooperative game theory:
The Shapley value, coined by Shapley (1953)[^shapley1953], is a method for assigning payouts to players depending on their contribution towards the total payout.
Players cooperate in a coalition and obtain a certain gain from that cooperation.

Players?
Game?
Payout?
What is the connection to machine learning prediction and interpretability?
The 'game' is the prediction task for a single instance of the dataset.
The 'gain' is the actual prediction for this instance minus the average prediction of all instances.
The 'players' are the feature values of the instance, which collaborate to receive the gain (= predict a certain value).
In our apartment example, the feature values 'park-allowed', 'cat-forbidden', 'area-50m^2^' and 'floor-2nd' worked together to achieve the prediction of 300,000€.
Our goal is to explain the difference of the actual prediction (300,000€) and the average prediction (310,000€): a difference of -10,000€.

The answer might be:
The 'park-nearby' contributed 30,000€; 'size-50m^2^' contributed 10,000€; 'floor-2nd' contributed 0€; 'cat-forbidden' contributed -50,000€.
The contributions add up to -10,000€: the final prediction minus the average predicted apartment price.

**How do we calculate the Shapley value for one feature?**

The Shapley value is the average marginal contribution of a feature value over all possible coalitions.
All clear now?

In the following figure, we assess the contribution of the 'cat-forbidden' feature value when added to a coalition of 'park-nearby', 'size-50m^2^'.
We simulate that only 'park-nearby', 'cat-forbidden' and 'size-50m^2^' are in a coalition by randomly drawing the value for the floor feature.
Then we predict the price of the apartment with this combination (310,000€).
In a second step we remove 'cat-forbidden' from the coalition by replacing it with a random value of the cat allowed/forbidden feature from the randomly drawn apartment.
In the example it was 'cat-allowed', but it could have been 'cat-forbidden' again.
We predict the apartment price for the coalition of 'park-nearby' and 'size-50m^2^' (320,000€).
The contribution of 'cat-forbidden' was 310,000€ - 320,000€ = -10.000€. This estimation depends on the sampled non-participating feature values and we get better estimates by repeating this procedure.
This figure shows the computation of the marginal contribution for only one coalition.
The Shapley value is the weighted average of marginal contributions over all coalitions.


```{r shapley-instance-intervened, fig.cap = "The change in prediction when we change 'cat-forbidden' to 'cat-allowed'.", out.width=500}
knitr::include_graphics("images/shapley-instance-intervention.png")
```

We repeat this computation for all possible coalitions.
The computation time increases exponentially with the number of features, so we have to sample from all possible coalitions.
The Shapley value is the average over all the marginal contributions.


The following figure shows all coalitions of feature values that are needed to assess the Shapley value for 'cat-forbidden'.
The first row shows the coalition without any feature values.
The 2nd, 3rd and 4th row show different coalitions - separated by '|' - with increasing coalition size.
All in all, the following coalitions are possible:
No feautures, 'park-allowed', 'size-50m^2^', 'floor-2nd', 'park-allowed'+'size-50m^2^', 'park-allowed'+'floor-2nd',  'size-50m^2^'+'floor-2nd', 'park-allowed'+'size-50m^2^'+'floor-2nd'.
For each of those coalitions we compute the predicted apartment price with and without the 'cat-forbidden' feature value and take the difference to get the marginal contribution.
The Shapley value is the (weighted) average of marginal contributions.
We replace the feature values of features that are not in a coalition with random feature values from the apartment dataset to get a prediction from the machine learning model.


```{r shapley-coalitions, fig.cap = "All 8 coalitions needed for computing the Shapley value of the 'cat-forbidden' feature value.", out.width=500}
knitr::include_graphics("images/shapley-coalitions.png")
```

When we repeat the Shapley value for all feature values, we get the complete distribution of the prediction (minus the average) among the feature values.


### Examples and Interpretation

The interpretation of the Shapley value $\phi_j$ for feature j and a single instance is:
the feature value $x_j$ contributed $\phi_j$ towards the prediction for instance i compared to the average prediction for the dataset.

The Shapley value works for both classification (if we deal with probabilities) and regression.

We use the Shapley value to analyze the predictions of a random forest model predicting [cervical cancer](#cervical):

```{r shapley-cervical-prepare}
data("cervical")
library("caret")
library("iml")


ntree = 30
cervical.x = cervical[names(cervical) != 'Biopsy']

model <- caret::train(cervical.x,
               cervical$Biopsy,
               method = 'rf', ntree=ntree, maximise = FALSE)
predictor = Predictor$new(model, class = "Cancer", data = cervical.x, type = "prob")

instance_indices = 326
x.interest = cervical.x[instance_indices,]

avg.prediction = mean(predict(model, type = 'prob')[,'Cancer'])
actual.prediction = predict(model, newdata = x.interest, type = 'prob')['Cancer']
diff.prediction = actual.prediction - avg.prediction
```


```{r shapley-cervical-plot, fig.cap = sprintf("Feature value contributions for woman %i in the cervical cancer dataset. With a prediction of %.2f, this woman's cancer probability is %.2f above the average prediction of %.2f. The feature value that increased the probability the most is the number of diagnosed STDs. The feature contributions sum up to the difference of actual and average prediction (%.2f).", instance_indices, actual.prediction,diff.prediction, avg.prediction, diff.prediction)}
shapley2 = Shapley$new(predictor, x.interest = x.interest, sample.size = 100)
plot(shapley2) + scale_y_continuous("Feature value contribution") +
  ggtitle(sprintf("Actual prediction: %.2f\nAverage prediction: %.2f\nDifference: %.2f", actual.prediction, avg.prediction, diff.prediction))
```


For the [bike rental dataset](#bike-data) we also train a random forest to predict the number of rented bikes for a day  given the weather conditions and calendric information.
The explanations created for the  random forest prediction of one specific day:

```{r shapley-bike-prepare}
data("bike")
ntree = 30
bike.train.x = bike[names(bike) != 'cnt']

model <- caret::train(bike.train.x,
               bike$cnt,
               method = 'rf', ntree=ntree, maximise = FALSE)
predictor = Predictor$new(model, data = bike.train.x)

instance_indices = c(295, 285)

avg.prediction = mean(predict(model))
actual.prediction = predict(model, newdata = bike.train.x[instance_indices[2],])
diff.prediction = actual.prediction - avg.prediction
x.interest = bike.train.x[instance_indices[2],]
```


```{r shapley-bike-plot, fig.cap = sprintf("Feature value contributions for instance %i. With a predicted %.0f rented bikes, this day is %.0f below the average prediction of %.0f. The feature values that had the most negative effects were the weather situation, humidity and the time trend (days since 2011). The temperature on that day had a positive effect compared to the average prediction. The feature contributions sum up to the difference of actual and average prediction (%.0f).", instance_indices[2], actual.prediction, diff.prediction, avg.prediction, diff.prediction)}
shapley2 = Shapley$new(predictor, x.interest = x.interest)
plot(shapley2) +  scale_y_continuous("Feature value contribution") +
  ggtitle(sprintf("Actual prediction: %.0f\nAverage prediction: %.0f\nDifference: %.0f", actual.prediction, avg.prediction, diff.prediction))
```


Be careful to interpret the Shapley value correctly:
The Shapley value is the average contribution of a feature value towards the prediction in different coalitions.
The Shapley value is NOT the difference in prediction when we would drop the feature from the model.



### The Shapley Value in Detail

This Section goes deeper into the definition and computation of the Shapley value for the curious reader.
Skip this part straight to 'Advantages and Disadvantages' if you are not interested in the technicalities.

We are interested in the effect each feature has on the prediction of a data point.
In a linear model it is easy to calculate the individual effects.
Here's how a linear model prediction looks like for one data instance:

$$\hat{f}(x)=\hat{f}(x_{1},\ldots,x_{p})=\beta_0+\beta_{1}x_{1}+\ldots+\beta_{p}x_{p}$$

where x is the instance for which we want to compute the feature effects.
Each $x_j$ is a feature value, with j = 1,...,p.
The $\beta_j$ are the weights corresponding to the feature values $x_j$.

The feature effect $\phi_j$ of $x_j$ on the prediction $\hat{f}(x)$ is:

$$\phi_j(\hat{f})=\beta_{j}x_j-E(\beta_{j}X_{j})=\beta_{j}x_j-\beta_{j}E(X_{j})$$

where $E(\beta_jX_{j})$ is the mean effect estimate for feature j.
The effect is the difference between the feature contribution to the equation minus the average contribution.
Nice!
Now we know how much each feature contributed towards the prediction.
If we sum up all the feature effects over all features for one instance, the result is:

$$\sum_{j=1}^{p}\phi_j(\hat{f})=\sum_{j=1}^p(\beta_{j}x_j-E(\beta_{j}X_{j}))=(\beta_0+\sum_{j=1}^p\beta_{j}x_j)-(\beta_0+\sum_{j=1}^{p}E(\beta_{j}X_{j}))=\hat{f}(x)-E(\hat{f}(X))$$

This is the predicted value for the data point x minus the average predicted value.
Feature effects $\phi_j$ can be negative.

Now, can we do the same for any type of model?
It would be great to have this as a model-agnostic tool.
Since we do not have the weights from a linear equation in other model types, we need a different solution.

Help comes from unexpected places: cooperative game theory.
The Shapley value is a solution for computing feature effects $\phi_j(\hat{f})$ for single predictions for any machine learning model $\hat{f}$.


#### The Shapley Value

The Shapley value is defined via a value function val over players in S.

The Shapley value of a feature value $x_j$ is its contribution to the payed outcome, weighted and summed over all possible feature value combinations:

$$\phi_j(val)=\sum_{S\subseteq\{x_{1},\ldots,x_{p}\}\setminus\{x_j\}}\frac{|S|!\left(p-|S|-1\right)!}{p!}\left(val\left(S\cup\{x_j\right)-val(S)\right)$$

where S is a subset of the features used in the model, x is the vector feature values of the instance to be explained and p the number of features.
$val_{x}(S)$ is the prediction for feature values in set S, marginalised over features not in S:

$$val_{x}(S)=\int\hat{f}(x_{1},\ldots,x_{p})d\mathbb{P}_{x\notin{}S}-E_X(\hat{f}(X))$$

You actually do multiple integrations, for each feature not in S.
One concrete example:
The machine learning model works on 4 features $\{x_{1},x_{2},x_{3},x_{4}\}$ and we evaluate $\hat{f}$ for the coalition S consisting of feature values $x_{1}$ and $x_{i}$:

$$val_{x}(S)=val_{x}(\{x_{1},x_{3}\})=\int_{\mathbb{R}}\int_{\mathbb{R}}\hat{f}(x_{1},X_{2},x_{3},X_{4})d\mathbb{P}_{X_2,X_4}-E_X(\hat{f}(X))$$

This looks similar to the linear model feature effects!

Don't get confused by the many uses of the word 'value':
The feature value is the numerical value of a feature and instance;
the Shapley value is the feature contribution towards the prediction;
the value function is the payout function given a certain coalition of players (feature values).

The Shapley value is the only attribution method that satisfies the properties **Efficiency**, **Symmetry**, **Dummy** and **Additivity**,  which all together can be seen as a definition of a fair payout.

**Efficiency**  
The feature effects have to sum up to the difference of prediction for x and the average.

$$\sum\nolimits_{j=1}^p\phi_j=\hat{f}(x)-E_X(\hat{f}(X))$$

**Symmetry**  
The contribution for two features should be the same if they contribute equally to all possible coalitions.
If 

$$val(S\cup\{x_j\})=val(S\cup\{x_k\})$$ 

for all 

$$S\subseteq\{x_{1},\ldots,x_{p}\}\setminus\{x_j,x_k\}$$
then 

$$\phi_j=\phi_{k}$$

**Dummy**  
A feature which does not change the predicted value - no matter to which coalition of feature values it is added - should have a Shapley value of 0.
If 

$$val(S\cup\{x_j\})=val(S)$$ 

for all 

$$S\subseteq\{x_{1},\ldots,x_{p}\}$$

then 

$$\phi_j=0$$

**Additivity**  
For a game with combined payouts val+val^+^ the respective Shapley values are 

$$\phi_j+\phi_j^{+}$$

Suppose you train a random forest, meaning that the prediction is an average of many decision trees.
The additivity property guarantees that, for a feature j, you can calculate the Shapley value for each tree indvidiually, average them and get feature j's Shapley value for the random forest.

#### Intiution of Shapley Value

An intuitive way to understand the Shapley value is  the following illustration:
The feature values enter a room in random order.
All feature values in the room participate in the game (= contribute to the prediction).
The Shapley value is the average marginal contribution of feature value by joining whatever features already entered the room before.


#### Estimating the Shapley Value

All possible coalitions (sets) of features have to be evaluated, with and without the feature of interest for calculating the exact Shapley value for one feature value.
For more than a few features, the exact solution to this problem becomes intractable, because the number of possible coalitions increases exponentially by adding more features.
Strumbelj et al. (2014)[^strumbelj2014] suggest an approximation with Monte-Carlo sampling:

$$\hat{\phi}_{j}=\frac{1}{M}\sum_{m=1}^M\left(\hat{f}(x^{m}_{+j})-\hat{f}(x^{m}_{-j})\right)$$

where $\hat{f}(x^{m}_{*+j})$ is the prediction for x, but with a random number of features values replaced by feature values from a random data point x, excluding the feature value for feature j $x_j$
The x-vector  $x^{m}_{-j}$ is almost identical to $x^{m}_{+j}$, but the value $x_j^{m}$ is also taken from the sampled x.
Each of those M new instances are kind of 'Frankensteins', pieced together from two instances.

**Approximate Shapley Estimation Algorithm for a Single Feature**:
Each contribution of a feature value $x_j$ to the difference $\hat{f}(x)-\mathbb{E}(\hat{f})$ for instance of interest x from data X.
The procedure has to be repeated for each of the features.

- Require: Number of iterations M, instance of interest x, feature j, data X, and machine learning model f
  - For all m = 1,...,M:
    - Draw random instance z from the data X
    - Choose a random permutation o of the feature values
    - Order instance x: x~o~ = (x~(1)~,...,x~(j)~,...,x~(p)~)
    - Order instance z: z~o~ = (z~(1)~,...,z~(j)~,...,z~(p)~)
    - Construct two new instancs
      - x~+j~ = (x~(1)~,..., x~(j-1)~, x~(j)~, z~(j+1)~, ...,z~(p)~)
      - x~-j~ = (x~(1)~,..., x~(j-1)~, z~(j)~, z~(j+1)~, ...,z~(p)~)
    - $\phi_j^{m}=\hat{f}(x_{+j})-\hat{f}(x_{-j})$
- Compute the Shapley value as the average: $\phi_j(x)=\frac{1}{M}\sum_{m=1}^M\phi_j^{m}$

First, select an instance of interest i, a feature j and the number of samples M.
For each sample, a random instance from the data is chosen and the order of the features is mixed.
From this instance, two new instances are created, by combining values from the instance of interest x and the sample.
The first instance x~+j~ is the instance of interest, but where all values in order before and including feature j are replaced by feature values from the sample.
The second instance x~-j~ is similar, but has all the values in order before, but excluding feature j, replaced by features from the sample.
The difference in prediction from the black box is computed: 

$$\phi_j^{m}=\hat{f}(x^m_{+j})-\hat{f}(x^m_{-j})$$

All these differences are averaged and result in 

$$\phi_j(x)=\frac{1}{M}\sum_{m=1}^M\phi_j^{m}$$

Averaging implicitly weighs samples by the probability distribution of X.


### Advantages

The difference between the prediction and the average **prediction is fairly distributed** among the features values of the instance - the Shapley efficiency property.
This property sets the Shapley value apart from other methods like [LIME](#lime).
LIME does not guarantee to perfectly distribute the effects.
It might make the Shapley value the only method to deliver a full explanation.
In situations that demand explainability by law - like EU's "right to explanations"
The Shapley value might be the only legally compliant method, because it is based on a solid theory and fairly distributes the effects.
I am not a lawyer, so this reflects only my intuition about the requirements.

The Shapley value allows **contrastive explanations**.
Instead of comparing a prediction with the average prediction of the whole dataset, you could compare it to a subset or even to a single data point.
This contrastiveness is also something that local models like LIME do not have.

The Shapley value is the only explanation method with a **solid theory**.
The axioms - efficiency, symmetry, dummy, additivity - give the explanation a reasonable foundation.
Methods like LIME assume linear behavior of the machine learning model locally but there is no theory why this should work or not.

It is mind-blowing to **explain a prediction as a game** played by the feature values.


### Disadvantages

The Shapley value needs **a lot of computation time**.
In 99.9% of the real world problems the approximate solution -- not the exact one -- is feasible.
An accurate computation of the Shapley value is potentially computational expensive, because there are $2^k$ possible coalitions of features and the "absence" of a feature has to be simulated by drawing random samples, which increases the variance for the estimate $\phi_j$.
The exponential number of the coalitions is handled by sampling coalitions and fixing the number of samples M.
Decreasing M reduces computation time, but increases the variance of $\phi_j$.
It is unclear how to choose a sensitive M.
It should be possible to choose M based on Chernoff bounds, but I have not seen any paper on doing this for Shapley Values for machine learning predictions.

The Shapley value **can be misinterpreted**.
The Shapley value $\phi_j$ of a feature j is not the difference in predicted value after the removal of feature j.
The interpretation of the Shapley value is rather:
Given the current set of feature values, the total contribution of feature value $x_j$ to the difference in the actual prediction and the mean prediction is $\phi_j$.

The Shapley value is the wrong explanation method if you seek sparse explanations (explanations that involve only a few features).
Explanations created with the Shapley value method **always use all the features**.
Humans prefer selective explanations, like LIME produces. 
Especially for explanations facing lay-persons, LIME might be the better choice for feature effects computation.
Another solution is [SHAP](https://github.com/slundberg/shap) introduced by Lundberg and Lee (2016)[^lundberg2016], which is based on the Shapley value, but can also produce explanations with few features.

The Shapley value returns a simple value per feature, and **not a prediction model** like LIME.
This means it cannot be used to make statements about changes in the prediction for changes in the input like:
"If I would earn 300 € more per year, my credit score should go up by 5 points."

Another disadvantage is that **you need access to the data** if you want to calculate the Shapley value for a new data instance.
It is not sufficient to access the prediction function because you need the data to replace parts of the instance of interest with values from randomly drawn instances from the data.
This can only be avoided if you have a generator method that can create new data instances that look like real data instances but are not actual instances from the training data.
Then you could use the generator instead of the real data.

Like many other permutation-based interpretation methods, the Shapley value method suffers from the **inclusion of unrealistic data instances** when features are correlated.
To simulate that a feature value is missing from a coalition, we marginalize the feature.
This is achieved by sampling values from the feature's marginal distribution.
This is fine as long as the features are independent.
When feature are dependent, then we could sample feature values that do not make sense for this instance.
But we use it to compute the feature's Shapley value.
There is not enough research on what this means for the Shapley values, or a suggestion how to fix it to the best of my knowledge.
One solution might be to only permute correlated features together and only get one mutual Shapley value for them.
Or the sampling procedure might have to be adjusted to account for dependence of features

### Software and Alternatives

Shapley values are implemented in the `iml` R package.

SHAP, an alternative formulation of the Shapley values  is implemented in the Python package `shap`.
SHAP turns the Shapley values method into an optimization problem and uses a special kernel function to measure proximity of data instances.
The results of SHAP are sparse (many Shapley values are estimated at zero), which is the biggest difference from the classic Shapley values.

Another approach is called breakDown, which is implemented in the `breakDown` R package[^breakdown].
BreakDown also shows the contributions of each feature to the prediction, but computes them step by step. 
Let us use the game analogy again:
We start with an empty team, add the feature values that would contribute the most to the prediction and iterate until all feature values are added.
How much each feature value contributes depends on the respective feature values that are already in the "team", which is the big drawback of the breakDown method.
It is faster than the Shapley method, and for models without interactions, the results are the same.


[^shapley1953]: Shapley, Lloyd S. 1953. “A Value for N-Person Games.” Contributions to the Theory of Games 2 (28): 307–17.

[^strumbelj2014]: Strumbelj, Erik, Igor Kononenko, Erik Štrumbelj, and Igor Kononenko. 2014. “Explaining prediction models and individual predictions with feature contributions.” Knowledge and Information Systems 41 (3): 647–65. doi:10.1007/s10115-013-0679-x.

[^lundberg2016]: Lundberg, Scott, and Su-In Lee. 2016. “An unexpected unity among methods for interpreting model predictions,” no. Nips: 1–6. http://arxiv.org/abs/1611.07478.

[^breakdown]: Staniak M, Biecek P (2018). "Explanations of Model Predictions with live and breakDown Packages." _ArXiv e-prints_. 1804.01955, <URL: https://arxiv.org/abs/1804.01955>.
