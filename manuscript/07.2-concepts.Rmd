## Detecting Concepts 

`r if(is.html){only.in.html}`


<!--intro -->
This chapter presents techniques for analyzing which concepts a neural network learned.
Concept here means an abstract idea, which is pre-defined by a human.
While  [feature visualization](#feauture-visualization) tries to detect features from neural network units, which might match a concept (e.g. dog snouts) but doesn't have to, concept detection starts with a concept and analyzes how the neural network handles this concept.


<!-- why are concepts interesting? -->
Feature visualization is more explorational:
What does the neural network detect?
But it does not help when we have more concret questions, like how important was the concept of dog snouts for the classification?


<!-- Approaches that we will look at -->
We will look at two approaches: Network Dissection and Concept Activation Vectors.
Both approaches require additional labeling of data, but in different ways.

### Network Dissection

What if we can automatically attach the concepts to the feature visualizations?
That's the question that network dissection by Bau & Zhou et al. (2017) [^dissect] tries to answer.
It works by having a broadly labeled image dataset, so we can tell for each pixel to which concept it belongs.
The individual pixels are labeled with concepts of objects, parts,
scenes, textures, materials, and colors.


What we learned in the [Feature Visualization chapter](#feature-visualization) was that the network already learns somewhat disentangled representations of concepts in the units or layers.
But we could not prove that one neuron really learned to e.g. detect stripes and not anything else, because these images were open for interpretation and not 100% clear cut.
If a network really were to learn disentangled representations of concepts, i.e. if we could definitely say neuron 13939 detects dog snouts, neuron 112 detects stripes with angle 30 degrees and neuron 131793 detects golden, round door handles, then we could simply read out each of those concepts.
So for a prediction we could simply read all those neurons and say how much each concept was activated for that image.
Then for a missclassification for exmample, we could use our human knowledge and cross-check with the learned concepts.
The chihuahua was missclassified as a muffin?
Ah! The concept for chocolate chips in dough was highly activated, along with the blue frosting neuron, because someone put this weird hat on the chihuahua before taking the photo.
We have to train our network with more ugly chihuhuas in costumes.

But let's get back to Network Dissection.
To really analyse the concepts, they propose to label the concepts on a pixel level in the training data.
This dataset is called 'Broden' which stands for broadly and densely labeled data.
The idea is that the framework aligns maps those concepts to the hidden units.


<!-- How does Network Dissection work? -->

1. Identify visual concepts which are labeled then in the data
1. Measure the activation of each neuron to these concepts
1. Quantify the alignment between activation and concept

A neuron might respond to more than on concept.
Since the researchers are only interested in completely disentangled, they propose to measure how much each neuron responds to a single concept.

The Broden dataset is segmented to the pixel level mostly.
All in all, there are following concept counts:
468 scences, 585 objects, 234 parts, 32 materials, 47 textures and 11 colors.

Every convolutional unit in the CNN is cross-checked with all concepts in broden whether they are good at segmenting the concepts.

```{r fig.cap = "For a given input image and a trained network (fixed weights), we forward propagate the image up to the target layer, upscale the activations to match the original image size and compare the maximum activations with the ground truth pixel-wise segmentation."}
knitr::include_graphics("images/dissection-network.png")
```


Algorithm:

1. For each image x  in Broden:
  1. Collect Activation map of each convolutional unit k: $A_k(x)$
  1. Compute distribution of of unit activations $\alpha_k$ over image
  1. The top quantile level $T_k$ is determined so that the probability that an activation is bigger than $T_k$ is 0.005: $P(\alpha_k>T_k)=0.005)$ for every location of the activation map. So the 0.5% quantile
  1. Scale up the possibly lower-resolution activation map $A_k(x)$ to the same resolution as the annotation mask $L_c(x)$ of the Broden dataset. We call the result $S_k(x)$.
  1. To reduce the activation map to the high activations, we binarize it: It's either on or off, depending whether a location exceeds the threshold. New mask is $M_k(x)=S_k(x)\leq{}T_k(x)$.

In the end we have a list with one max. activation masks per convolutional layer, $K$ masks to be precise.
We also have for each concept in Broden a mask.
For each combination of max-activation k and concept mask c, we simply compute how strongly they overlap with the intersection over union score:

$$IoU_{k,c}=\frac{\sum|M_k(x)\intersect{}L_c(x)|}{\sum|M_k(x)\union{}L_c(x)|}$$

where $|\cdot|$ is the cardinality of a set.

```{r, fig.cap = "The Intersection over Union (IoU) is computed by comparing the human ground truth annotation and the top activated pixels."}
knitr::include_graphics("images/dissection-dog-exemplary.jpg")
```



A unit can detect multiple concepts.
And multiple units can detect the same concept.
$IoU_{k,c}$ can be interpreted as the accuracy with which  unit k can detect the concept c.
The threshold chosen by the authors is $IoU_{k,c}>0.04$. 
When this treshold is exceeded, the unit was said to detect concept c.


Example of a unit detecting dog as concept.
It is a unit from "inception_4e unit 750". It has an $IoU=0.203$ for the dog concept.

```{r, fig.cap = "inception_4e unit 750 detecting dogs with $IoU=0.203$. The cut out area are the pixels with a high activation for this unit. It is a surprisingly coherent patch."}
knitr::include_graphics("images/dissection-dogs.jpeg")
```



#### Experiments

The authors tested different network architectures (e.g. VGG, and Inception-V3).
All models were trained from scratch on ImageNet and some other datasets (Places205, Places 365).
Additionally they had some self-supervised training tasks, like predicting context, video frame order or tracking, detecting object-centric alignment, colorizing images, predicting corsschannel and predicting ambient sound from frames.
For the self-supervised tasks, the AlexNet architecture was used.

Evaluation was done by humans, through Amazon Mechanical Turk.
For each unit, each rater was shown 15 images.
On these images, the patches with the highest activations as masks were shown.
Then they had to say whether or not this patch can be described with a given phrase, answered simply by yes/no.
If the answer was yes, the unit was deemed interpetable.
Interpretable units are the ones for which the raters agreed with the ground-truth interpretations.
Based on the set of interpetable units.
First measured human consistency, i.e. how much raters agreed with the ground truth (which is also from humans).
The it was measured how much the raters agreed with interpretability of network dissection methods.



The findings throught the experiments across architectures, tasks and training settings reveal very interesting insights about the interpretability of convolutional neural networks:


- Interpretability of CNNs is axis-dependent, i.e. using random directions of units (weighted averages) reduces interpretability.
  Interpretability is measured as the number of units where IoU is larger than the treshold.
  This was tested by rotating images and comparing the concepts.
- The methods detects, similar to [Feature Visualizations](#feature-visualization) that the network detects lower-level concepts at lower layers and higher-level concepts at higher layers.
- Different architectures have different interpretability.
  Also different tasks have different interpretability.
- Interpretability is independent of discriminative power, since the units can be transformed with orthogonal transformations whiel the discriminative power remains the same, but interpretability decreases. A unit is a channel of a whole convolutional layer (e.g. one of 256 channels in layer conv5 from AlexNet)
- The number of intepretable CNN units strongly depends on the task and the architecture
- Also dependent on training of course, only after some training iterations these concepts emerge.
- Even random initializations (training the model multiple times with different random seeds but all else equal) yields slight differences in number of interpretable units.
- Batch normalization seems to decrease interpretability
- 
- 

You can look at results here.
Example with Resnet:
http://netdissect.csail.mit.edu/dissect/resnet-152-torch-places365/

Note how many units there are detecting airplanes.

Source for Network Dissection
http://netdissect.csail.mit.edu/


TODO: Cover a bit about GAN Dissection https://gandissect.csail.mit.edu/



Advantages:

- Can detect concepts which are not used in your classification task. Because concepts are different data.
 
Disadvantages

- Concepts covered by many units
- Still many units to look at.

### TCAV: Testing with concept activation vectors.

But what about more implicit concepts?
Concepts for which we have no prior labeled data?

TCAV by Kim et al. (2019)[^tcav] explain a prediction by showing the importance of more high level concepts (e.g. texture, gender, color) for the prediction or classification.

You have to learn the concepts from data.
That means if you want to understand whether the network uses the concept of "female" for the classification of e.g. images, you have to provide some examples of "female" (could be images with women in it), and non-female (images without women in it).

You send all those images through the network 

Good thing is that TCAV does not require to change the network you are using, but you can use the network that you already have.




TCAV uses directional derivatives to quantify the importance of a concept for the classification or prediction.
The concept is defined by the user and must be defined via some positive and negative data examples.
For example for the image classification of a zebra, the concept might be stripes.
The concept is defined byselecting images of stripes and some randomly sampled images without stripes.

```{r tcav, fig.cap="Figure from TCAV paper, Been Kim et. al (2018) ", out.width=800}
knitr::include_graphics("images/tcav.png")
```


Code for TCAV: https://github.com/tensorflow/tcav

TODO: CONTINUE DESCRIBING TCAV

Good things about TCAV:
The concepts are not required to be known at training time.
Really any concept can be analyzed, as long as you find some positive and negative examples.

<!-- Feature Visualization for RNNs -->
For RNNs: https://medium.com/@plusepsilon/visualizations-of-recurrent-neural-networks-c18f07779d56
https://distill.pub/2019/memorization-in-rnns/
http://lstm.seas.harvard.edu/

TODO: Checkout RNNVis and LSTMVis

List of notebooks:
https://github.com/tensorflow/lucid
More a tool for getting a general, better understanding of cnns, but not for daily job.

### Word Embeddings

**Word Embeddings**
Word embeddings represent words as vectors which can be used to compute the similarity between words.
As another way to visualize concepts that were learned are word embeddings.
An embedding maps a discrete feature (e.g. a word) to a m-dimensional vector.
A word embedding is the vector in some embedding space a word is mapped onto.
The embedding space is learned by the neural network.
The directions in that space often correlate to concepts.
This means that words with similar vectors have some similarity, e.g. cat and dog.
This also has the nice effect that we can do arithmetics in that space.
e.g.

$$embedding(king)-embedding(queen)=embedding(man)-embedding(woman)$$

The embeddings are high-dimensional vectors.
For visualization, they are often mapped to 2 Dimensions (e.g. with tSNE) TODO: CITE

What can you do with embeddings?
You can visualize the concepts that were learned.
Embedding let us analyze what the neural network learned.
For example, did it learn some kind of bias?
How do we get word embeddings?
Other use cases include to use these embeddings as feature transformations before the e.g. text is used in a machine learning model.

How are they created?
It's a mapping from categorical features (e.g. words) to some vectors.
They can be initialized with random weights and the embeddings are learned along with the thing you are trying to predict, e.g. with a recurrent neural network.
An alternativ is to use a pre-trained embedding like word2vec, GloVe or fasttext.
Those are trained over huge corpuses of text to predict words from their neighboring words.


 - concepts can transform when learning, e.g. dog into waterfall

**Detecting Concepts During Training Time**

Towards Robust Interpretability with Self-Explaining Neural Networks


**Software**

- CAffee and with GANS https://github.com/Evolving-AI-Lab/synthesizing


### Other approaches for concepts

- Word embeddings https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
- 

[^TCAV]: Kim, Been, et al. "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)." arXiv preprint arXiv:1711.11279 (2017).

[^dissect]: Bau, David, et al. "Network dissection: Quantifying interpretability of deep visual representations." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.
