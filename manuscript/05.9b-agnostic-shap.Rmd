```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```

<!--{pagebreak}-->

## SHAP {#shap}

The SHAP (SHapley Additive exPlanations) method by Lundberg and Lee (2016)[^lundberg2016] explains individual predictions with the game-theoretic optimal [Shapley Values](#shapley).

There are two reasons SHAP got its own chapter instead of being a subchapter of the [Shapley value chapter](#shapley).
First, the authors propose a new kernel-based estimation approach inspired by [local surrogated models](#lime) and additionally introduce model-specific and efficient approaches for tree based models and neural networks.
Second, SHAP comes with many global explanation methods, based on aggregations of the Shapley values. 


I recommend reading the [chapter on Shapley values](#shapley) first, since SHAP computes Shapley values.
I also recommend reading the [chapter on Local Models, aka LIME](#lime), since SHAP estimation is inspired by LIME.

### Definition

The goal of SHAP is to explain an indiviudal prediction, by attributing each feature its contribution.
A new thing that SHAP brings to the table is that the Shapley value explanation is represented as an additive feautre attribution method, a linear model.
That view connects LIME and Shapley Values.
SHAP is represented as an additive feature attribution method that fits following explanation model:

$$g(x')=\phi_0+\sum_{j=1}^M\phi_jx_j'$$

where g is the explanation model, $z'\in\{0,1\}^M$ a data instance in a simplified feature space, M the number of simplified features and $\phi_i\in\mathbb{R}$ the feature attribution values.
The $\phi$'s are the indiviual feature contributions, the Shapley values.
A quick reminder: Shapley values are a fair distribution of the prediction among the features.
Simplified feature space means that we simply note whether a feature is present with a 1 and whether a feature is absent with a 0.
Sounds counterintuitive, since the x' will be a vector of all 1's
The representation as this linear formula is mainly a "trick" for the computation of the $\phi$'s, where we will also see that there can be 0's.
More on that later.

The formula becomes simpler then:

$$g=\phi_0+\sum_{j=1}^M\phi_j$$

This formula is then also the same as the Shapley Values as we know them. 


<!-- Desirable properties -->
There many solutions that result in a linear model for explaining a prediction (e.g. [LIME](#lime)), so the question is which one do we want.
As we know, the [Shapley Values](#shapley) fulfill desirable properties: Efficiency, Symmetry, Dummy and Additivity.
SHAP fullfils those as well, since it computes Shapley values.

When reading the paper, a few discrepancies occur between SHAP properties and Shapley properties.
SHAP reports the following three desirable properties:

1) Local accuracy

$$f(x)=g(x')=\phi_0+\sum_{j=1}^M\phi_jx_j'$$

If you define $\phi_0=E_X(\hat(f)(x))$ this is the same as in Shapley Efficiency property, just differently named.

2) Missingness

$$x_j'=0\Rightarrow\phi_j=0$$

Missingness says that a missing feature gets an attribution of zero.
Note that x_j'refers to a feature in the simplified feature space, where a 0 value represents absence of the feature value.
In the simplified feature space, all feature values $x_j'$ of the instance to be explained should be a vector with all 1's.
The presence of a 0 would mean that the feature value is already missing for the instance of interest.
Through the local accuracy property, if there were missing features, the corresponding Shapley values could take on any value without hurting the local accuracy property.
The Missingness property enforces that they get a zero attribution.
The authors call it a ["minor book-keeping property"](https://github.com/slundberg/shap/issues/175#issuecomment-407134438).
In practice this is only relevant for features that are constant.
There is no equivalent to this in Shapley values.

3)  Consistency 

Let $f_x(z')=f(h_x(z'))$ and $z_{\setminus{}i'}$ denote that $z_i'=0$, than for any two models f and f':

$$f_x'(z')-f_x'(z_{\setminus{}i}')\geq{}f_x(z')-f_x(z_{\setminus{}i}')$$

for all inputs $z'\in\{0,1\}^M$, then

$$\phi_i(f',x)\geq\phi_i(f,x)$$

Consistency says that if a model changes so that the contribution of a simplifed feature increases or stays the same (regardles of other inputs), the attribution should also increase or stay the same.

Lundberg showed that from Consistency the Shapley properties Linearity, Dummy and Symmetry follow.


<!-- What is the simplified feature space -->



<!-- h(x) and h_x(x')-->



### Estimation

Instead of the Shapley sampling strategy, it uses a linear regression approach, inspired by LIME.
Along with SHAP comes an interpretation toolbox, which has global methods for feature importance, interaction detection and feature effects included.
These methods basically aggregate the Shapley values and bring them in connection with the feature values to provide global insights into the model.
The tools would work as well for Shapley Values not computed with SHAP.

SHAP emphasizes the view of an explanation method as prediction model.
Shapley Values does not have this emphasis, but can be viewed as such.
Since Shapley Values sum up to the correct prediction (when samples are enough), the view as model is not necessary.


**SHAP theory**
New estimation method, inspired by LIME.
It is called Kernel SHAP.

Again, we can have feature x' instead of x.
Where x' is some simplified version of x, like super-pixels instead of pixels.
We assume that there is some function that maps x to x': $x'=h(x)$
Also, this new x' is binary, which says whether feature is there or not (??)
Then we have the other direction $x=h_x(x')$ which maps the 1's back to x values of the original input and the 0's to random replacements (e.g. pixel averages when superpixels were used).

The function h and h_x are a way of connecting LIME and Shapley Values.

<!-- Explaining h(x) -->

The function h() maps the features of instances to a simplified feature space $\mathbb{R}^p\rightarrow\{0,1\}^k$.
Applied to the original x, h(x) is a vector of 1's.
Applied to an instance z that is completely different from x, h(z) is a vector of 0's.
Applied to an instance z that is partially the same as x, h(z) contains 1's for the parts where x and z are identical and 0's otherwise.

How does the function h look like?
This depends on the type of data.
For images, h() maps the image data (raw pixels) to super pixels, i.e. patches of pixels that are similar.
Let's say we split the image that we want to explain in 6 super pixels (there are many methods that can do this).
The h-function then checks for each super pixel whether it has the same content as image x that we want to explain.
So h(x) for the original image will be a vector of 1's.
If we grey out one of the super pixels from the original image and draw a flower into another super pixel, then h will give us a vector where the two entries corresponding to the super pixels are set to 0.
The other direction $h_x(x')$ is a bit more complicated.
We have two cases per simplified feature: 1 and 0.
For the 1's in the simplified space, the mapping back is easy, since we simply use for that super pixel the part from the original image.
For the 0's, h_x has to map back to a version of the image where those super pixels are "missing". 
We have many options there: grey out the super pixel, fill with average color of neighbouring super pixels or let a Generative Adversarial Network paint it.

For text, we have many option how to map to the simplified feature space.
Again, we start with the original text that we want to explain.
This original text sets the h-function.
One way to create this simplified space is to map to word present vs. not present.
Again, the original text maps to a vector of 1's, which has the same length as the word count of the text.
For version where we remove some of the words, the corresponding entries are set to 0.
The inverse function h_x that maps from simplified feature space to text omits words with 0's.
Another option would be to replace it with random words or words that could make sense in the context but are not the original word.

For tabular data, the dimensionality of the simplified feature space is usually the same as the original feature space.
If h(z) yields a vector of 1's, the instance has all entries like x.
If h(z) yields a vector of 0's, all features are different from x.
The inverse function h_x maps 1's back to the original feature values of x, the 0's have to map to "missing feature".
Problem is that the model can't handle missing features, so one way is to randomly sample an instance z from the data distribution and fill all features with 0's in the simplified space with feature value from z.
This, again, has the problem that unlikely combinations of features when features are dependent.



Given the h_x() function, we can evaluate f(h_x(z')) to calculate the predictions with observed or not observed features.

For SHAP computation, 

<!-- Kernel -->
Also called KernelSHAP.
They define a kernel, which tells us the weights:

$$\pi_{x'}(z')=\frac{(M-1)}{(M{}choose|z'|)|z'|(M-|z'|)}$$


Simple weighted linear regression on simplified feature space (feature present, feature absent).
With L1 regression for sparse feature selection.

Loss function that is minimized:

$$L(f,g,\pi_{x'})=\sum_{z'\in{}Z}[f(h_x(z'))-g(z')]^2\pi_{x'}(z')$$

where $|z'|$ is the number of non-zero elements in z'.

$$g(x')=\phi_0+\sum_{i=1}^M\phi_ix_i'$$
That's basically the same formula as LIME uses, but with a different kernel.
And this kernel here forces the coefficient to approximate to the Shapley Values.

h is used to connect Shapley Value and LIME approach.
Basically, h is:
$$f(h_x(z'))=E[f(x)|x_S]$$

$h_x(x')=x$ even though x' has only 1's and 0's since it is tied to x (if all 1's, then it is x)


Neat trick:
The Shapley Value (and therefore the kernel) weights combinations very high, when either almost all (simplified) features are at zero or almost all (simplified) features are at 1.
That means we learn the most about the contribution of the j-th feature to the prediction when we look at the situation where we only get to know the j-th features value or the situation where we know all feature values and look how much the prediction changes.

The situation where we know halve of the features, we don't learn as much, at least for a specific combination of features to know.
All combinations of "knowing half of the features and adding feature j" summed up have the same weight as adding feature j to the empty set or adding feature j to get the full set.

The trick now is that we can enumerate these situations (also where 1 feature is known or where two features is known, etc.) and get a relative high weight already done, even though we are sampling.
Then we sample from the remaining combinations, with a readjusted weights.


y generated by inverse computation of features (and sampling).

TODO: Keep property stuff short since it really is just the Shapley properties.

**SHAP estimation**

**Unifying many methods**

The original SHAP paper talks a lot about the aspect of "unifying" lots of other prediction explanation approaches, like [LIME](#lime) or Deeplift.

This is a clever narrative, because instead of placing SHAP as one among many methods, the narrative places SHAP is the one explanation method to rule them all.
All approaches are additive feature attribution methods.
This means they explain a prediction as the weighted sum of the features (= a local linear model).

$$g(x)=\phi_+\sum_{i=1}^M\phi_ix_i'$$

Where g(x) is the predicted outcome of the explanation method and should be close to f(x).


Attribution methods like LIME do not satisfy local acurracy and consistency.



There are lots of analysis build on top.

Result of running shap for every instance of the data is a matrix of Shapley Values, rows are instances, features are columns, each entry a shap value.

We can aggregate this matrix in different ways together with the data.

We also have the feature matrix X, which we can link to their shapley values to understand the feature.

### TreeSHAP, DeepSHAP

KernelSHAP is just a heuristic and very expensive to compute.
There are version for tree-based models and ensembles of such, and also for gradient based methods like neural networks.
Those versions of SHAP are much faster to compute.
In the case of trees, the correlation / dependence problem can also be avoided.


**TreeSHAP**


Described by Lundberg et. al (2018)[^tree-shap].
TreeSHAP is a fast and exact method to compute SHAP for tree-based models and ensembles.

Reduces computation for the exact Shapley Values from $O(TL2^M)$ to $O(TLD^2)$ where T is the number of trees, L the maximum number of leaves in any tree and D the maximal depth of any tree.

TreeSHAP can handle the problem with correlated features, since it can use the tree structure to use the conditional instead of marginal distribution.

Goes like this: Follow the tree down the path, use instances in terminal node to estimate the conditional distribution.

Goal: Estimate $E(f(x)|x_S)$.
To estimate this, we can follow the tree.
But since not all feature values are given -- we only have subset S -- we have two types of decision nodes.
First possibility is that the node used a feature in S for the split.
Then we go down the path of that node where the feature value matches the decision.
Repeat recursively.
If a node uses a feature that is not in set S, we follow both paths, recursively.
Once we hit the terminal nodes, we take the prediction from that nodes, weight them by the number of instances they had.


We have T number of terminal nodes in a tree.
And we have the instance of interest x and the set of feature values S on which we condition.
If we would condition on all features -- i.e. S is the set of all features -- then we would use the prediction from the node in which x would fall.
If we would condition on no feature at all -- i.e. S is the empty set -- we would use the prediction from all terminal nodes.
This is the same as taking the overall average prediction.
For the inbetween case where S has some features, but not all, we ignore the predictions from the nodes that contradict the values from x_S.
From the remaining terminal nodes, we average the predictions, weighted by the node sizes (i.e. number of training samples in that node).
But the thing is, we need to do this procedure for every possible subset S of the feature values.
That is $\sum_{i=1}{p}\frac{(p-i)!i!}{i!}$.
Here, each summand is the set of all possible subsets S with the same cardinality (e.g. all possible subsets with 2 features).


This same thing, the authors proposed as algorithm in the paper.
But a faster version, that computes in polynomial time instead of exponential.


The Shapley Value is basically a sum of sum for trees:
The inner sum is for computation of E(f(x)|X_S), which we can get by going to the terminal nodes.
The outer sum is over all possible subsets S.

But in the end, this outer sum only weights the terminal nodes.
So we can be faster when we can compute the weights of the terminal nodes.
So we have to keep track how many of the subset land in which nodes.
It is a bit tricker since we need also to keep track of the combined weight of the subsets, since they have different weights.
For example, when the first split in a tree is on feature $x_3$, then all the subsets that contain feature $x_3$ will go to one node (to the one where x goes).
The subsets that do not contain feature value for $x_3$ go to both.
And so on.

TreeSHAP works with boosting frameworks XGBoost, LightGBM, CatBoost, and most tree-based scikit-learn models.


**DeepSHAP**

Approximates shap values for deep neural networks.




### Examples

Let us try SHAP out.
We trained a random forest classifier to predict [risk for cervical cancer](#cervical) with 100 trees.
We use SHAP to explain individual predictions.

Explaining a prediction can be visualized as forces that either push the prediction to be lower and higer.
The prediction starts from the baseline, the average of all predictions.
Some of the feature values of this instance have a positive effect on the predicted value:
They increase the prediction.
Some have a negative effect and decrease the prediction.
Some have a larger, some a smaller effect.
The effects, which we get through SHAP, can be visualized as "forces".
Each SHAP get represented by an arrow that represents a force that pushes the baseline prediction either up (positive SHAP) or down (negative SHAP).
The forces balance out at the actual prediction of that particular data instance.

The following figure shows the SHAP values visualized as forces for two women from the cervical cancer dataset:

```{r, fig.cap = "SHAP values for explaining the predicted cancer probabilities of two individuals. The first woman has an average risk. Risk increasing effects such as STDs are canceled out by decreasing effects such as age. The second woman has a very high risk. Age of 51 and 34 years of smoking are the main drivers of cancer risk increase. There are almost no risk lowering effects.", out.width = 800}
knitr::include_graphics("images/shap-explain-1.png")
knitr::include_graphics("images/shap-explain-2.png")
```

### SHAP Feature Importance

Mean of absolute shap values.

$$I_j=\sum_{i=1}^n{}|\phi_j^{(i)}|$$

The simply sort in descending order and plot.
Very summarized view, more interesting when we also show the direction of the effects.

SHAP Feature Importance is an alternative to the [Permutation Feature IMportance](#feature-importance) measure.
A big difference: Permutation Feature Importance is based on model performance, whereas SHAP is based on feature attribution.

```{r out.width=800}
knitr::include_graphics("images/shap-importance.png")
```


TODO: Add example

TODO: Example interpretation

TODO: Comparison with permutation feature importance plot


### Summary Plot

- Sort features by SHAP Importance
- y-axis: one feature
- x-axis: SHAP value
- color: feature value (low to high)
- points: instances from the data.

Shows

- Importance of feature through ranking
- the range and distribution of the impact: e.g. does a feature only positively affect prediction? what's the range?
- effect direction: are high feature values associated with higher contribution? or with negative?

For the feature dependence, the summary plot give a first hint, but to really see what is the effect of one feature, the SHAP dependence plots is a better choice.
```{r}
knitr::include_graphics("images/shap-importance-extended.png")
```


### Feature Dependence

Plot with feature value on x-axis and the corresponding shap values on y-axis.

Pairs of $\{(x_j^{(i)},\phi_j^{(i)})\}_{i=1}^n$

SHAP Dependence Plots are an alternative to [Partial Dependence Plots](#pdp) and [Accumulated Local Effects](#ale).


```{r out.width=800}
knitr::include_graphics("images/shap-dependence.png")
```

TODO: Example interpretation
TODO: Comparison plot with PDP and ALE

The plot can be improved by also visualizing feature interactions:


### Feature Interactions

Currently only implemented for TreeSHAP (?)
They are based on Shapley interaction index from game theory.

Result is per instance a matrix of size n.features x n.features.

Shapley Interaction Index is defined as:

$$\phi_{i,j}=\sum_{S\subseteq\setminus\{i,j\}}\frac{|S|!(M-|S|-2)!}{2(M-1)!}\delta_{ij}(S)$$


when $i\neq{}j$ and:

$$\delta_{ij}(S)=f_x(S\cup\{i,j\})-f_x(S\cup\{i\})-f_x(S\cup\{j\})+f_x(S)$$

Basically the interaction gain by using both features, when account for the individual feature effects.
That's in line with the definition of interaction in the [Feature Interaction chapter](#interaction).

In TreeSHAP, the tree structure is again leveraged for computing the exact interaction values in an efficient way.

TODO: Explain how feature interaction is computed.

```{r out.width=800}
knitr::include_graphics("images/shap-dependence-interaction.png")
```


TODO: Add example of Feature Interactions
TODO: Example interpretation

### Clustering SHAP values

Usually in clustering, you cluster on the features to get groups of instances from your data that are similar to each other and different between groups.

This has the problem that you might use irrelevant features in the clustering, which get equal weight.

SHAP clustering works by clustering on the SHAP values of each instance, instead of the feature values.
That means you cluster instances by how similar their explanations are.
Another advantage is that SHAP values are all on the same scale, as opposed to the original features that might stand for completely different things (how do you compare meter and number of children?).
SHAP values for all features are contributions to the prediction and therefore on the same scale.

You can use any type of clustering method of your choice.
For example in shap package hierachical agglomerative cluster was used.


The following plot shows all shapley values sorted by explanation similarity.


We take the force plots from before, for each instance.
We rotate them, so that they are vertical and put them side by side.

```{r, fig.cap="Stacked SHAP feature attributions clustered by explanation similarity. Each position on the x-axis is an instance from the data. Red SHAP values increase the prediction, blue ones decrease it.", out.width=800}
knitr::include_graphics("images/shap-clustering.png")
```


TODO: Example
TODO: Interpretation


### Advantages

- Based on Shapley, therefore theoretical foundation
- Similarity / Unification of other methods
- Coherent Ecosystem
- Works for multiclass
- link function, GLMs
- Automatic interaction detection 
- Fast model-specific implementations, but also model-agnostic
- comes with many visualizations

### Disadvantages

- As everything else, problem with correlated features, at least for KernelSahp
- KernelShap is slow
- gt


### Software

Python: shap, also integrated into xgboost (LINK) and LightGBM (LINK)
R:  [shapper](https://modeloriented.github.io/shapper/), integrated into [R xgboost packages](https://rdrr.io/cran/xgboost/man/xgb.plot.shap.html)`
Others?


[^lundberg2016]: Lundberg, Scott M., and Su-In Lee. "A unified approach to interpreting model predictions." Advances in Neural Information Processing Systems. 2017.

[^tree-shap]: Lundberg, Scott M., Gabriel G. Erion, and Su-In Lee. "Consistent individualized feature attribution for tree ensembles." arXiv preprint arXiv:1802.03888 (2018).
