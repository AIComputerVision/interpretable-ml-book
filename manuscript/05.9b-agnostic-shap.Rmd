```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```

<!--{pagebreak}-->

## SHAP (SHapley Additive exPlanations) {#shap}

`r if(is.html){only.in.html}`

The SHAP (SHapley Additive exPlanations) method by Lundberg and Lee (2016)[^lundberg2016] explains individual predictions.
SHAP is based on the game theoretically optimal [Shapley Values](#shapley).

There are two reasons why SHAP has its own chapter and is not a subchapter of [Shapley values](#shapley).
First, the SHAP authors propose KernelSHAP, a new kernel-based estimation approach for Shapley values inspired by [local surrogate models](#lime).
Additionally they propose TreeSHAP, an efficient estimation approach for tree-based models.

Second, SHAP is also an umbrella term for the many global explanation methods based on aggregations of Shapley values.
This chapter explains both the new estimation approaches and the global interpretation methods.

I recommend reading the [chapter on Shapley values](#shapley) first, as SHAP computes Shapley values.
I also recommend reading the [chapter on local models (LIME)](#lime), as SHAP estimation is inspired by LIME.

### Definition

The goal of SHAP is to explain the prediction of an instance x by computing the contribution of each feature to the prediction.
One innovation that SHAP brings to the table is that the Shapley value explanation is represented as an additive feauture attribution method, a linear model.
That view connects LIME and Shapley Values.
SHAP specifies the explanation as an additive feature attribution with the following linear model:

$$g(z')=\phi_0+\sum_{j=1}^M\phi_jz_j'$$

where g is the explanation model, $z'\in\{0,1\}^M$ is a data instance in a simplified feature space, M is the number of simplified features and $\phi_j\in\mathbb{R}$ is the feature attribution for a feature j, the Shapley values.
Simplified feature space means that we write a 1 if a feature is "present" and a 0 if it is absent.
This should sound familiar to you if you know about Shapley values.
To compute Shapley values, we simulate that features are not playing ("absent").
Therefore the mapping to simplified features bridges the gap between Shapley values and linear model estimation.
The representation as a linear model with simplified features is a trick for the computation of the $\phi$'s.
For the instance of interest x, the simplified version x' is a vector of all 1's in simplified feature space.
This means that for the instance for which we want the explanation, all features are "present".
The formula becomes simpler:

$$g(x')=\phi_0+\sum_{j=1}^M\phi_j$$

You can find this formula in similar notation in the [Shapley Value Chapter](#shapley).
More about this later the actual estimation comes later.
Let us first talk about the properties of the $\phi$'s before we go into the details of their estimation.

<!-- Desirable properties -->
From the [Shapley values chapter](#shapley) we know that Shapley values are the only solution that satisfies properties of efficiency, symmetry, dummy and additivity.
SHAP also satisfies these, since it computes Shapley values.
In the SHAP paper, you will find discrepancies between SHAP properties and Shapley properties.
SHAP describes the following three desirable properties:

**1) Local accuracy**

$$f(x)=g(x')=\phi_0+\sum_{j=1}^M\phi_jx_j'$$

If you define $\phi_0=E_X(\hat{f}(x))$ and set all $x_j'$ to 1, this is the Shapley efficiency property.
Only with a different name and using the simplified features.

$$f(x)=\phi_0+\sum_{j=1}^M\phi_jx_j'=E_X(\hat{f}(X))+\sum_{j=1}^M\phi_j$$

**2) Missingness**

$$x_j'=0\Rightarrow\phi_j=0$$

Missingness says that a missing feature gets an attribution of zero.
Note that x_j' refers to a feature in the simplified feature space, where a value of 0 represents the absence of the feature value.
In the simplified feature space, all feature values $x_j'$ of the instance to be explained should be a vector with all 1's.
The presence of a 0 would mean that the feature value is already for the instance of interest.
This property is not among the properties of the "normal" Shapley values.
So why do we need it for SHAP?
Lundberg calls it a ["minor book-keeping property"](https://github.com/slundberg/shap/issues/175#issuecomment-407134438).
A missing feature could -- in theory -- have an arbitrary Shapley value without hurting the local accuracy property, since it is multiplied with x_j'=0.
The Missingness property enforces that missing features get a Shapley value of 0. 
In practice this is only relevant for features that are constant.

**3)  Consistency**

Let $f_x(z')=f(h_x(z'))$ and $z_{\setminus{}j'}$ indicate that $z_j'=0$, than for any two models f and f':

$$f_x'(z')-f_x'(z_{\setminus{}j}')\geq{}f_x(z')-f_x(z_{\setminus{}j}')$$

for all inputs $z'\in\{0,1\}^M$, then

$$\phi_j(f',x)\geq\phi_j(f,x)$$

The consistency property says that if a model changes so that the marginal contribution of a simplified feature increases or stays the same (regardless of other features), the Shapley value also increases or stay the same.
From Consistency the Shapley properties Linearity, Dummy and Symmetry follow, as described in the Appendix of Lundberg and Lee [^shap].

### Estimation with KernelSHAP


TODO: CONTINUE HERE

<!-- The general Idea of linear model -->
Instead of the Shapley sampling strategy, SHAP uses weighted linear regression inspired by LIME.
The Shapley sampling approach estimates the Shapley values by sampling coalitions of features, replacing features that are not in this coalition with values from a random instance and computes the marginal contribution of adding feature j.

SHAP emphasizes the view of Shapley explanation as an additive attribution method that can be represented as a weighted linear model.
This linear model is not on the original features.
LIME for tabular data -- when numeric features are not binned -- works on original feature space.

<!-- What is the simplified feature space -->
The linear model for SHAP is on a simplified feature space.
A 1 for feature $x_k'$ means that a feature is present.
Converting a simplified feature with a 1 back to the original feature space means that the original feature takes on the value of the instance x that is to be explained.
A 0 means that a feature is not present.
Converting a simplified feature with a 0 back to the original feature space means that we have to fill in some value that simulates feature absence.
For tabular data, we sample another instance which donates values to simulate feature absence.
For image data, the original space are the individual pixels and the simplified feature space super-pixels (clusters of pixels).
We can grey out all pixels belonging to a super pixel to simulate the absence of that super pixel.

The mapping between original and simplified feature space is visualized in the following figure:


```{r shap-simplified-feature, fig.cap = "Function h() maps a data instance to a binary vector which indicates which features are equal to the instance to be explained. The inverse function maps the binary vector to a valid instance. Absent features (with a 0 in simplified feature space) are replaced by sampled values.", out.width=800}
knitr::include_graphics("images/shap-simplified-features.jpg")
```

Expressing this mathematically, we define a function that maps an instance z to z': $z'=h(z)$ with $h:\mathbb{R}^p\rightarrow\{0,1\}^k$.

This new x' is a binary vector, where each entry says whether the corresponding simplified feature is present or not.
Then we have the other direction $x=h_x(x')$ which maps the 1's back to x values of the original input and the 0's to random replacements (e.g. pixel averages when superpixels were used).
Applied to the original x, h(x) is a vector of 1's.
Applied to an instance z that is completely different from x, h(z) is a vector of 0's.
Applied to an instance z that is partially the same as x, h(z) contains 1's for the parts where x and z are identical and 0's otherwise.
The function h and h_x are a way of connecting LIME and Shapley Values.
LIME (except for non-binned numerical features) also uses this binarized feature space.
Given the h_x() function, we can evaluate f(h_x(z')) to calculate the predictions with observed or not observed features.
Basically, h is:

$$f(h_x(z'))=E[f(x)|x_S]$$

Here we can see a possible issue:
We have to define h_x in such a way that it samples from the distribution of the features conditional on features in set S.
But the KernelSHAP implementation samples from the marginal distribution, creating unlikely instances.

$h_x(x')=x$ even though x' has only 1's and 0's since it is tied to x (if all 1's, then it is x)

After this excourse into the simplified feature space, we can talk about the actual estimation.
SHAP is estimated with weighted linear regression.

A rough summary of the KernelSHAP estimation algorithm, which estimates the feature attributions for explaining instance x's prediction of model f:

- Choose number of samples M.
- Sample new instances $z_k',\quad{}k\in\{1,\ldots,M\}$ in the simplified feature space (0's represent feature absence, 1's feature presence).
- Get prediction for each z_k' by first converting them to the original feature space and then applying model f: $f(h_x(z_k'))$
- Compute weights for each z_k'
- Fit weighted (generalized) linear model
- Return the $\phi_k$, the coefficients from the linear models, which are the Shapley values

<!-- Kernel -->
This procedure is almost identical to LIME (see [chapter on local models](#lime)).
The big difference is the weighting of the samples.
LIME weights the instances by how close they are to the original instance.
An instance in the simplified feature space can be seen as a coalition.
All the features with 1's are in this coalition, the 0's are absent.
SHAP weights the sampled instances by the weight the coalition would get in the Shapley Value estimation:
Coalations with very few features and with almost all features get high weights, the lowest weight is for coalations with half of the players present, the other half absent.


To achieve this Shapley conform weight, the authors propose the SHAP kernel:

$$\pi_{x'}(z')=\frac{(p-1)}{(p{}choose|z'|)|z'|(p-|z'|)}$$

Here, p is the number of features in the simplified feature space, $|z'|$ the number of present features in instance z'.
Lundberg and Lee show that, with this Kernel, indeed the correct SHAP values are recovered.

TODO: Add image of ShAP kernel

```{r shap-kernel, fig.cap="", out.width=800}
```

If you would use this SHAP kernel with LIME plus the simplified features, LIME would also estimate Shapley values!


<!-- Sampling trick -->
From this Figure we can also see that we can cover a high percentage of the cumulative weight, when we include low and high number of present features.
The Shapley Value (and therefore the kernel) weights combinations very high, when either almost all (simplified) features are at zero or almost all (simplified) features are at 1.
That means we learn the most about the contribution of the j-th feature to the prediction when we look at the situation where we only get to know the j-th features value or the situation where we know all feature values and look how much the prediction changes.
We can enumerate these situations (also where 1 feature is known or where two features is known, etc.) and get a relative high weight already done, even though we are sampling.
Then we sample from the remaining combinations, with a readjusted weights.


<!-- Linear Model -->
Packing it all together, we estimate the Shapley values by optimizing this linear function:

$$L(f,g,\pi_{x'})=\sum_{z'\in{}Z}[f(h_x(z'))-g(z')]^2\pi_{x'}(z')$$

with g(x') being the additive attribution, the sum of Shapley values:

$$g(x')=\phi_0+\sum_{i=1}^M\phi_ix_i'$$

The loss optimization is a standard linear model.
Instead of the original features, we use the simplified feature space and the sampled instances.

Since we are in a classic linear regression setting, we can also make use of the tools that come with it.
For example, regularization.
By adding an L1 penalty to this term, we can generate sparse explanations.
(I am not so sure whether the resulting attribution values are still valid Shapley values)


### TreeSHAP

KernelSHAP is a heuristic and very expensive to compute.
Lundberg et. al (2018)[^tree-shap] proposed TreeSHAP, a fast implementation of SHAP for tree-based machine learning models such as decision trees, random forests and gradient boosted trees.
TreeSHAP is fast, computes exact Shapley values and avoids creating unrealistic instances for the estimation as KernelSHAP does when features are dependent.

How much faster is it?
For the exact Shapley values, it reduces the computational complexity from $O(TL2^M)$ to $O(TLD^2)$ where T is the number of trees, L the maximum number of leaves in any tree and D the maximal depth of any tree.

To explain an individual prediction with exact Shapley values, we have to estimate  $E(f(x)|x_S)$ for all possible feature value subsets S.

I will explain it for one tree, but it can be done for multiple trees and the results averaged.

For a given subset S and an instance x, we want to compute the expected prediction.
For features in set S, we know that they take on the values of the corresponding features in instance x that we want to explain.
For KernelSHAP we would sample the values for features not in S.
To get the expected prediction we have to start at the root of the tree and go down until we hit the terminal nodes.
Since not all feature values are given -- we only have subset S -- we have two distinguish two situations at each internal tree node.
First situation: the node used a feature from S for the split.
Then we go down the path of that node where the feature value matches the decision.
If a node uses a feature that is not in set S, we follow both paths.
We do this until we reach the terminal node in each path that we followed.
The mean of these terminal nodes, weighted by the number of instances per node, is the expected prediction for x given S.


Another way to think about it:
If we would condition on all features -- if S would be the set of all features -- then we would use the prediction from the node in which x would fall.
If we would condition on no feature at all -- if S would be the empty set -- we would use the weighted averaged prediction from all terminal nodes.
This is the same as taking the overall average prediction.
For the inbetween case where S contains some features, but not all, we ignore the predictions from the nodes that we can't reach, because their decisions path contradict values in x_S.
From the remaining terminal nodes, we average the predictions, weighted by the node sizes (i.e. number of training samples in that node).


The thing is, we need to do this procedure for every possible subset S of the feature values.
That is $\sum_{i=1}{p}\frac{(p-i)!i!}{i!}$.
Here, each summand is the set of all possible subsets S with the same cardinality (e.g. all possible subsets with 2 features).
To reduce the complexity, the authors proposed a faster algorithm in their paper that computes in polynomial time instead of exponential.
The basic idea is to push down all possible subsets S simultaneously.
Per node, we have to keep track of the number of subsets that go to the child nodes based on the split feature.
It is a bit trickier than that, since the subsets have different weights, so the algorithm also has to keep track of the overall weight of the subsets in each node.
For example, when the first split in a tree is on feature $x_3$, then all the subsets that contain feature $x_3$ will go to one node (to the one where x goes).
Subsets that do not contain feature $x_3$ go to both nodes with reduced weight.

TreeSHAP works with boosting frameworks XGBoost, LightGBM, CatBoost, and most tree-based scikit-learn models.

Next, we will look at SHAP explanations in action.

### Examples

I trained a random forest classifier with 100 trees to predict [risk for cervical cancer](#cervical).
We will use SHAP to explain individual predictions.
Since random forests are tree-based, we can use the fast TreeSHAP estimation method instead of the slower KernelSHAP method.

Since SHAP fits Shapley values, the interpretation is the same as presented in the [Shapley value chapter](#shapley).
But with the Python shap package comes a different visualization:
You can visualize feature attributions such as Shapley values as "forces".
Each feature value is a force that either increases or decreases the prediction.
The prediction starts from the baseline.
The baseline for Shapley values and therefore SHAP is the average of all predictions.
In the plot, each Shapley value is an arrow that pushes to increase the prediction (positive SHAP) or to decrease it (negative SHAP).
These forces balance out at the actual prediction of that particular data instance.

The following figure shows the SHAP values visualized as forces for two women from the cervical cancer dataset:

```{r, fig.cap = "SHAP values for explaining the predicted cancer probabilities of two individuals. The baseline -- average predicted probability -- is 0.066. The first woman has a low predicted risk of 0.06. Risk increasing effects such as STDs are canceled out by decreasing effects such as age. The second woman has a high predicted risk of 0.71. Age of 51 and 34 years of smoking increase predicted cancer risk. There are almost no effects that lower the cancer risk.", out.width = 800}
knitr::include_graphics("images/shap-explain-1.png")
knitr::include_graphics("images/shap-explain-2.png")
```

These were explanations for individual predictions.
Shapley values can be aggregated to global explanations.
When we run SHAP for every instance, we get a matrix of Shapley values.
The matrix has as many rows as data instances and as many columns as features.
We can aggregate and combine this matrix with the feature values to get further insights such as feature importance and feature effects.
The tools would work as well for Shapley Values not computed with SHAP.
We start with SHAP feature importance.

### SHAP Feature Importance

The idea behind SHAP feature importance is simple:
The features with the most impact on the model are the ones with the highest effects on the prediction.
For the importance we are not interested in the direction of the effect, so we take the absolute values.
Since we want to have a global importance, we measure the mean of absolute Shapley values over the data per feature:

$$I_j=\sum_{i=1}^n{}|\phi_j^{(i)}|$$

Once we have the importance for each feature, we sort them in descending order and plot them.
The following figure shows the feature importance for the random forest trained in the examples chapter to predict cervical cancer.

```{r fig.cap="SHAP feature importance measured as the mean absolute Shapley values. The number of years on hormonal contraceptives was the most important feature, changing the absolute cancer probability on average by 2.4 percentage points (0.024 on x-axis).", out.width=800}
knitr::include_graphics("images/shap-importance.png")
```

SHAP feature importance is an alternative to the [permutation feature importance](#feature-importance).
There is a big difference between both importance measures:
Permutation feature importance is based on drop in model performance.
SHAP is based on magnitude of feature attributions.


### Summary Plot

The feature importance plot is useful, but contains no information beyond the importances.
The summary plot combines feature importance with feature effects:
It shows the importance through ranking.
It shows range and distribution of Shapley values: e.g., does a feature only positively affect the prediction? What is the range of effects?
The summary shows also the effect direction, e.g. whether high feature values are associated with positive or negative effect on the predictions.

Each point on the summary plot is a Shapley value for one feature and one instance.
The position on the y-axis is set by the feature and on the x-axis by the Shapley value.
The color represents the value of the feature from low to high.
Overlapping points are jittered in y-axis direction, so we get a sense of the distribution of Shapley values per feature.

```{r fig.cap = "SHAP summary plot, which extends the SHAP feature importance plot. Low number of years on hormonal contraceptives decrease the predicted cancer risk, a large number of years increases the risk. Your regular reminder: All effects describe the behavior of the model, and are not necessarily causal in the real world.",  out.width=800}
knitr::include_graphics("images/shap-importance-extended.png")
```

In the summary plot we see first hints about the relationship between the value of a feature and the impact on the prediction.
But to see the exact shape of the relationship, we have to look at SHAP dependence plots.

### SHAP Feature Dependence

SHAP feature dependence is a simple plot:
Pick a feature.
For each instance from the data, plot a point with the feature value on the x-axis and corresponding Shapley value on the y-axis.
Done.
Mathematically, the plot contains this set of points: $\{(x_j^{(i)},\phi_j^{(i)})\}_{i=1}^n$

The following figure shows the SHAP feature dependence for years on hormonal contraceptives:

```{r fig.cap="SHAP dependence plot for years on hormonal contraceptives. Compared to 0 years, a few years lower the predicted probability and a high number of years increase the predicted cancer probability.", out.width=800}
knitr::include_graphics("images/shap-dependence.png")
```

SHAP Dependence Plots are an alternative to [Partial Dependence Plots](#pdp) and [Accumulated Local Effects](#ale).
Differences:
PDP and ALE are average effects, SHAP dependence also shows the variance on the y-axis.
Especially in case of interactions, the SHAP dependence plot will be much more dispersed.

The plot can be improved by also visualizing feature interactions.


### SHAP Feature Interactions

The interaction effect is the additional combined feature effect after accounting for the individual feature effects.

For Shapley values, there is a mathematical definition of so called Shapley interaction index, which also comes from game theory:

$$\phi_{i,j}=\sum_{S\subseteq\setminus\{i,j\}}\frac{|S|!(M-|S|-2)!}{2(M-1)!}\delta_{ij}(S)$$

when $i\neq{}j$ and:

$$\delta_{ij}(S)=f_x(S\cup\{i,j\})-f_x(S\cup\{i\})-f_x(S\cup\{j\})+f_x(S)$$

In words, the interaction is the marginal contribution of two feature i and j.
We average this again over all possible feature coalitions S, like in the standard Shapley value.
And we only count the interaction effect, i.e. we subtract the main effect of the features.
Basically the interaction gain by using both features, when account for the individual feature effects.
This is very similar to the definition of interaction in the [Feature Interaction chapter](#interaction).

When we compute SHAP interaction values for all features, we get one matrix per instance with dimensions p x p, where p is the number of features.

How can we use the interaction index?
For example to automatically color the SHAP feature dependence plot:

```{r fig.cap = "SHAP feature dependence plot with interaction visualization. Years on hormonal contraceptives interacts with STDs. In cases with close to 0 years, the occurence of a STD increases the predicted risk. For more years on contraceptives, the occurence of an STD decreases the predicted risk. Again, this is not a causal model. Effects might be due to confounding (e.g. STDs and lower cancer risk could be correlated with more doctor visits).", out.width=800}
knitr::include_graphics("images/shap-dependence-interaction.png")
```

### Clustering SHAP values

Shapley values can be used for clustering the data.
The goal of clustering is to find groups of similar instances.
Usually clustering is based on the features.
Features are often on different scales, height might be measured in meters, color intensity from 0 to 100 and some sensor output between -1 and 1.
The difficulty is to compute distances between instances with such different, non-comparable features.

SHAP clustering works by clustering on Shapley values of each instance, instead of the feature values.
This means you cluster instances by explanation similarity.
Another advantage is that SHAP values all have the same unit -- the unit of the prediction space.
The Shapley values for the p features describe an instance.
You can use any clustering method to cluster the instances.
For the following example, hierachical agglomerative cluster was used.
For the plot we create the force plot, which explains a prediction, for each instance.
We rotate them, so that they are vertical and put them side by side.
The clustering similarity sorts the instances.

```{r, fig.cap="Stacked SHAP feature attributions clustered by explanation similarity. Each position on the x-axis is an instance from the data. Red SHAP values increase the prediction, blue ones decrease it. One cluster stands out: To the right is a group with a high predicted cancer risk.", out.width=800}
knitr::include_graphics("images/shap-clustering.png")
```
### Advantages

Since SHAP computes Shapley values, all the advantages of Shapley values apply:
SHAP has a **solid theoretical foundation** in game theory.
The prediction is **fairly distributed** among the feature values.
We get **contrastive explanations** that compare the prediction with the average prediction.

SHAP **connects LIME and Shapley values**.
This is very useful for better understanding how explanations work.
This also helps to unify the field.
SHAP also unifies other methods such as DeepLift and LRP.

SHAP comes with a **fast model-specific implementation for tree-based models**.
I believe this was key to the popularity of SHAP.

Another strong argument in favor of SHAP is are the **global interpretation methods** built on top of the Shapley values.
The SHAP implementation comes with methods for importance, feature dependence, interactions, clustering and summary plots.
This has the advantage that local and global explanation have the same basis.
When you use e.g. LIME for local explanations and Partial Dependence Plot plus permutation feature importance for global explanations, you draw from very different techniques and can't directly relate both.

### Disadvantages

**KernelSHAP is slow**.
All the global interpretation methods require to compute Shapley values for a lot of instances and for all features.
This becomes quickly impractical when you cannot use the fast TreeSHAP, but when you have to rely on KernelSHAP

**KernelSHAP ignores feature dependence**.
This is the same issue as with many other model-agnostic methods which permute feature values.
By replacing feature values with values from random instances it is usually easier to randomly sample from the marginal distribution.
But when features are dependent, e.g. correlated, this can lead to unrealistic data instances.
The model might predict something unusual for this unseen data instances, but we still use them for computing Shapley values.
TreeSHAP fixes this problem by explicitly modeling the conditional distribution.

The disadvantages of Shapley values apply to SHAP as well:
Shapley values can be misinterpreted and access to data is needed to compute them for new data (except for TreeSHAP).

### Software

The authors implemented SHAP in the [shap Python package](https://github.com/slundberg/shap).
This package was used for all the examples in this chapter.

SHAP is integrated into the tree boosting frameworks [xgboost](https://github.com/dmlc/xgboost/tree/master/python-package) and [LightGBM](https://github.com/microsoft/LightGBM).

In R, there is the [shapper package](https://modeloriented.github.io/shapper/).
SHAP is also integrated into [the R xgboost package](https://rdrr.io/cran/xgboost/man/xgb.plot.shap.html)`

[^lundberg2016]: Lundberg, Scott M., and Su-In Lee. "A unified approach to interpreting model predictions." Advances in Neural Information Processing Systems. 2017.

[^tree-shap]: Lundberg, Scott M., Gabriel G. Erion, and Su-In Lee. "Consistent individualized feature attribution for tree ensembles." arXiv preprint arXiv:1802.03888 (2018).
