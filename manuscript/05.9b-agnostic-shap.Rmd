```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```

<!--{pagebreak}-->

## SHAP {#shap}

SHAP (SHapley Additive exPlanations) by Lundberg and Lee (2016)[^lundberg2016] is a sparse and faster version of Shapley values for explaining individual predictions.
SHAP is also a little universe of interpretability built on top, providing its own measures for feature importance, feature dependence and feature interactions, methods for images and text and model-specific implementations.


### Definition
I recommend reading the [chapter on Shapley Values](#shapley) first, since SHAP builds on Shapley values.
Also read the [chapter on Local Models, aka LIME](#lime).
Quick reminder: Shapley Values are a fair distribution of the prediction among the features.
Shapley Values are slow to compute and not sparse.

Assuming you know Shapley Value and LIME, SHAP can be summarized very quickly.

SHAP computes Shapley Values.
Instead of the Shapley sampling strategy, it uses a linear regression approach, inspired by LIME.
Along with SHAP comes an interpretation toolbox, which has global methods for feature importance, interaction detection and feature effects included.
These methods basically aggregate the Shapley values and bring them in connection with the feature values to provide global insights into the model.
The tools would work as well for Shapley Values not computed with SHAP.

SHAP emphasizes the view of an explanation method as prediction model.
Shapley Values does not have this emphasis, but can be viewed as such.
Since Shapley Values sum up to the correct prediction (when samples are enough), the view as model is not necessary.


**SHAP theory**

New estimation method, inspired by LIME.
It is called Kernel SHAP.

Again, we can have feature x' instead of x.
Where x' is some simplified version of x, like super-pixels instead of pixels.
We assume that there is some function that maps x to x': $x'=h(x)$
Also, this new x' is binary, which says whether feature is there or not (??)
Then we have the other direction $x=h_x(x')$ which maps the 1's back to x values of the original input and the 0's to random replacements (e.g. pixel averages when superpixels were used).


Also called KernelSHAP.
They define a kernel, which tells us the weights:

$$\pi_{x'}(z')=\frac{(M-1)}{(M{}choose|z'|)|z'|(M-|z'|)}$$


Simple weighted linear regression on simplified feature space (feature present, feature absent).
With L1 regression for sparse feature selection.

Loss function that is minimized:

$$L(f,g,\pi_{x'})=\sum_{z'\in{}Z}[f(h_x(z'))-g(z')]^2\pi_{x'}(z')$$

where $|z'|$ is the number of non-zero elements in z'.

$$g(x')=\phi_0+\sum_{i=1}^M\phi_ix_i'$$
That's basically the same formula as LIME uses, but with a different kernel.
And this kernel here forces the coefficient to approximate to the Shapley Values.

h is used to connect Shapley Value and LIME approach.
Basically, h is:
$$f(h_x(z'))=E[f(x)|x_S]$$

$h_x(x')=x$ even though x' has only 1's and 0's since it is tied to x (if all 1's, then it is x)


Neat trick:
The Shapley Value (and therefore the kernel) weights combinations very high, when either almost all (simplified) features are at zero or almost all (simplified) features are at 1.
That means we learn the most about the contribution of the j-th feature to the prediction when we look at the situation where we only get to know the j-th features value or the situation where we know all feature values and look how much the prediction changes.

The situation where we know halve of the features, we don't learn as much, at least for a specific combination of features to know.
All combinations of "knowing half of the features and adding feature j" summed up have the same weight as adding feature j to the empty set or adding feature j to get the full set.

The trick now is that we can enumerate these situations (also where 1 feature is known or where two features is known, etc.) and get a relative high weight already done, even though we are sampling.
Then we sample from the remaining combinations, with a readjusted weights.


y generated by inverse computation of features (and sampling).

TODO: Keep property stuff short since it really is just the Shapley properties.
Three desirable attributes for feature attribution methods:

- Local accuracy (same as efficiency in Shapley)

$$f(x)=g(x')=\phi_0+\sum_{i=1}^M\phi_ix_i'$$

- Consistency (implie Linearity, Dummy, Symmetry)

- Missingness. Which is somewhat confusing, because it is not explained in detail.
  It basically says, when a feature is missing, its corresponding Shapley Value is zero.
  In practice that's used for constant features.
  Not sure how it is needed, since its more or less the same as Dummy?


- Monotonicity <-> Linearity + Dummy
- Lundberg showed: Monotonicity <-> Linearity + Dummy + Symmetry.
  Can be found in supplementary material of the paper.


**SHAP estimation**

**Unifying many methods**

The original SHAP paper talks a lot about the aspect of "unifying" lots of other prediction explanation approaches, like [LIME](#lime) or Deeplift.

This is a clever narrative, because instead of placing SHAP as one among many methods, the narrative places SHAP is the one explanation method to rule them all.
All approaches are additive feature attribution methods.
This means they explain a prediction as the weighted sum of the features (= a local linear model).

$$g(x)=\phi_+\sum_{i=1}^M\phi_ix_i'$$

Where g(x) is the predicted outcome of the explanation method and should be close to f(x).


Attribution methods like LIME do not satisfy local acurracy and consistency.



There are lots of analysis build on top.

Result of running shap for every instance of the data is a matrix of Shapley Values, rows are instances, features are columns, each entry a shap value.

We can aggregate this matrix in different ways together with the data.

We also have the feature matrix X, which we can link to their shapley values to understand the feature.

### Examples

- Explain individual predictions

TODO: Explain force plot
TODO: Two examples of prediction explanations



### Feature Importance

Mean of absolute shap values.

$$I_j=\sum_{i=1}^n{}|\phi_j^{(i)}|$$


TODO: Add example


### Feature Dependence

Plot with feature value on x-axis and the corresponding shap values on y-axis.

Pairs of $\{(x_j^{(i)},\phi_j^{(i)})\}_{i=1}^n$

### Feature Interactions

Currently only implemented for TreeSHAP (?)

Result is per instance a matrix of size n.features x n.features.

TODO: Explain how feature interaction is computed.

TODO: Add example of Feature Interactions

### Clustering SHAP values

TODO: How are they clustered?
TODO: Example


### TreeSHAP, DeepSHAP


**TreeSHAP**

TreeSHAP can handle the problem with correlated features, since it can use the tree structure to use the conditional instead of marginal distribution.

Goes like this: Follow the tree down the path, use instances in terminal node to estimate the conditional distribution.


TreeSHAP works with boosting frameworks XGBoost, LightGBM, CatBoost, and most tree-based scikit-learn models.


**DeepSHAP**

Approximates shap values for deep neural networks.





### Advantages

- Based on Shapley, therefore theoretical foundation
- Similarity / Unification of other methods
- Coherent Ecosystem
- Works for multiclass
- link function, GLMs
- Automatic interaction detection 
- Fast model-specific implementations, but also model-agnostic
- comes with many visualizations

### Disadvantages

- As everything else, problem with correlated features, at least for KernelSahp
- KernelShap is slow
- gt


### Software

Python: shap
R:  [shapper](https://modeloriented.github.io/shapper/), integrated into [R xgboost packages](https://rdrr.io/cran/xgboost/man/xgb.plot.shap.html)`
Others?


[^lundberg2016]: Lundberg, Scott M., and Su-In Lee. "A unified approach to interpreting model predictions." Advances in Neural Information Processing Systems. 2017.


