```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```

<!--{pagebreak}-->

## SHAP (SHapley Additive exPlanations) {#shap}

The SHAP (SHapley Additive exPlanations) method by Lundberg and Lee (2016)[^lundberg2016] explains individual predictions with the game-theoretic optimal [Shapley Values](#shapley).

There are two reasons SHAP got its own chapter instead of being a subchapter of the [Shapley value chapter](#shapley).
First, the authors propose a new kernel-based estimation approach inspired by [local surrogated models](#lime) and additionally introduce model-specific and efficient approaches for tree based models and neural networks.
Second, SHAP comes with many global explanation methods, based on aggregations of the Shapley values. 


I recommend reading the [chapter on Shapley values](#shapley) first, since SHAP computes Shapley values.
I also recommend reading the [chapter on Local Models, aka LIME](#lime), since SHAP estimation is inspired by LIME.

### Definition

The goal of SHAP is to explain an indiviudal prediction, by attributing each feature its contribution.
A new thing that SHAP brings to the table is that the Shapley value explanation is represented as an additive feautre attribution method, a linear model.
That view connects LIME and Shapley Values.
SHAP is represented as an additive feature attribution method that fits following explanation model:

$$g(x')=\phi_0+\sum_{j=1}^M\phi_jx_j'$$

where g is the explanation model, $z'\in\{0,1\}^M$ a data instance in a simplified feature space, M the number of simplified features and $\phi_i\in\mathbb{R}$ the feature attribution values.
The $\phi$'s are the indiviual feature contributions, the Shapley values.
A quick reminder: Shapley values are a fair distribution of the prediction among the features.
Simplified feature space means that we simply note whether a feature is present with a 1 and whether a feature is absent with a 0.
Sounds counterintuitive, since the x' will be a vector of all 1's
The representation as this linear formula is mainly a "trick" for the computation of the $\phi$'s, where we will also see that there can be 0's.
More on that later.

The formula becomes simpler then:

$$g=\phi_0+\sum_{j=1}^M\phi_j$$

This formula is then also the same as the Shapley Values as we know them. 


<!-- Desirable properties -->
There many solutions that result in a linear model for explaining a prediction (e.g. [LIME](#lime)), so the question is which one do we want.
As we know, the [Shapley Values](#shapley) fulfill desirable properties: Efficiency, Symmetry, Dummy and Additivity.
SHAP fullfils those as well, since it computes Shapley values.

When reading the paper, a few discrepancies occur between SHAP properties and Shapley properties.
SHAP reports the following three desirable properties:

1) Local accuracy

$$f(x)=g(x')=\phi_0+\sum_{j=1}^M\phi_jx_j'$$

If you define $\phi_0=E_X(\hat(f)(x))$ this is the same as in Shapley Efficiency property, just differently named.

2) Missingness

$$x_j'=0\Rightarrow\phi_j=0$$

Missingness says that a missing feature gets an attribution of zero.
Note that x_j'refers to a feature in the simplified feature space, where a 0 value represents absence of the feature value.
In the simplified feature space, all feature values $x_j'$ of the instance to be explained should be a vector with all 1's.
The presence of a 0 would mean that the feature value is already missing for the instance of interest.
Through the local accuracy property, if there were missing features, the corresponding Shapley values could take on any value without hurting the local accuracy property.
The Missingness property enforces that they get a zero attribution.
The authors call it a ["minor book-keeping property"](https://github.com/slundberg/shap/issues/175#issuecomment-407134438).
In practice this is only relevant for features that are constant.
There is no equivalent to this in Shapley values.

3)  Consistency 

Let $f_x(z')=f(h_x(z'))$ and $z_{\setminus{}i'}$ denote that $z_i'=0$, than for any two models f and f':

$$f_x'(z')-f_x'(z_{\setminus{}i}')\geq{}f_x(z')-f_x(z_{\setminus{}i}')$$

for all inputs $z'\in\{0,1\}^M$, then

$$\phi_i(f',x)\geq\phi_i(f,x)$$

Consistency says that if a model changes so that the contribution of a simplifed feature increases or stays the same (regardles of other inputs), the attribution should also increase or stay the same.
Lundberg showed that from Consistency the Shapley properties Linearity, Dummy and Symmetry follow.




### Estimation with KernelSHAP

There is a model-agnostic version of SHAP, called KernelSHAP and two model-specific implementations.
We will start with KernelSHAP.

<!-- The general Idea of linear model -->
Instead of the Shapley sampling strategy, SHAP uses a weighted linear regression inspired by LIME.
The Shapley samping approach estimates the Shapley Values by sampling coalitions of features, replacing features that are not in this coalition with values from a random instance and computes the marginal contribution of adding feature j.

SHAP emphasizes the view of Shapley explanation as an additive attribution method that can be represented as a weighted linear model.
This linear model is not on the original features.
LIME for tabular data -- when numeric features are not binned -- works on original feature space.

<!-- What is the simplified feature space -->
The linear model for SHAP is on a simplified feature space.
A 1 for feature $x_k'$ means that a feature is present.
Converting a simplified feature with a 1 back to the original feature space means that the original feature takes on the value of the instance x that is to be explained.
A 0 means that a feature is not present.
Converting a simplified feature with a 0 back to the original feature space means that we have to fill in some value that simulates feature absence.
For tabular data, we sample another instance which donates values to simulate feature absence.
For image data, the original space are the individual pixels and the simplified feature space super-pixels (clusters of pixels).
We can grey out all pixels belonging to a super pixel to simulate the absence of that super pixel.

The mapping between original and simplified feature space is visualized in the following figure:



TODO: Add image of h, h_x

```{r shap-simplified-feature, fig.cap = "", out.width=800}
```

Expressing this mathematically, we define a function that maps an instance z to z': $z'=h(z)$ with $h:\mathbb{R}^p\rightarrow\{0,1\}^k$.

This new x' is a binary vector, where each entry says whether the corresponding simplified feature is present or not.
Then we have the other direction $x=h_x(x')$ which maps the 1's back to x values of the original input and the 0's to random replacements (e.g. pixel averages when superpixels were used).
Applied to the original x, h(x) is a vector of 1's.
Applied to an instance z that is completely different from x, h(z) is a vector of 0's.
Applied to an instance z that is partially the same as x, h(z) contains 1's for the parts where x and z are identical and 0's otherwise.
The function h and h_x are a way of connecting LIME and Shapley Values.
LIME (except for non-binned numerical features) also uses this binarized feature space.
Given the h_x() function, we can evaluate f(h_x(z')) to calculate the predictions with observed or not observed features.
Basically, h is:

$$f(h_x(z'))=E[f(x)|x_S]$$

Here we can see a possible issue:
We have to define h_x in such a way that it samples from the distribution of the features conditional on features in set S.
But the KernelSHAP implementation samples from the marginal distribution, creating unlikely instances.

$h_x(x')=x$ even though x' has only 1's and 0's since it is tied to x (if all 1's, then it is x)

After this excourse into the simplified feature space, we can talk about the actual estimation.
SHAP is estimated with weighted linear regression.

A rough summary of the KernelSHAP estimation algorithm, which estimates the feature attributions for explaining instance x's prediction of model f:

- Choose number of samples M.
- Sample new instances $z_k',\quad{}k\in\{1,\ldots,M\}$ in the simplified feature space (0's represent feature absence, 1's feature presence).
- Get prediction for each z_k' by first converting them to the original feature space and then applying model f: $f(h_x(z_k'))$
- Compute weights for each z_k'
- Fit weighted (generalized) linear model
- Return the $\phi_k$, the coefficients from the linear models, which are the Shapley values

<!-- Kernel -->
This procedure is almost identical to LIME (see [chapter on local models](#lime)).
The big difference is the weighting of the samples.
LIME weights the instances by how close they are to the original instance.
An instance in the simplified feature space can be seen as a coalition.
All the features with 1's are in this coalition, the 0's are absent.
SHAP weights the sampled instances by the weight the coalition would get in the Shapley Value estimation:
Coalations with very few features and with almost all features get high weights, the lowest weight is for coalations with half of the players present, the other half absent.


To achieve this Shapley conform weight, the authors propose the SHAP kernel:

$$\pi_{x'}(z')=\frac{(p-1)}{(p{}choose|z'|)|z'|(p-|z'|)}$$

Here, p is the number of features in the simplified feature space, $|z'|$ the number of present features in instance z'.
Lundberg and Lee show that, with this Kernel, indeed the correct SHAP values are recovered.

TODO: Add image of ShAP kernel

```{r shap-kernel, fig.cap="", out.width=800}
```


<!-- Sampling trick -->
From this Figure we can also see that we can cover a high percentage of the cumulative weight, when we include low and high number of present features.
The Shapley Value (and therefore the kernel) weights combinations very high, when either almost all (simplified) features are at zero or almost all (simplified) features are at 1.
That means we learn the most about the contribution of the j-th feature to the prediction when we look at the situation where we only get to know the j-th features value or the situation where we know all feature values and look how much the prediction changes.
We can enumerate these situations (also where 1 feature is known or where two features is known, etc.) and get a relative high weight already done, even though we are sampling.
Then we sample from the remaining combinations, with a readjusted weights.


<!-- Linear Model -->
Packing it all together, we estimate the Shapley values by optimizing this linear function:

$$L(f,g,\pi_{x'})=\sum_{z'\in{}Z}[f(h_x(z'))-g(z')]^2\pi_{x'}(z')$$

with g(x') being the additive attribution, the sum of Shapley values:

$$g(x')=\phi_0+\sum_{i=1}^M\phi_ix_i'$$

The loss optimization is a standard linear model.
Instead of the original features, we use the simplified feature space and the sampled instances.

Since we are in a classic linear regression setting, we can also make use of the tools that come with it.
For example, regularization.
By adding an L1 penalty to this term, we can generate sparse explanations.
(I am not so sure whether the resulting attribution values are still valid Shapley values)


### TreeSHAP

KernelSHAP is a heuristic and very expensive to compute.
Lundberg et. al (2018)[^tree-shap] proposed TreeSHAP, a fast implementation of SHAP for tree-based machine learning models such as decision trees, random forests and gradient boosted trees.
TreeSHAP is fast, computes exact Shapley values and avoids creating unrealistic instances for the estimation as KernelSHAP does when features are dependent.

How much faster is it?
For the exact Shapley values, it reduces the computational complexity from $O(TL2^M)$ to $O(TLD^2)$ where T is the number of trees, L the maximum number of leaves in any tree and D the maximal depth of any tree.

To explain an individual prediction with exact Shapley values, we have to estimate  $E(f(x)|x_S)$ for all possible feature value subsets S.

I will explain it for one tree, but it can be done for multiple trees and the results averaged.

For a given subset S and an instance x, we want to compute the expected prediction.
For features in set S, we know that they take on the values of the corresponding features in instance x that we want to explain.
For KernelSHAP we would sample the values for features not in S.
To get the expected prediction we have to start at the root of the tree and go down until we hit the terminal nodes.
Since not all feature values are given -- we only have subset S -- we have two distinguish two situations at each internal tree node.
First situation: the node used a feature from S for the split.
Then we go down the path of that node where the feature value matches the decision.
If a node uses a feature that is not in set S, we follow both paths.
We do this until we reach the terminal node in each path that we followed.
The mean of these terminal nodes, weighted by the number of instances per node, is the expected prediction for x given S.


Another way to think about it:
If we would condition on all features -- if S would be the set of all features -- then we would use the prediction from the node in which x would fall.
If we would condition on no feature at all -- if S would be the empty set -- we would use the weighted averaged prediction from all terminal nodes.
This is the same as taking the overall average prediction.
For the inbetween case where S contains some features, but not all, we ignore the predictions from the nodes that we can't reach, because their decisions path contradict values in x_S.
From the remaining terminal nodes, we average the predictions, weighted by the node sizes (i.e. number of training samples in that node).


The thing is, we need to do this procedure for every possible subset S of the feature values.
That is $\sum_{i=1}{p}\frac{(p-i)!i!}{i!}$.
Here, each summand is the set of all possible subsets S with the same cardinality (e.g. all possible subsets with 2 features).
To reduce the complexity, the authors proposed a faster algorithm in their paper that computes in polynomial time instead of exponential.
The basic idea is to push down all possible subsets S simultaneously.
Per node, we have to keep track of the number of subsets that go to the child nodes based on the split feature.
It is a bit trickier than that, since the subsets have different weights, so the algorithm also has to keep track of the overall weight of the subsets in each node.
For example, when the first split in a tree is on feature $x_3$, then all the subsets that contain feature $x_3$ will go to one node (to the one where x goes).
Subsets that do not contain feature $x_3$ go to both nodes with reduced weight.

TreeSHAP works with boosting frameworks XGBoost, LightGBM, CatBoost, and most tree-based scikit-learn models.

Next, we will look at SHAP explanations in action.

### Examples

TODO: CONTINUE HERE

Let us try SHAP out.
We trained a random forest classifier to predict [risk for cervical cancer](#cervical) with 100 trees.
We use SHAP to explain individual predictions.
Since the black box model is a random forest, we can use the fast TreeSHAP estimation method instead of the slower KernelSHAP.

Feature attributions can be visualized as forces that either increase or decrease the prediction.
The prediction starts from the baseline, the average of all predictions.
Some of the feature values of this instance have a positive effect on the predicted value and increase the prediction.
Some have a negative effect and decrease the prediction.
Besides the direction, the strength of the effect can be different.
Each SHAP get represented by an arrow that represents a force that increases the prediction (positive SHAP) or decreases it (negative SHAP).
The forces balance out at the actual prediction of that particular data instance.

The following figure shows the SHAP values visualized as forces for two women from the cervical cancer dataset:

```{r, fig.cap = "SHAP values for explaining the predicted cancer probabilities of two individuals. The first woman has an average risk. Risk increasing effects such as STDs are canceled out by decreasing effects such as age. The second woman has a very high risk. Age of 51 and 34 years of smoking are the main drivers of cancer risk increase. There are almost no risk lowering effects.", out.width = 800}
knitr::include_graphics("images/shap-explain-1.png")
knitr::include_graphics("images/shap-explain-2.png")
```



There are lots of analysis build on top.

Result of running shap for every instance of the data is a matrix of Shapley Values, rows are instances, features are columns, each entry a shap value.

We can aggregate this matrix in different ways together with the data.

We also have the feature matrix X, which we can link to their shapley values to understand the feature.


### SHAP Feature Importance
Along with SHAP comes an interpretation toolbox, which has global methods for feature importance, interaction detection and feature effects included.
These methods basically aggregate the Shapley values and bring them in connection with the feature values to provide global insights into the model.
The tools would work as well for Shapley Values not computed with SHAP.


Mean of absolute shap values.

$$I_j=\sum_{i=1}^n{}|\phi_j^{(i)}|$$

The simply sort in descending order and plot.
Very summarized view, more interesting when we also show the direction of the effects.

SHAP Feature Importance is an alternative to the [Permutation Feature IMportance](#feature-importance) measure.
A big difference: Permutation Feature Importance is based on model performance, whereas SHAP is based on feature attribution.

```{r out.width=800}
knitr::include_graphics("images/shap-importance.png")
```


TODO: Add example

TODO: Example interpretation

TODO: Comparison with permutation feature importance plot


### Summary Plot

- Sort features by SHAP Importance
- y-axis: one feature
- x-axis: SHAP value
- color: feature value (low to high)
- points: instances from the data.

Shows

- Importance of feature through ranking
- the range and distribution of the impact: e.g. does a feature only positively affect prediction? what's the range?
- effect direction: are high feature values associated with higher contribution? or with negative?

For the feature dependence, the summary plot give a first hint, but to really see what is the effect of one feature, the SHAP dependence plots is a better choice.
```{r}
knitr::include_graphics("images/shap-importance-extended.png")
```


### Feature Dependence

Plot with feature value on x-axis and the corresponding shap values on y-axis.

Pairs of $\{(x_j^{(i)},\phi_j^{(i)})\}_{i=1}^n$

SHAP Dependence Plots are an alternative to [Partial Dependence Plots](#pdp) and [Accumulated Local Effects](#ale).


```{r out.width=800}
knitr::include_graphics("images/shap-dependence.png")
```

TODO: Example interpretation
TODO: Comparison plot with PDP and ALE

The plot can be improved by also visualizing feature interactions:


### Feature Interactions

Currently only implemented for TreeSHAP (?)
They are based on Shapley interaction index from game theory.

Result is per instance a matrix of size n.features x n.features.

Shapley Interaction Index is defined as:

$$\phi_{i,j}=\sum_{S\subseteq\setminus\{i,j\}}\frac{|S|!(M-|S|-2)!}{2(M-1)!}\delta_{ij}(S)$$


when $i\neq{}j$ and:

$$\delta_{ij}(S)=f_x(S\cup\{i,j\})-f_x(S\cup\{i\})-f_x(S\cup\{j\})+f_x(S)$$

Basically the interaction gain by using both features, when account for the individual feature effects.
That's in line with the definition of interaction in the [Feature Interaction chapter](#interaction).

In TreeSHAP, the tree structure is again leveraged for computing the exact interaction values in an efficient way.

TODO: Explain how feature interaction is computed.

```{r out.width=800}
knitr::include_graphics("images/shap-dependence-interaction.png")
```


TODO: Add example of Feature Interactions
TODO: Example interpretation

### Clustering SHAP values

Usually in clustering, you cluster on the features to get groups of instances from your data that are similar to each other and different between groups.

This has the problem that you might use irrelevant features in the clustering, which get equal weight.

SHAP clustering works by clustering on the SHAP values of each instance, instead of the feature values.
That means you cluster instances by how similar their explanations are.
Another advantage is that SHAP values are all on the same scale, as opposed to the original features that might stand for completely different things (how do you compare meter and number of children?).
SHAP values for all features are contributions to the prediction and therefore on the same scale.

You can use any type of clustering method of your choice.
For example in shap package hierachical agglomerative cluster was used.


The following plot shows all shapley values sorted by explanation similarity.


We take the force plots from before, for each instance.
We rotate them, so that they are vertical and put them side by side.

```{r, fig.cap="Stacked SHAP feature attributions clustered by explanation similarity. Each position on the x-axis is an instance from the data. Red SHAP values increase the prediction, blue ones decrease it.", out.width=800}
knitr::include_graphics("images/shap-clustering.png")
```


TODO: Example
TODO: Interpretation


### Advantages

- Based on Shapley, therefore theoretical foundation
- Similarity / Unification of other methods
- Coherent Ecosystem
- Works for multiclass
- link function, GLMs
- Automatic interaction detection 
- Fast model-specific implementations, but also model-agnostic
- comes with many visualizations

### Disadvantages

- As everything else, problem with correlated features, at least for KernelSahp
- KernelShap is slow
- gt


### Software

Python: shap, also integrated into xgboost (LINK) and LightGBM (LINK)
R:  [shapper](https://modeloriented.github.io/shapper/), integrated into [R xgboost packages](https://rdrr.io/cran/xgboost/man/xgb.plot.shap.html)`
Others?


[^lundberg2016]: Lundberg, Scott M., and Su-In Lee. "A unified approach to interpreting model predictions." Advances in Neural Information Processing Systems. 2017.

[^tree-shap]: Lundberg, Scott M., Gabriel G. Erion, and Su-In Lee. "Consistent individualized feature attribution for tree ensembles." arXiv preprint arXiv:1802.03888 (2018).
