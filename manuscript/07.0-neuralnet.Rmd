# Neural Networks Interpretation {#neural-networks}

<!-- General Intro -->
This chapter focuses on interpretation methods for neural networks.
The presented methods visualize features and concepts learned by a neural network, explain individual predictions and simplify neural networks.

<!-- What are NNs -->
Deep neural networks are very powerful if you have a lot of data with a spatial structure, like images or texts.
The chapters assume that you are  familiar with deep learning, including convolutional neural networks.

#Image pixels have neighbouring pixels and each pixel individually is uninteresting, words in a sentence have an order which is consequential for the meaning of the sentence.
Deep learning has been very successful recently, especially in image classification and segmentation, language translation and many more tasks. 
The success story of deep neural networks took of in 2012, when the ImageNet image classification challenge [^imagenet] was won by a deep learning approach.
Since then, we have witnessed a Cambrian explosion of deep neural network architectures, with a trend towards deeper networks with more and more weight parameters.

<!-- Why not interpretable -->
For predictions, the input data is passed through many layers of multiplication with the learned weights and non-linear transformations.
A single prediction can involve millions of mathematical operations, depending on the architecture of the neural network.
There is no chance that we humans can follow the exact mapping from the data input to the prediction.
We would have to consider millions of weights to understand a prediction by a neural network.
And all these weights interact with each other in complex ways.
To interpret the behavior and predictions of neural networks, we need specific interpretation methods.

<!-- Why specific interpretation -->
We can certainly use [model-agnostic methods](#agnostic), like [local models](#lime) or [partial dependence plots](#pdp), but there are two reasons why it makes sense to look at interpretation methods that are specifically designed for neural networks:
First, neural networks learn feature and concepts in their hidden layers and second, the gradient can be utilized to implement interpretation methods that are more computationally efficient compared to  model-agnostic method that only look at the model "from the outside".
An additional reason is that most other methods in this book are for interpreting models for tabular data, but image and text data require and allow different explanations.

We next chapters cover the following topics:
- [Feature Visualization](#feature-visualisation): What features did the neural network learn?
- [Adversarial Examples](#adversarial): How can we manipulate the inputs to get a wrong classification?
- [Concepts](#neural-concepts) (IN PROGRESS): Which more abstract concepts did the neural network learn?
- [Feature Attribution](#feature-attribution) (IN PROGRESS): How did each input contribute to a certain prediction?
- [Modell Distillation](#neural-distillation) (IN PROGRESS): How can we explain a neural network with a simpler model?
