

# Model-Agnostic Methods {#agnostic}

Separating the explanations from the machine learning model (= model-agnostic interpretation methods) has some advantages (Ribeiro, Singh, and Guestrin 2016[^Ribeiro2016]).
The great advantage of model-agnostic interpretation methods over model-specific ones is their flexibility.
Machine learning developer are free to use any machine learning model they like, when the interpretation methods can be applied to any model.
Anything that builds on an interpretation of a machine learning model, such as a graphic or user interface, also becomes independent of the underlying machine learning model.
Typically, not just one, but many types of machine learning models are evaluated to solve a task, and when comparing models in terms of interpretability, it is easier to work with model-agnostic explanations, because the same method can be used for any type of model.

An alternative to model-agnostic interpretation methods is to use only [interpretable models](#simple), which often has the big disadvantage that predictive performance is lost compared to other machine learning models and you limit yourself to this type of interpretation method.
The other alternative is to use model-specific interpretation methods.
The disadvantage of this is that it binds you to this one algorithm and it will be difficult to switch to something else.

Desirable aspects of a model-agnostic explanation system are (Ribeiro, Singh, and Guestrin 2016):

- **Model flexibility:**
The interpretation method can work with any machine learning model.
The method should work for both random forests and deep neural networks.
- **Explanation flexibility:**
You are not limited to a certain form of explanation.
In some cases it might be useful to have a linear formula, in other cases a graphic with feature importances.
- **Representation flexibility:**
The explanation system should not have to use the same feature representation as the model being explained.
For a text classifier that uses abstract word embedding vectors, it might be preferable to use the presence of individual words for the explanation.


**The bigger picture**

Let us take a high level look at model-agnostic interpretability.
We abstract the world first by capturing it by collecting data, and abstract it further by learning to predict the data (for the task) with a machine learning model.
Interpretability is just another layer on top that helps humans understand.

![The big picture of explainable machine learning. The real world goes through many layers before it reaches the human in the form of explanations.](images/big-picture.png)

The lowest layer is the **World**.
This could literally be nature itself, like the biology of the human body and how it reacts to medication, but also more abstract things like the real estate market.
The World layer contains everything that can be observed and is of interest.
Ultimately, we want to learn something about the World and interact with it.

The second layer is the **Data** layer.
We have to digitalise the World in order to make it processable for computers and also to store information.
The Data layer contains anything from images, texts, tabular data and so on.

By fitting machine learning models based on the Data layer, we get the **Black Box Model** layer.
Machine learning algorithms learn with data from the real world to make predictions or find structures.

Above the Black Box Model layer is the **Interpretability Methods** layer, which helps us deal with the opacity of machine learning models.
What were the most important features for a particular diagnosis?
Why was a financial transaction classified as fraud?

The last layer is occupied by a **Human**.
Look! This one waves to you because you are reading this book and helping to provide better explanations for black box models!
Humans are ultimately the consumers of the explanations.

This multi-layered abstraction also helps to understand the differences in approaches between statisticians and machine learning practitioners.
Statistician deal with the Data layer, such as planning clinical trials or designing surveys.
They skip the Black Box Model layer and go right to the Interpretability Methods layer.
Machine learning specialists also deal with the Data layer, such as collecting labeled samples of skin cancer images or crawling Wikipedia.
Then comes the machine learning model.
Interpretability Methods layer is skipped and humans directly deal with the black box models prediction.
It's a nice thing that in interpretable machine learning the work of a statistician and a machine learner fuses and becomes something better.

Of course this graphic does not capture everything:
Data could come from simulations.
Black box models also output predictions that might not even reach humans, but only supply other machines and so on.
But overall it is a useful abstraction for to understand how (model-agnostic) interpretability becomes this new layer above machine learning models.


[^Ribeiro2016]: Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. "Model-Agnostic Interpretability of Machine Learning." ICML Workshop on Human Interpretability in Machine Learning, no. Whi.
