## Neural Networks

This chapter looks at a few tools that are specifically for interpreting neural networks.

There are three main themes: How can we visualize the abstract features that a neural networks learned, how can we explain predictions and how can we simplify a network?
There are of course more techniques but these three receive a lot of attention and are important.

What makes neural networks different from other models and what is special about the interpretation methods?
First, the intrinsically learned features are a specialty of neural networks.
Also gradient based methods, which many interpretation methods make use of and are thus more efficient as there model-agnostic sibling methods.

See also Visual Interpretability for Deep Learning: a Survey


### Feature Visualization / Neural Activation

Convolutional Neural Networks (CNNs) are known to "learn" features in it's layers.
Starting from edge detectors to detectors for faces.
This chapter explains how we can interpret what each neuron learned to detect.
Visualizing the images that maximize a neuron activation or channel activation are called "Feature Visualization", because 
Chapter based on https://distill.pub/2017/feature-visualization/
This chapter will not bother with all the details. 
Go to the blog post if you need those.
I just summarize here and criticially discuss the method.

The information in a neural network can be seen to become more and more refined until it is the classification outcome.
In each step, more abstract concepts are learned.
From simple edge detectors, over more complex textures to dog noses up to the dog classification.

These images look intepretable, we may detect that one of them shows a certain concept, like a tennis ball. 
But then again, we interpret it to be a tennis ball, the same as we interpret art.
It could be something else, and actually you might see something different on the image as well.
A blend of concepts.
And let me remind you that we only look at the image that maximizes the activation.
If we go the other direction and minimize activation, a completely different blend of concepts might occur.
There can be multiple neurons or layers that respond (activate) for the same patterns or images.

Feature attribution might also be called saliency maps.

This chapter assumes knowledge about CNNs, so I only say two sentences about CNNs here:
Convolutional Neural Networks have an architecture where first convolutional and pooling layers are connected, then fully connected layers.
The inputs are pixels.
While an image can be understood by a human, each single pixel does not hold much meaning.
By passing the pixels through the layers, the trained neural networks neurons get activated, based on the content of the picture.
The idea is to find for each neuron an image that maximizes that neurons activation.

TODO, use here first image of https://distill.pub/2017/feature-visualization/

So understanding a neural network is turned into an optimization problem:
For an optimization problem, we have to choose the objective we want to optimize and the search space.
Both things define what we interpret.


Search space: 
- from training data
- artificially created images

Training data has the problem that elements on the image can be correlated and we don't get to see what the neural network is looking for.
If all the images that have a high activation of a neuron / channel show a dog and a tennis ball, we don't know whether the neural network looks at the dog, the tennis ball or maybe at both.


TODO: Draw the different things by marking them in CNN drawing.
Also you have more options for what you maximize
- Neuron activiation. Makes no sense because you have millions of neurons in architecturs like GoogleNet.
- A whole channel. Still a lot of elements.
- A whole (convolutional) layer
- A neuron in the fully connected layer (pre-softmax)
- The final class probability (e.g. dog)





```{r feature-viz-units, fig.cap="Features can be visualized for different units. The most common is the vissualization for the individual channels. Here, n is the index of the layer, z the channel index, k the class index, x and y the spatial positions. Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/.", out.width=800}
knitr::include_graphics("images/feature-visualization-units.png")
```

For multiple artificial neurons, we maximize the mean activation.

TODO: Write about how many of each thing to optimize there are in some representative architectures (AlexNet, GoogleNet, ...)
Goal: Shows how feasible each is for looking at all of those images.
Keep in mind that images can be unstable and it is recommend to look at multiple images per unit that is looked at.

See for example how many images you have to look at for Google Net here:
https://distill.pub/2017/feature-visualization/appendix/

To not get these images with repetitive patterns (which are more like adversarial examples than real life images), many researchers applied some regularization or some constraint.
Three options for regularization:
- Frequency penalization
- transformation robustness
- learned priors (e.g. GANs)

Google DeepDream takes an image, extracts the activation from one and adds it to the image.
Start with an image, feed it to a pretrained nn, choose a layer.
During the forward pass until the chosen layer, extract the gradients.
We want to maximize for the given image the activation of the chosen layer, so we add the gradient to the image.
A few techniques like Gaussian blurring are applied to improve the resulting images.


See here: https://distill.pub/2017/feature-visualization/
Appendix of post: https://distill.pub/2017/feature-visualization/appendix/

"Because InceptionV1 is a convolutional network, there is not just one activation vector per layer per image. This means that the same neurons are run on each patch of the previous layer. Thus, when we pass an entire image through the network, each neuron will be evaluated hundreds of times, once for each overlapping patch of the image. We can consider the vectors of how much each neuron fired for each patch separately. "
See also: https://distill.pub/2019/activation-atlas/

Positive and negative images are not in some natural or logical way opposites.
Networks learn a very alien view of the world.
This also restricts the usefulness for day to day ML.
But helps to build a general understanding.

Deep connection with adversarial examples, because there you try also to find an image that maximizes something (e.g. class probability of an adversarial class).

#### Advantages
- Really great to learn this. makes clearer how nns work. For example it showed that CNNs look more on the texture, less on the shapes: I MAGE N ET - TRAINED CNN S ARE BIASED TOWARDS TEXTURE; INCREASING SHAPE BIAS IMPROVES  ACCURACY AND ROBUSTNESS
- Great Desktop Wallpaper or for T-Shirts
- Can be combined with feature attribution to get a better tool, see here: https://distill.pub/2018/building-blocks/.
Feature attributions are explained here: TODO: LINK CHAPTER

#### Disadvantages
- not practical for daily work
- difficult to get it to work.
- Maximum activation only small part of the space. 
  Negative activation exists as well, what about slightly postive activation? 
  We would have to describe the whole spectrum to describe that neuron.
- too many neurons
- what does it tell you in the end? only sense of understanding, but final prediction is still a difficult interaction between all those neurons, and these images only show the maximal activation, but neurons might only be have activated for some specific prediction, and it is unclear if that image really helps you then


More a tool for getting a general, better understanding of cnns, but not for daily job.



#### Software
- Blog Post with some code: https://github.com/fg91/visualizing-cnn-feature-maps/blob/master/filter_visualizer.ipynb
- Tensorflow: https://github.com/InFoCusp/tf_cnnvis
- Keras: https://github.com/jacobgil/keras-filter-visualization
- DeepViz Toolbox - https://github.com/yosinski/deep-visualization-toolbox
TODO: How does DeepDream fit in here?
- CAffee and with GANS https://github.com/Evolving-AI-Lab/synthesizing
### Feature Attribution


TODO: Formula for attribution (linear sum). 

DeepLIFT, SHAP, Layer-Wise Relevance Propagation, DeepLift
VISUALIZING DEEP NEURAL NETWORK DECISIONS: PREDICTION DIFFERENCE ANALYSIS



### TCAV

Useful to analyze a neural network with regard to specfic concepts, e.g. to check for gender bias.

