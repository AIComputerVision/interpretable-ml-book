## CNN Feature Visualization

Convolutional Neural Networks (CNNs) are known to "learn" features in it's layers.
Starting from edge detectors to detectors for faces.
This chapter explains how we can interpret what each neuron learned to detect.
Visualizing the images that maximize a neuron activation or channel activation are called "Feature Visualization", because 
Chapter based on https://distill.pub/2017/feature-visualization/
This chapter will not bother with all the details. 
Go to the blog post if you need those.
I just summarize here and criticially discuss the method.


Feature attribution might also be called saliency maps.

This chapter assumes knowledge about CNNs, so I only say two sentences about CNNs here:
Convolutional Neural Networks have an architecture where first convolutional and pooling layers are connected, then fully connected layers.
The inputs are pixels.
While an image can be understood by a human, each single pixel does not hold much meaning.
By passing the pixels through the layers, the trained neural networks neurons get activated, based on the content of the picture.
The idea is to find for each neuron an image that maximizes that neurons activation.

TODO, use here first image of https://distill.pub/2017/feature-visualization/

So understanding a neural network is turned into an optimization problem:
For an optimization problem, we have to choose the objective we want to optimize and the search space.
Both things define what we interpret.


Search space: 
- from training data
- artificially created images

Training data has the problem that elements on the image can be correlated and we don't get to see what the neural network is looking for.
If all the images that have a high activation of a neuron / channel show a dog and a tennis ball, we don't know whether the neural network looks at the dog, the tennis ball or maybe at both.


TODO: Draw the different things by marking them in CNN drawing.
Also you have more options for what you maximize
- Neuron activiation
- A whole channel
- A whole (convolutional) layer
- A neuron in the fully connected layer (pre-softmax)
- The final class probability (e.g. 

TODO: Write about how many of each thing to optimize there are in some representative architectures (AlexNet, GoogleNet, ...)
Goal: Shows how feasible each is for looking at all of those images.
Keep in mind that images can be unstable and it is recommend to look at multiple images per unit that is looked at.

See for example how many images you have to look at for Google Net here:
https://distill.pub/2017/feature-visualization/appendix/

To not get these images with repetitive patterns (which are more like adversarial examples than real life images), many researchers applied some regularization or some constraint.
Three options for regularization:
- Frequency penalization
- transformation robustness
- learned priors (e.g. GANs)

TODO: How does DeepDream fit in here?

See here: https://distill.pub/2017/feature-visualization/
Appendix of post: https://distill.pub/2017/feature-visualization/appendix/

### Advantages
- Really great to learn this. makes clearer how nns work
- Great Desktop Wallpaper or for T-Shirts
- Can be combined with feature attribution to get a better tool, see here: https://distill.pub/2018/building-blocks/.
Feature attributions are explained here: TODO: LINK CHAPTER

### Disadvantages
- not practical for daily work
- difficult to get it to work.
- Maximum activation only small part of the space. 
  Negative activation exists as well, what about slightly postive activation? 
  We would have to describe the whole spectrum to describe that neuron.
- too many neurons
- what does it tell you in the end? only sense of understanding, but final prediction is still a difficult interaction between all those neurons, and these images only show the maximal activation, but neurons might only be have activated for some specific prediction, and it is unclear if that image really helps you then


More a tool for getting a general, better understanding of cnns, but not for daily job.

