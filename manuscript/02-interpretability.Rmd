```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
```

# Interpretability {#interpretability}

Throughout the book I will use this rather simple but elegant definition of interpretability of Miller (2017) [^Miller2017] : **Interpretability is the degree to which a human can understand the cause of a decision.**
Another one is: **Interpretability is the degree to which a human can consistently predict the model's result** [^kim].
The higher the interpretability of a model, the easier it is for someone to comprehend why certain decisions or predictions have been made.
A model is better interpretable than another model if its decisions are easier for a human to comprehend than decisions from the second model.
I will use both the terms interpretable and explainable interchangeably.
Like  Miller (2017), I think it makes sense to distinguish between the terms interpretability/explainability and explanation.
I will use "explanation" for explanations of individual predictions.
See the [section about explanations](#explanation) to learn what we humans see as a good explanation.

## The Importance of Interpretability {#interpretability-importance}

If a machine learning model performs well, **why don't you just trust the model** and ignore **why** it made a certain decision?
"The problem is that a single metric, such as classification accuracy, is an incomplete description of most real-world tasks." (Doshi-Velez and Kim 2017 [^Doshi2017])

Let's dive deeper into the reasons why interpretability is so important.
When it comes to predictive modeling, you have to make a trade-off:
Do you just want to know **what** is predicted?
For example, the probability that a customer will churn or how effective some drug will be for a patient.
Or do you want to know **why** the prediction was made and possibly pay for the interpretability with a drop in predictive performance?
In some cases, you don't care why a decision was made, it's enough to know that the predictive performance on a test dataset was good.
But in other cases, knowing the 'why' can help you learn more about the problem, the data and the reason why a model might fail.
Some models may not require explanations  because they are used in a low-risk environment, meaning a mistake will not have serious consequences, (e.g. a movie recommender system) or the method has already been extensively studied and evaluated (e.g. optical character recognition).
The need for interpretability arises from an incompleteness in problem formalization (Doshi-Velez and Kim 2017), which means that for certain problems or tasks it is not enough to get the prediction (the **what**).
The model must also explain how it came to the prediction (the **why**), because a correct prediction only partially solves your original problem.
The following reasons drive the demand for interpretability and explanations (Doshi-Velez and Kim 2017 and Miller 2017).

**Human curiosity and learning**: Humans have a mental model of their environment that is updated when something unexpected happens.
This update is performed by finding an explanation for the unexpected event.
For example, a human feels unexpectedly sick and asks, "Why do I feel so sick?".
He learns that he gets sick every time he eats those red berries.
He updates his mental model and decides that the berries caused the sickness and should therefore be avoided.
When opaque machine learning models are used in research, scientific findings remain completely hidden if the model only gives predictions without explanations.
To facilitate learning and satisfy curiosity as to why certain predictions or behaviors are created by machines, interpretability and explanations are crucial.
Of course, humans don't need explanations for everything that happens.
For most people it's okay that they don't understand how a computer works.
Unexpected events makes us curious.
For example: Why is my computer shutting down unexpectedly?


Closely related to learning is the human desire to **find meaning in the world**.
We want to harmonize contradictions or inconsistencies between elements of our knowledge structures.
"Why did my dog bite me even though it has never done so before?" a human might ask.
There is a contradiction between the knowledge of the dog's past behavior and the newly made, unpleasant experience of the bite.
The vet's explanation reconciles the dog owner's contradiction:
"The dog was under stress and bit."
The more a machine's decision affects a person's life, the more important it will be for the machine to explain its behavior.
If a machine learning model rejects a loan application, this may be completely unexpected for the applicant.
He can only reconcile this inconsistency between expectation and reality with some kind of explanation.
The explanations don't actually have to fully explain the situation, but should address a main cause.
Another example is algorithmic product recommendation.
Personally, I always think about why certain products or movies have been algorithmically recommended to me.
Often it is quite clear:
Advertising follows me on the Internet because I recently bought a washing machine, and I know that in the next days I will be followed by advertisements for washing machine.
Yes, it makes sense to suggest gloves if I already have a winter hat in my shopping cart.
The algorithm recommended this movie, because users who liked other movies I liked also enjoyed the recommended movie.
Increasingly, Internet companies are adding explanations to their recommendations.
A good example is the Amazon product recommendation, which is based on frequently purchased product combinations:
```{r amazon-recommendation, fig.cap='Recommended products when buying some paint from [Amazon](https://www.amazon.com/Colore-Acrylic-Paint-Set-12/dp/B014UMGA5W/). Visited on December 5th 2012.', out.width=500}
knitr::include_graphics("images/amazon-freq-bought-together.png")
```


In many scientific disciplines there is a change from qualitative to quantitative methods (e.g. sociology, psychology), and also towards machine learning (biology, genomics).
The **goal of science** is to gain knowledge, but many problems are solved with big datasets and black box machine learning models.
The model itself becomes the source of knowledge instead of the data.
Interpretability makes it possible to extract this additional knowledge captured by the model.

Machine learning models take on real-world tasks that require **safety measures** and testing.
Imagine a self-driving car automatically detects cyclists based on a deep learning system.
You want to be 100% sure that the abstraction the system has learned is error-free, because running over cyclists is quite bad.
An explanation might reveal that the most important learned feature is to recognize the two wheels of a bicycle, and this explanation helps you think about edge cases like bicycles with side bags that partially cover the wheels.

By default, machine learning models will pick up biases from the training data.
This can turn your machine learning models into racists that discriminate against protected groups.
Interpretability is a useful debugging tool for **detecting bias** in machine learning models.
It might happen that the machine learning model you have trained for automatic approval or rejection of credit applications discriminates against a minority.
Your main goal is to grant loans to people who will eventually repay them.
The incompleteness of the problem formulation in this case lies in the fact that you not only want to minimize loan defaults, but are also obliged not to discriminate on the basis of certain demographics.
This is an additional constraint that is part of your problem formulation (granting loans in a low-risk and compliant way) that is not covered by the loss function the machine learning model was optimized for.

The process of integrating machines and algorithms into our daily lives requires interpretability to increase **social acceptance**.
People attribute beliefs, desires, intentions and so on to objects.
In a famous experiment, Heider and Simmel (1944) [^Heider] showed participants videos of shapes in which a circle opened a door to enter a "room" (which was simply a rectangle).
The participants described the actions of the shapes as they would describe the actions of a human agent, assigning intentions and even emotions and personality traits to the shapes.
Robots are a good example, like my vacuum cleaner, which I named "Doge".
If Doge gets stuck, I think:
"Doge wants to keep cleaning, but asks me for help because it got stuck."
Later, when Doge finishes cleaning and searches the home base to recharge, I think:
"Doge has a desire to recharge and intends to find the home base".
Also I attribute personality traits:
"Doge is a bit dumb, but in a cute way".
These are my thoughts, especially when I find out that Doge has knocked over a plant while dutifully vacuuming the house.
A machine or algorithm that explains its prediction will find more acceptance.
See also the [chapter on explanations](#explanation), which argues that explanations are a social process.


Explanations are used to **manage social interactions**.
By creating a shared meaning of something, the explainer influences the actions, emotions and beliefs of the recipient of the explanation.
For a machine to interact with us, it may need to shape our emotions and beliefs.
Machines have to "persuade" us, so that they can achieve their intended goal.
I would not fully accept my robot vacuum cleaner if it did not explain its behavior to some degree.
The vacuum cleaner creates a shared meaning of, for example, an "accident" (like getting stuck on the bathroom carpet ... again) by explaining that it got stuck instead of simply stopping to work without comment.
Interestingly, there may be a misalignment between the goal of the explaining machine (create trust) and the goal of the recipient (understand the prediction or behavior).
Perhaps the full explanation for why Doge got stuck could be that the battery was very low, that one of the wheels is not working properly and that there is a bug that makes the robot go to the same spot over and over again even though there was an obstacle.
These reasons (and a few more) caused the robot to get stuck, but it only explained that something was in the way, and that was enough for me to trust its behavior and get a shared meaning of that accident.
By the way, Doge got stuck in the bathroom again.
We have to remove the carpets  each time before we let it clean.

```{r doge-stuck, fig.cap="Doge, my vacuum cleaner got stuck. As an explanation for the accident, Doge told me that it needs to be on an even surface.", out.width=800}
knitr::include_graphics("images/doge-stuck.png")
```

Machine learning models can only be **debugged and audited** when they can be interpreted.
Even in low risk environments, such as movie recommendation, the ability to interpret is valuable in the research and development phase as well as after deployment.
Later, when a model is used in a product, things can go wrong.
An interpretation for an erroneous prediction helps to understand the cause of the error.
It delivers a direction for how to fix the system.
Consider an example of a husky versus wolf classifier that misclassifies some huskies as wolves.
Using interpretable machine learning methods, you would find that the misclassification was due to the snow on the image.
The classifier learned to use snow as a feature for classifying images as "wolf", which might make sense in terms of separating wolves from huskies in the training dataset, but not in the real-world use.

If you can ensure that the machine learning model can explain decisions, you can also check the following traits more easily (Doshi-Velez and Kim 2017):

- Fairness: Ensuring that predictions are unbiased and don't discriminate against protected groups (implicitly or explicitly).
An interpretable model can tell you why it has decided that a certain person should not get a loan, and it becomes easier for a human to judge whether the decision is based on a learned demographic (e.g. racial) bias.
- Privacy: Ensuring that sensitive information in the data is protected.
- Reliability or Robustness: Ensuring that small changes in the input don't lead to large changes in the prediction.
- Causality: Check that only causal relationships are picked up. 
- Trust: It is easier for humans to trust a system that explains its decisions compared to a black box.

**When we don't need interpretability.**

The following scenarios illustrate when we don't need or even don't want interpretability of machine learning models.

Interpretability is not required if the model **has no significant impact**.
Imagine someone named Mike working on a machine learning side project to predict where his friends will go for their next holidays based on Facebook data.
Mike just likes to surprise his friends with educated guesses where they're going on holidays.
There is no real problem if the model is wrong (at worst just a little embarrassment for Mike), nor is there a problem if Mike can't explain the output of his model.
It's perfectly fine not to have interpretability in this case.
The situation would change if Mike started building a business around these holiday destination predictions.
If the model is wrong, the business could lose money, or the model may work worse for some people because of learned racial bias.
As soon as the model has a significant impact, be it financial or social, interpretability becomes relevant.

Interpretability is not required when the **problem is well studied**.
Some applications have been sufficiently well studied so that there is enough practical experience with the model and problems with the model have been solved over time.
A good example is a machine learning model for optical character recognition that processes images from envelopes and extracts addresses.
There is years of experience with these systems and it is clear that they work. 
Also, we are not really interested in gaining additional insights about the task at hand. 

Interpretability might enable people or programs to **manipulate the system**.
Problems with users who deceive a system result from a mismatch between the goals of the creator and the user of a model.
Credit scoring is such a system because banks want to ensure that loans are only given to applicants who are likely to return them, and applicants aim to get the loan even if the bank doesn't want to give them one.
This mismatch between the goals introduces incentives for applicants to game the system to increase their chances of getting a loan.
If an applicant knows that having more than two credit cards negatively affects his score, he simply returns his third credit card to improve his score, and organizes a new card after the loan has been approved.
While his score improved, the actual probability of repaying the loan remained unchanged.
The system can only be gamed if the inputs are proxies for another feature, but don't actually cause the outcome.
Whenever possible, proxy features should be avoided as they make models gameable.
For example, Google developed a system called Google Flu Trends to predict flu outbreaks.
The system correlated Google searches with flu outbreaks - and it has performed poorly. 
The distribution of search queries changed and Google Flu Trends missed many flu outbreaks.
Google searches don't cause the flu.
When people search for symptoms like "fever" it's merely a correlation with actual flu outbreaks.
Ideally, models would only use causal features because they would not be gameable.

## Taxonomy of Interpretability Methods

Methods for machine learning interpretability can be classified according to various criteria. 

**Intrinsic or post hoc?**
This criteria distinguishes whether interpretability is achieved by restricting the complexity of the machine learning model (intrinsic) or by applying methods that analyze the model after training (post hoc).
Intrinsic interpretability refers to machine learning models that are considered interpretable due to their simple structure, such as short decision trees or sparse linear models.
Post hoc interpretability refers to the application of interpretability methods after model training.
Permutation feature importance is, for example, a post hoc interpretability method.
Post hoc methods can also be applied to intrinsically interpretable models.
For example, permutation feature importance can be computed for decision trees.
The organization of the chapters in this book is determined by the distinction between [intrinsically interpretable models](#simple) and [post hoc (and model-agnostic) interpretability methods](#agnostic).

**Result of the interpretability method**  
The various interpretability methods can be roughly differentiated according to their results.

- **Feature summary statistic**:  
Many interpretation methods provide summary statistics for each feature.
Some methods return a single number per feature, such as feature importance, or a more complex result, such as the pairwise feature interaction strengths, which consists of a number for each feature pair.
- **Feature summary visualization**:  
Most of the feature summary statistics can also be visualized. 
Some feature summaries are actually only meaningful if they are visualized and a table would be a wrong choice.
The partial dependence of a feature is such a case. 
Partial dependence plots are curves that show a feature and the average predicted outcome.
The best way to to present partial dependences is to actually draw the curve instead of printing the coordinates.
- **Model internals (e.g. learned weights)**: 
The interpretation of intrinsically interpretable models falls into this category. 
Examples are the weights in linear models or the learned tree structure (the features and thresholds used for the splits) of decision trees. 
The lines are blurred between model internals and feature summary statistic in, for example, linear models, because the weights are both model internals and summary statistics for the features at the same time.
Another method that outputs model internals is the visualization of feature detectors learned in convolutional neural networks. 
Interpretability methods that output model internals are by definition model-specific (see next criteria). 
- **Data point**: 
This category includes all methods that return data points (already existent or newly created) to make a model interpretable. 
One method is called counterfactual explanations.
To explain the prediction of a data instance, the method finds a similar data point by changing some of the features for which the predicted outcome changes in a relevant way (e.g., a flip in the predicted class). 
Another example is the identification of prototypes of predicted classes. 
To be useful, interpretability methods that output new data points require that the data points themselves can be interpreted. 
This works well for images and text, but is less useful for tabular data with hundreds of features.
- **Intrinsically interpretable model**: 
One solution to interpreting black box models is to approximate them (either globally or locally) with an interpretable model. 
The interpretable model itself is interpreted by looking at internal model parameter or feature summary statistics.


**Model-specific or model-agnostic?**  
Model-specific interpretation tools are limited to specific model classes.
The interpretation of regression weights in a linear model is a model-specific interpretation, since - by definition - the interpretation of intrinsically interpretable models is always model-specific.
Tools that only work for the interpretation of e.g. neural networks are model-specific.
Model-agnostic tools can be used on any machine learning model and are usually post hoc.
These agnostic methods usually work by analyzing feature input and output pairs. 
By definition, these methods can't have access to model internals such as weights or structural information.

**Local or global?**  
Does the interpretation method explain an individual prediction or the entire model behavior? 
Or is the scope somewhere in between?
Read more about the scope criterion in the next section.

## Scope of Interpretability
An algorithm trains a model that produces the predictions. 
Each step can be evaluated in terms of transparency or interpretability.

###  Algorithm Transparency
*How does the algorithm create the model?*

Algorithm transparency is about how the algorithm learns a model from the data and what kind of relationships it can learn.
If you use convolutional neural networks to classify images, you can explain that the algorithm learns edge detectors and filters on the lowest layers.
This is an understanding of how the algorithm works, but not for the specific model that is learned in the end, and not for how individual predictions are made.
Algorithm transparency only requires knowledge of the algorithm and not of the data or learned models.
This book focuses on model interpretability and not algorithm transparency.
Algorithms such as the least squares method for linear models are well studied and understood.
They are characterized by a high transparency.
Deep learning approaches (pushing a gradient through a network with millions of weights) are less well understood and the inner workings are the focus of ongoing research.
It is not clear exactly how they work, so they are less transparent.


### Global, Holistic Model Interpretability
*How does the trained model make predictions?*

You could describe a model as interpretable if you can comprehend the entire model at once (Lipton 2016[^Lipton2016]).
To explain the global model output, you need the trained model, knowledge of the algorithm and the data.
This level of interpretability is about understanding how the model makes decisions, based on a holistic view of its features and each of the learned components such as weights, other parameters, and structures.
What features are the important and what kind of interactions between them take place?
Global model interpretability helps to understand the distribution of your target outcome based on the features.
Global model interpretability is very difficult to achieve in practice.
Any model that exceeds a handful of parameters or weights is unlikely to fit into the short-term memory of the average human.
I argue that you can't really imagine a linear model with 5 features
Because it would mean drawing the estimated hyperplane mentally in the 5-dimensional feature space.
Any feature space with more than 3 dimensions is simply inconceivable for humans.
Usually, when people try to comprehend a model, they consider only parts of it, such as the weights in linear models.

### Global Model Interpretability on a Modular Level
*How do parts of the model affect predictions?*


A Naive Bayes model with many hundreds of features would be too big for me and you to keep in our working memory. 
And even if we manage to memorize all the weights, we wouldn't be able to quickly make predictions for new data points. 
In addition, you need to have the joint distribution of all features in your head to estimate the importance of each feature and how the features affect the predictions on average.
An impossible task.
But you can easily understand a single weight.
While global model interpretability is usually out of reach, there is a good chance of understanding at least some models on a modular level.
Not all models are interpretable at a parameter level.
For linear models, the interpretable parts are the weights and distribution of the features, for trees it would be splits (used features plus cut-off points) and leaf node predictions.
Linear models, for example, look like as if they could be perfectly interpreted on a modular level, but the interpretation of a single weight is interlocked with all other weights.
The interpretation of a single weight always comes with the footnote that the other input features remain at the same value, which is not the case with many real applications.
A linear model hat predicts the value of a house, that takes into account both the size of the house and the number of rooms, can have a negative weight for the room feature.
It can happen because there is already the highly correlated house size feature.
In a market where people prefer larger rooms, an  house with fewer rooms could be worth more than a house with more rooms if both have the same size. 
The weights only make sense in the context of the other features in the model.
But the weights in a linear model can still be interpreted better than the weights of a deep neural network.

### Local Interpretability for a Single Prediction 
*Why did the model make a certain prediction for an instance?*

You can zoom in on a single instance and examine what kind of prediction the model makes for this input, and explain why it made this decision.
If you look at an individual prediction, the behavior of the otherwise complex model might behave more pleasently.
Locally, the prediction might only depend linearly or monotonously on some features, rather than having a complex dependence on them.
For example, the value of a house may depend non-linearly on its size.
But if you are looking at only one particular 100 square meters house, there is a possibility that for that data subset, your model prediction depends linearly on the size. 
You can find this out by simulating how the predicted price changes when you increase or decrease the size by 10 square meters.
Local explanations can therefore be more accurate than global explanations.
This book presents methods that can make individual predictions more interpretable in the [section on model-agnostic methods](#agnostic).

### Local Interpretability for a Group of Predictions
*Why did the model make specific predictions for a group of instances?*

Model predictions for multiple instances can be explained either with global model interpretability methods (on a modular level) or with explanations of individual instances.
The global methods can be applied by taking the group of instances, treating them as if the group were the complete dataset, and using the global methods with this subset.
The individual explanation methods can be used on each instance and then listed or aggregated for the entire group.


## Evaluating Interpretability

There is no real consensus about what interpretability is in machine learning.
Nor is it clear how to measure it.
But there is some initial research on this and an attempt to formulate some approaches for evaluation, as described in the following section.

Doshi-Velez and Kim (2017) propose three main levels for the evaluation of interpretability:

**Application level evaluation (real task)**:
Put the explanation into the product and have it tested by the end user.
Imagine fracture detection software with a machine learning component that locates and marks fractures in x-rays.
At the application level, radiologists would test the fracture detection software directly to evaluate the model.
This requires a good experimental setup and an understanding of how to assess quality.
A good baseline for this is always how good a human would be at explaining the same decision.

**Human level evaluation (simple task)** is a  simplified application level evaluation.
The difference is that these experiments are not carried out with the domain experts, but with laypersons.
This makes experiments cheaper (especially if the domain experts are radiologists) and it is easier to find more tester.
An example would be to show a user different explanations and the human would choose the best one.

**Function level evaluation (proxy task)** does not require humans.
This works best when the class of models used has already been evaluated by someone else in a human level evaluation.
For example, it might be known that the end users understand decision trees.
In this case, a proxy for explanation quality may be the depth of the tree.
Shorter trees would get a better explainability score.
It would make sense to add the constraint that the predictive performance of the tree remains good and does not decrease too much compared to a larger tree.

The next chapter focuses on the evaluation of explanations for individual predictions on the function level.
What are the relevant properties of explanations that we would consider for their evaluation?

## Properties of Explanations {#properties}

We want to explain the predictions of a machine learning model.
To achieve this, we rely on an explanation method, an algorithm that generates explanations.
**An explanation usually relates the feature values of an instance to its model prediction in a humanly understandable way.**
Other types of explanations consist of a set of data instances (e.g in the case of the k-nearest neighbour model).
For example, we could predict cancer risk using a support vector machine and explain predictions using the [local surrogate method](#lime), which generates decision trees as explanations.
Or we could use a linear regression model instead of a support vector machine.
The linear regression model is already equipped with an explanation method (interpretation of the parameters of the weighted sum).

We take a closer look at the properties of explanation methods and explanations (Robnik-Sikonja, Marko, and Marko Bohanec, 2018 [^human-ml]).
These properties can be used to judge how good an explanation (method) is.
It's not clear for all these properties how to measure them correctly, so one of the challenges is to formalize how they could be calculated.


**Properties of Explanation Methods**

- **Expressive Power** is the "language" or structure of the explanations the method is able to generate.
An explanation method could generate IF-THEN rules, decision trees, a weighted sum, natural language or something else.
- **Translucency** describes how much the explanation method relies on looking into the machine learning model, like its parameters.
For example, explanation methods relying on intrinsically interpretable models like the linear regression model (model-specific) are highly translucent.
Methods only relying on manipulating inputs and observing the predictions have zero translucency.
Depending on the scenario, different levels of translucency might be desirable.
The advantage of high translucency is that the method can rely on more information to generate explanations.
The advantage of low translucency is that the explanation is more portable.
- **Portability** describes the range of machine learning models with which the explanation method can be used.
Methods with a low translucency have a higher portability because they treat the machine learning model as a black box.
[Local-surrogate models](#lime) might be the explanation method with the highest portability.
Methods that only work for e.g. recurrent neural networks have low portability.
- **Algorithmic Complexity** describes the computational complexity of the method that generates the explanation.
This property is important to consider when computation time is a bottleneck in generating explanations.

**Properties of Individual Explanations**

- **Accuracy**: How well does an explanation predict unseen data?
High accuracy is especially important if the explanation is used for predictions in place of the machine learning model. 
Low accuracy can be fine if the accuracy of the machine learning model is also low, and if the goal is to explain what the black box model does.
In this case, only fidelity is important.
- **Fidelity**: How well does the explanation approximate the prediction of the black box model?
High fidelity is one of the most important properties of an explanation, because an explanation with low fidelity is useless to explain the machine learning model.
Accuracy and fidelity are closely linked.
If the black box model has high accuracy and the explanation has high fidelity, the explanation also has high accuracy.
Some explanations offer only local fidelity, meaning the explanation only approximates well to the model prediction for a subset of the data (e.g. [local surrogate models](#lime)) or even for only an individual data instance (e.g. [Shapley Values](#shapley)).
- **Consistency**: How much does an explanation differ between models that have been trained on the same task and that produce similar predictions?
For example, I train an SVM and a linear regression model on the same task and both produce very similar predictions.
I compute explanations using a method of my choice and analyse how different the explanations are.
If the explanations are very similar, the explanations are highly consistent.
I find this property somewhat tricky, since the two models could use different features, but get similar predictions (also called ["Rashomon Effect"](https://en.wikipedia.org/wiki/Rashomon_effect)). 
In this case a high consistency is not desirable because the explanations have to be very different.
High consistency is desirable if the models really rely on similar relationships.
- **Stability**: How similar are the explanations for similar instances?
While consistency compares explanations between models, stability compares explanations between similar instance for a fixed model.
High stability means that slight variations in the features of an instance don't substantially change the explanation (unless these slight variations also strongly change the prediction).
A lack of stability can be the result of a high variance of the explanation method, i.e. the explanation method is strongly affected by slight changes of the feature values of the instance to be explained.
A lack of stability can also be caused by non-deterministic components of the explanation method, such as a data sampling step, like  the [local surrogate method](#lime) has.
High stability is always desirable.
- **Comprehensibility**: How well do humans understand the explanations?
This looks just like one more property among many, but it's the elephant in the room. 
Difficult to define and measure, but extremely important to get right.
Many people agree that comprehensibility depends on the audience.
Ideas for measuring comprehensibility include measuring the size of the explanation (number of features with a non-zero weights in a linear model, number of decision rules, ...) or testing how well people can predict the behavior of the machine learning model from the explanations.
The comprehensibility of the features used in the explanation should also be considered.
A complex transformation of features might be less comprehensible than the original features.
- **Certainty**: Does the explanation reflect the certainty of the machine learning model?
Many machine learning models only give predictions without a statement about the models confidence that the prediction is correct.
If the model predicts a 4% probability of cancer for one patient, is it as certain as the 4% probability that another patient, with different attributes, received?
An explanation that includes the model's certainty is very useful.
- **Degree of Importance**: How well does the explanation reflect the importance of features or parts of the explanation?
For example, if a decision rule is generated as an explanation for an individual prediction, is it clear which of the conditions of the rule was the most important?
- **Novelty**: Does the explanation reflect whether a data instance to be explained comes from a "new" region far removed from the distribution of training data?
In such cases, the model may be inaccurate and the explanation may be useless.
The concept of novelty is related to the concept of certainty.
The higher the novelty, the more likely it is that the model will have low certainty due to lack of data.
- **Representativeness**: How many instances does an explanation cover?
Explanations can cover the entire model (e.g. interpretation of weights in a linear regression model) or represent only an individual prediction (e.g. [Shapley values, a game theoretical attribution method](#shapley)).



## Human-friendly Explanations {#explanation}

Let's dig deeper and discover what we humans accept as 'good' explanations and what the implications for interpretable machine learning are.

Research from the humanities can help us to figure that out.
Miller (2017) did a huge survey of publications about explanations and this Chapter builds on his summary.

In this Chapter, I want to convince you of the following:
As an explanation for an event, humans prefer short explanations (just 1 or 2 causes), which contrast the current situation with a situation where the event would not have happened.
Especially abnormal causes make good explanations.
Explanations are social interactions between the explainer and the explainee (receiver of the explanation) and therefore the social context has a huge influence on the actual content of the explanation.

If you build the explanation system to get ALL the factors for a certain prediction or behavior, you do not want a human-style explanation, but rather a complete causal attribution.
You probably want a causal attribution when you are legally required to state all influencing features or if you are debugging the machine learning model.
In this case, ignore the following points.
In all other setting, where mostly lay persons or people with little time are the recipients of the explanation, follow the advice here.


### What is an explanation?

An explanation is the **answer to a why-question** (Miller 2017).

- Why did the treatment not work on the patient?
- Why was my loan rejected?
- Why haven't we been contacted by alien life yet?

The first two kind of questions can be answered with an "everyday"-explanation, while the third one is from the category "More general scientific phenomena and philosophical questions".
We focus on the "everyday"-type explanation, because this is relevant for interpretable machine learning.
Questions starting with "how" can usually be turned into "why" questions:
"How was my loan rejected?" can be turned into "Why was my loan rejected".


The term "explanation" means the social and cognitive process of explaining, but it's also the product of these processes.
The explainer can be a human or a machine


### What is a "good" explanation? {#good-explanation}

Now that we know what an explanation is, the question arises, what a good explanation is.

"Many artificial intelligence and machine learning publications and methods claim to be about 'interpretable' or 'explainable' methods, yet often this claim is only based on the authors intuition instead of hard facts and research." - Miller (2017)

Miller (2017) summarises what a 'good' explanation is, which this Chapter replicates in condensed form and with concrete suggestions for machine learning applications.

**Explanations are contrastive** (Lipton 2016):
Humans usually don't ask why a certain prediction was made, but rather why this prediction was made instead of another prediction.
We tend to think in counterfactual cases, i.e. "How would the prediction have looked like, if input X were different?".
For a house value prediction, a person might be interested in why the predicted price was high compared to the lower price she expected.
When my loan application is rejected, I am not interested what in general constitutes a rejection or an approval.
I am interested in the factors of my application that would need to change so that it got accepted.
I want to know the contrast between my application and the would-be-accepted version of my application.
The realisation that contrastive explanations matter, is an important finding for explainable machine learning.
As we will see, most interpretable models allow to extract some form of explanation that implicitly contrast it to an artificial data instance or an average of instances.
A doctor who wonders: "Why did the treatment not work on the patient?", might ask for an explanation contrastive to a patient, where the treatment worked and who is similar to the non-responsive patient.
Contrastive explanations are easier to understand than complete explanations.
A complete explanation to the doctor's why question (why does the treatment not work) might include:
The patient has the disease already since 10 years, 11 genes are over-expressed making the disease more severe, the patients body is very fast in breaking down the medication into ineffective chemicals , etc..
The contrastive explanation, which answers the question compared to the other patient, for whom the drug worked, might be much simpler:
The non-responsive patient has a certain combination of genes, that make the medication much less effective, compared to the other patient.
The best explanation is the one that highlights the greatest difference between the object of interest and the reference object.
**What it means for interpretable machine learning**:
Humans don't want a complete explanation for a prediction but rather compare what the difference were to another instance's prediction (could also be an artificial one).
Making explanations contrastive is application dependent, because it requires a point of reference for comparison.
And this might depend on the data point to be explained, but also on the user receiving an explanation.
A user of a house price prediction website might want to have an explanation of a house price prediction contrastive to her own house or maybe to some other house on the website or maybe to an average house in the neighbourhood.
The solution for creating contrastive explanations in an automated fashion might include finding prototypes or archetypes in the data to contrast to.

**Explanations are selected**:
People don't expect explanations to cover the actual and complete list of causes of an event.
We are used to selecting one or two causes from a huge number of possible causes as THE explanation.
For proof, switch on the television and watch some news:
"The drop in share prices is blamed on a growing backlash against the product due to problems consumers are reporting with the latest software update.",
"Tsubasa and his team lost the match because of a weak defence: they left their opponents to much free space to play out their strategy.",
"The increased distrust in established institutions and our government are the main factors that reduced voter turnout."
The fact that an event can be explained by different causes is called the Rashomon Effect.
Rashomon is a Japanese movie in which alternative, contradictory stories (explanations) of a samurai's death are told.
For machine learning models it is beneficial, when a good prediction can be made from different features.
Ensemble methods can combine multiple models with different features (different explanations) and thrive because averaging over those "stories" makes the predictions more robust and accurate.
But it also means that there is no good selective explanation why they made the prediction.
**What it means for interpretable machine learning**:
Make the explanation very short, give only 1 to 3 reasons, even if the world is more complex.
The [LIME method](#lime) does a good job with this.


**Explanations are social**:
They are part of a conversation or interaction between the explainer and the receiver of the explanation.
The social context determines the content and type of explanations.
If I wanted to explain why digital cryptocurrencies are worth so much, to a technical person I would say things like:
"The decentralised, distributed blockchain-based ledger that cannot be controlled by a central entity resonates with people's desire to secure their wealth, which explains the high demand and price.".
But to my grandma I might say:
"Look Grandma: Cryptocurrencies are a bit like computer gold. People like and pay a lot for gold, and young people like and pay a lot for computer gold."
**What it means for interpretable machine learning**:
Be mindful of the social setting of your machine learning application and of the target audience.
Getting the social part of the machine learning model right depends completely on your specific application.
Find experts from the humanities (e.g. psychologists and sociologists) to help you out.

**Explanations focus on the abnormal**.
People focus more on abnormal causes to explain events (Kahnemann 1981[^Kahnemann]).
These are causes, that had a small likelihood but happened anyways (counterfactual explanation).
And removing these abnormal causes would have changed the outcome a lot.
Humans consider these kinds of "abnormal" causes to be
 good explanations.
An example (Štrumbelj and Kononenko (2011)[^Strumbelj2011]):
Assume that we have a dataset of test situations between teachers and students.
The teachers have the option to directly let students pass a course after they have given a presentation or they can ask additional questions to test the student's knowledge, which determines if the student passes.
This means we have one feature 'teacher'-feature, which is either 0 (teacher does not test) or 1 (teacher does test).
The students can have different levels of preparation (student feature), which translate to different probabilities of correctly answering the teacher's question (in case she decides to test the student).
We want to predict if a student will pass the course and explain our prediction.
The chance to pass is 100% if the teacher does not ask additional questions, else the probability to pass is according to the student's level of preparation and the resulting probability to correctly answer the questions.
Scenario 1:
The teacher asks the students additional questions most of the time (e.g. 95 out of 100 times).
A student who did not study (10% chance to pass the questions part)  was not among the lucky ones and gets additional questions, which he fails to correctly answer.
Why did the student fail the course?
We would say it was the student's fault to not study.
Scenario 2:
The teacher rarely asks additional questions (e.g. 3 out of 100 times).
For a student who did not learn for possible questions, we would still predict a high probability to pass the course, since questions are unlikely.
Of course, one of the students did not prepare for the questions (resulting in a 10% chance to pass the questions).
He is unlucky and the teacher asks additional questions, which the student cannot answer and he fails the course.
What is the reason for failing?
I'd argue that now, the better explanation is that the teacher did test the student, because it was unlikely that the teacher would test.
The teacher feature had an abnormal value.
**What it means for interpretable machine learning**:
If one of the input features for a prediction was abnormal in any sense (like a rare category of a categorical feature) and the feature influenced the prediction, it should be included in an explanation, even if other 'normal' features have the same influence on the prediction as the abnormal one.
An abnormal feature in our house price predictor example might be that a rather expensive house has three balconies.
Even if some attribution method finds out that the three balconies contribute the same price difference as the above average house size, the good neighbourhood and the recent renovation, the abnormal feature "three balconies" might be the best explanation why the house is so expensive.

**Explanations are truthful**.
Good explanations prove to be true in reality (i.e. in other situations).
But, disturbingly, this is not the most important factor for a 'good' explanation.
For example selectiveness is more important than truthfulness.
An explanation that selects only one or two possible causes can never cover the complete list of causes.
Selectivity omits part of the truth.
It's not true that only one or two factors caused a stock market crash for example, but the truth is that there are millions of causes that influence millions of people to act in a way that caused a crash in the end.
**What it means for interpretable machine learning**:
The explanation should predict the event as truthfully as possible, which is sometimes called **fidelity** in the context of machine learning.
So when we say that three balconies increase the price of a house, it should hold true for other houses as well (or at least for similar houses).
To humans, fidelity is not as important for a good explanations as selectivity, contrast and the social aspect.

**Good explanations are coherent with prior beliefs of the explainee**.
Humans tend to ignore information that is not coherent with their prior beliefs.
This effect is known as confirmation bias (Nickerson 1998[^Nickerson]).
Explanations are not spared from this type of bias:
People will tend to devalue or ignore explanations that do not cohere with their beliefs.
This of course differs individually, but there are also group-based prior beliefs like political opinions.
**What it means for interpretable machine learning**:
Good explanations are consistent with prior beliefs.
This one is hard to infuse into machine learning and would probably drastically compromise predictive accuracy.
An example would be negative effects of house size for the predicted price of the house for a few of the houses, which, let's assume, improves accuracy (because of some complex interactions), but strongly contradicts prior beliefs.
One thing you can do is to enforce monotonicity constraints (a feature can affect the outcome only into one direction) or use something like a linear model that has this property.

**Good explanations are general and probable**.
A cause that can explain a lot of events is very general and could be considered as a good explanation.
Note that this contradicts the fact that people explain things with abnormal causes.
As I see it, abnormal causes beat general causes.
Abnormal causes are, by definition, rare.
So in the absence of some abnormal event, a general explanation is judged to be good by humans.
Also keep in mind that people tend to judge probabilities of joint events incorrectly.
(Joe is a librarian. Is it more likely that he is shy or that he is a shy person that loves reading books?).
A good example is 'The bigger a house the more expensive it is', which is a very general, good explanation why houses are expensive or cheap.
**What it means for interpretable machine learning**:
Generality is easily measured by a feature's support, which is the number of instances for which the explanation applies over the total number of instances.

[^Miller2017]: Miller, Tim. 2017. "Explanation in Artificial Intelligence: Insights from the Social Sciences." arXiv Preprint arXiv:1706.07269.

[^Doshi2017]: Doshi-Velez, Finale, and Been Kim. 2017. "Towards A Rigorous Science of Interpretable Machine Learning," no. Ml: 1–13. http://arxiv.org/abs/1702.08608.

[^Heider]: Heider, Fritz, and Marianne Simmel. 1944. "An Experimental Study of Apparent Behavior." The American Journal of Psychology 57 (2). JSTOR: 243–59.

[^Lipton2016]: Lipton, Zachary C. 2016. "The Mythos of Model Interpretability." ICML Workshop on Human Interpretability in Machine Learning, no. Whi.

[^Kahnemann]: Kahneman, Daniel, and Amos Tversky. 1981. "The Simulation Heuristic." STANFORD UNIV CA DEPT OF PSYCHOLOGY.

[^Strumbelj2011]: Štrumbelj, Erik, and Igor Kononenko. 2011. "A General Method for Visualizing and Explaining Black-Box Regression Models." In International Conference on Adaptive and Natural Computing Algorithms, 21–30. Springer.

[^Nickerson]: Nickerson, Raymond S. 1998. "Confirmation Bias: A Ubiquitous Phenomenon in Many Guises." Review of General Psychology 2 (2). Educational Publishing Foundation: 175.

[^kim]: Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. "Examples are not enough, learn to criticize! criticism for interpretability." Advances in Neural Information Processing Systems. 2016.

[^human-ml]: Robnik-Sikonja, Marko, and Marko Bohanec. "Perturbation-Based Explanations of Prediction Models." Human and Machine Learning. Springer, Cham, 2018. 159-175.

[^lipton2]: Lipton, Peter. "Contrastive explanation." Royal Institute of Philosophy Supplements 27 (1990): 247-266.
