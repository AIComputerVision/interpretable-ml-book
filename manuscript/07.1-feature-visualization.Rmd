## Feature Visualization {#feature-visualization}

Convolutional neural networks learn abstract features from the raw image pixels.
Methods for feature visualization aim to visualize these features by generating images that maximally activate units of a trained convolutional neural network.

TODO: CONTINUE DEEPL HERE

<!-- Background: Why feature visualization -->
One of the biggest strengths of deep neural networks is that they automatically learn more abstract features from the input features in the hidden layers.
This partially reduces the need for feature engineering.
The integrated feature learning part is especially remarkable for image and text data.
Let's say you want to build an image classifier using a support vector machine.
The raw pixels are not the best input to your SVM, so you create new features based on color, frequency domain, edge detectors and so on and train your machine learning model with it.
With convolutional neural networks, the image is passed into the network in its raw form (pixels), transformed multiple times first through the convolutional layers and then the fully connected layers and the network spits out the predictions.
During training, the convolutional neural network learns new features in its layers.

```{r fig.cap = "Architecture of Inception V1 neural network. Each enumerated unit (3a to 5b) represents a layer with differently sized convolutions and pooling. Figure from Olah, et al. 2019 (CC-BY 4.0) https://distill.pub/2017/activation-atlas/.", out.width = 800, include = FALSE}
knitr::include_graphics("images/inceptionv1.svg")
```
```{r fig.cap = "Features learned by a convolutional neural network (Inception V1) trained on the ImageNet data. The features range from low-level in the lower convolutional layers (left) to high-level in the higher convolutional layers (right). Figure from Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/appendix/.", out.width = 800}
knitr::include_graphics("images/cnn-features.png")
```

- The first convolutional layer(s) learn low-level features like edges and simple textures.
- Later convolutional layers learn mid-level features like different textures and patterns.
- Higher up convolutional layers learn high-level features like objects or parts of objects.
- The fully connected layers learn to connect the activations from the high-level features to the individual classes to be predicted.

Cool.
But how do we actually get those hallucinatory images?

<!-- Feature Visualization explained-->
The approach of making the learned features explicit is called **Feature Visualization**.
Feature visualization for a unit of the neural network is done by finding the input that maximizes the activation of that unit.
"Unit" refers to either individual neurons, whole feature maps, whole (convolutional) layers or the final class probability in classification.
The indiviudal neurons are atomic units of the network, but visualizing features on the neuron level results in a huge number of features.
A good choice of unit for feature visualization are the channels, also called activiation maps of the convolution layers.
We can go a step further and visualize a whole convolutional layer.
This objective is used for GoogleDream, which repeatedly adds the visualized features of a layer to the original image, resulting in a dream-like version of the input.
It's recommended to use the pre-softmax neuron of that class, because this gives better images (see Olah et. al 2017).

```{r units, fig.cap="Features visualization can be done for different units. A) Convolution neuron, B) Convolution channel, C) Convolution layer, D) Neuron, E) Hidden layer, F) Class probability neuron", out.width=800}
knitr::include_graphics("images/units.jpg")
```


```{r trippy, fig.cap="Optimized images for Inception V1 (channels mixed3a, mixed4c, mixed4d and mixed5a). Images are maximized for a random direction of the activations. Figure from Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/.", out.width=800, include = FALSE}
knitr::include_graphics("images/trippy.png")
```

### Feature Visualization through Optimization

Mathematically, feature visualization is an optimization problem.
We assume that the weights of the neural networks are fixed, meaning that the network is fully trained.
We are looking for a new image $img^*$ that maximize the (mean) activation of a unit, here a single neuron:

$$img^*=\arg\max_{img}h_{n,x,y,z}(img)$$

The function $h$ is the activation of a neuron, x the input to the network (e.g. an image), x and y describe the spatial position of the neuron, n enumerates the layers and z is the channel index.
For the mean activation of a whole channel z in layer n we maximize:

$$img^*=\arg\max_{img}\sum_{x,y}h_{n,x,y,z}(img)$$

This weighs the activation all neurons in channel z equally.
Alternatively, you could also maximize random directions, which means that the neurons would be differently weighted.
By that, we study how the neurons interact within the channel.
Intuitively, taking the mean activation makes sense, but in reality, the mean is just one of infinite ways to combine neurons and as good as any other weighted combination of neurons.
Instead of maximizing the activation, you can also minimize the activation (which is the same as maximizing the negative direction).
Interestingly, when you do that you get very different features for the same unit:

```{r pos-neg, fig.cap="Positive (left) and negative (right) activation of Inception V1 neuron 484 from layer mixed4d pre relu. While the neuron is maximally activated by wheels, something weird (which seems to have eyes) brings a negative activation. Code: https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/feature-visualization/negative_neurons.ipynb", out.width=800}
knitr::include_graphics("images/a484.png")
```

<!-- Why not use training data? -->
We can tackle this optimization problem in different ways.
First, why should we generate new images, we could simple try out all the images in our training data and choose the ones that maximizes the activation.
That's a valid approach, but training data has the problem that elements on the image can be correlated and we don't get to see what the neural network is really looking for.
If all the images that have a high activation of a certain channel show a dog and a tennis ball, we don't know whether the neural network looks at the dog, the tennis ball or maybe at both.

<!-- Direct optimization --> 
The other approach is to generate a new image, starting from random noise.
To get meaningful results, there are usually constraints on the image *img*, e.g. that only small changes are allowed.
To reduce noise in the feature visualization, you can apply jittering, rotation or scaling to the image before the optimization step.
These steps are also called transformation robustness.
Other ways to regularize are frequency penalization (reduce variance of neighboring pixels, blurring in each optimization step) and learned priors, e.g. GANs.

```{r activation-optim, fig.cap="Iterative optimization from random image to maximizing activation. Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/.", out.width=800}
knitr::include_graphics("images/activation-optim.png")
```
More sophisticated approach involve generative adversarial networks [^synthesize] and optimizing within their latent space or denoising autoencoder priors [^plugandplay].


### Connection to Adversarial Examples

<!-- Connection to Adversarial Examples -->
There is a connection between feature visualization and [adversarial examples](#adversarial):
Both techniques maximize the activation of a neural network unit.
For adversarial examples we search the maximum activation of the neuron after the softmax for the adversarial (= incorrect) class.
A difference is the image we start with:
For adversarial examples, it's obviously the image for which we want to generate the adversarial image.
For feature visualization it's, depending on the approach, random noise.



### Text and Tabular Data

The literature focuses on feature visualization for convolutional neural networks for image recognition.
Technically, there is nothing that prevents you from finding the input that maximally activates a neuron for tabular data or text data.
For tabular data, you would get a data point that maximally activates a (usually) fully connected neural network.
But of course, you might not really call it feature visualization, because it becomes harder to visualize.
For credit default prediction, the input might be number of prior credicts, number of contracts, address and dozens of other features.
In [^viz-rnn], the authors showed that recurrent neural networks indeed have neurons that learn interpretable features, like opening brackets or being highly activated in URLs.
They trained a character-level model, e.g. feed text to the neural network with the task to always predict the next character in the sequence.
Example closing brace: Once an opening brace "(" occurs, one of the neurons gets highly activated, and gets lower activated when the matching ")" closing bracket occurs.
Other neurons fire for neurons at the end of the line.
Some neurons fire in URLs.
This depends of course on which data the network was trained.
The difference to the feature visualization for CNNs is that the examples where not maximized for a given neuron.
But the activations of different neurons is observed for a given text to study its behavior.


### Advantages

- Feature visualizations give **unique insight into the workings of neural networks**, especially for image recognition.
  Given the opaque nature of neural networks, feature visualization is an important step for analyzing and describing neural networks.
  Through feature visualization we learned that the neural network first learns simple edge and texture detectors and more abstract parts and objects in higher levels.
- It's also a great tool for **communicating how neural networks work in a non-technical way**.
By visualizing how neural networks transform the data, it becomes clearerer why the neural network works.
Even if you might not understand all the mathematical details.
-  Feature visualization can be **combined with feature attribution** to get a better tool, see here: https://distill.pub/2018/building-blocks/.
  To explain why a certain prediction was made, we can combine the regional attribution of image parts and the channels that are most activated by those parts to get a better understanding what the network is looking for.
Feature attributions are explained here (IN PROGRESS)
- And, finally, feature visualizations make great desktop wallpaper or prints for **T-Shirts**.

### Disadvantages

-**Illusion of interpretability?** Many images look intepretable, we may detect that one of them shows a certain concept, like a tennis ball. 
But then again, we interpret it to be a tennis ball, the same as we interpret art.
It could be something else, and actually you might see something different on the image as well.
If we go the other direction and minimize activation, a completely different blend of concepts might occur.
There can be multiple neurons or layers that respond (activate) for the same patterns or images.
See for example how many images you have to look at for Google Net here.
And these are only for optimizing the channels, not the individual neurons.
https://distill.pub/2017/feature-visualization/appendix/
Keep in mind that images can be unstable and it is recommend to look at multiple images per unit that is looked at.
- **Too ,any units to look at**, even if you go to the channel activation.
  For Inception V1, it's already over 5000  channels from 9 conv layers.
  And that's only for one direction of activation.
  Also showing the negative activation and also a few images from the training data that maximally or minimally activates the layer (let's say 4 positive, 4 negative images), then we already have over 50000 images to look at.\
  And we haven't started to inspect random directions, or created a diverse set of multiple images per layer for the activations.
  And I probably should not start talking about doing this for the individual neurons...
- Many images are not interpretable show some abstract and, for us, confuse concepts.
  Showing it along with training data can help, but might not reveal a common feature
- **Restricted usefulness for applied settings**.
 feature visualization are not practical for daily work and difficult to get it to work.
 what does it tell you in the end? only sense of understanding, but final prediction is still a difficult interaction between all those neurons, and these images only show the maximal activation, but neurons might only be have activated for some specific prediction, and it is unclear if that image really helps you then
- **Lack of diversity**:
  An optimized image might only show a part of what activates the neural unit the most. Also training data can help here, because we see a broader spectrum.
  Another trick to achieve diversity is to add a term to the objective that e.g. penalizes the cosine similarity between examples.

The long list of disadvantages speak for the difficulties with feature visualization.
Nonetheless, it is a valuable technique to furthering our understanding of how deep learning work.
Just make sure not to read too much in the individual images.

### Software and Further Reading

You can use the open-source implementation [Lucid](https://github.com/tensorflow/lucid) to produce feature visualizations.
The cool thing is, you can conveniently try it out fromyour browser, by using the notebook links that are provided on the Lucid github page.
No additional software required.

If you want to dive a lot deeper into feature visualization, check out the distil.pub blog/journal, especially the feature visualization post by Olah et al. [^distill-fv] from which I got many of the images, and also about the building blocks of interpretability [^distill-blocks].


[^distill-fv]: Olah, et al., "Feature Visualization", Distill, 2017.

[^distill-blocks]: Olah, et al., "The Building Blocks of Interpretability", Distill, 2018.

[^plugandplay]: Nguyen, Anh, et al. "Plug & play generative networks: Conditional iterative generation of images in latent space." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.

[^synthesize]: Nguyen, Anh, et al. "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks." Advances in Neural Information Processing Systems. 2016.

[^viz-rnn]: Karpathy, Andrej, Justin Johnson, and Li Fei-Fei. "Visualizing and understanding recurrent networks." arXiv preprint arXiv:1506.02078 (2015).

[^imagenet]: Olga Russakovsky*, Jia Deng*, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei. (* = equal contribution) ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015


