## Activation Maximization {#activation-maximization}

`r if(is.html){only.in.html}`

Convolutional neural networks learn abstract features and concepts from the raw image pixels.
We can study units of the neural network (e.g. channels/feature maps) by visualizing and quantifying the concepts that maximally activate these units.
[Feature Visualization](#feature-visualization) visualizes the concepts that maximally activate a unit and [Network Dissection](#network-dissection) labels units by the concepts that maximally activate them.

<!-- Background: Why feature visualization -->
One of the greatest strengths of deep neural networks is that they automatically learn high-level features in the hidden layers.
This partially reduces the need for feature engineering.
Let's say you want to build an image classifier with a support vector machine.
The raw pixel matrices are not the best input for training your SVM, so you create new features based on color, frequency domain, edge detectors and so on.
With convolutional neural networks, the image is fed into the network in its raw form (pixels), transformed multiple times, first through multiple convolutional layers and then through the fully connected layers and turned into a classification or prediction.
During training, the convolutional neural network learns new, increasingly complex features in its layers.

```{r fig.cap = "Architecture of Inception V1 neural network. Each enumerated unit (3a to 5b) represents a layer with differently sized convolutions and pooling. Figure from Olah, et al. 2019 (CC-BY 4.0) https://distill.pub/2017/activation-atlas/.", out.width = 800, include = FALSE}
knitr::include_graphics("images/inceptionv1.svg")
```
```{r fig.cap = "Features learned by a convolutional neural network (Inception V1) trained on the ImageNet data. The features range from simple features in the lower convolutional layers (left) to more abstract features in the higher convolutional layers (right). Figure from Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/appendix/.", out.width = 800}
knitr::include_graphics("images/cnn-features.png")
```

- The first convolutional layer(s) learn features such as edges and simple textures.
- Later convolutional layers learn features such as more complex textures and patterns.
- The last convolutional layers learn features such as objects or parts of objects.
- The fully connected layers learn to connect the activations from the high-level features to the individual classes to be predicted.

Cool.
But how do we actually get those hallucinatory images?

### Feature Visualization {#feature-visualization}

<!-- Feature Visualization explained-->
The approach of making the learned features explicit is called **Feature Visualization**.
Feature visualization for a unit of a neural network is done by finding the input that maximizes the activation of that unit.
"Unit" refers either to individual neurons, entire feature maps, entire (convolutional) layers or the final class probability in classification (or the corresponding pre-softmax neuron, which is recommended).
Individual neurons are atomic units of the network, so we would get the most information by creating feature visualizations for each neuron.
But there is a problem: 
Neural networks often contain millions of neurons and looking at each neuron's feature visualization would take too long.
The channels (sometimes called activation maps) as units are a good choice for feature visualization.
We can go one step further and visualize an entire convolutional layer.
Layers as a unit are used for Google's DeepDream, which repeatedly adds the visualized features of a layer to the original image, resulting in a dream-like version of the input.

```{r units, fig.cap="Feature visualization can be done for different units. A) Convolution neuron, B) Convolution channel, C) Convolution layer, D) Neuron, E) Hidden layer, F) Class probability neuron (or corresponding pre-softmax neuron)", out.width=800}
knitr::include_graphics("images/units.jpg")
```


```{r trippy, fig.cap="Optimized images for Inception V1 (channels mixed3a, mixed4c, mixed4d and mixed5a). Images are maximized for a random direction of the activations. Figure from Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/.", out.width=800, include = FALSE}
knitr::include_graphics("images/trippy.png")
```

### Feature Visualization through Optimization

In mathematical terms, feature visualization is an optimization problem.
We assume that the weights of the neural network are fixed, which means that the network is trained.
We are looking for a new image that maximizes the (mean) activation of a unit, here a single neuron:

$$img^*=\arg\max_{img}h_{n,x,y,z}(img)$$

The function $h$ is the activation of a neuron, *img* the input of the network (an image), x and y describe the spatial position of the neuron, n specifies the layer and z is the channel index.
For the mean activation of an entire channel z in layer n we maximize:

$$img^*=\arg\max_{img}\sum_{x,y}h_{n,x,y,z}(img)$$

In this formula, all neurons in channel z are equally weighted.
Alternatively, you can also maximize random directions, which means that the neurons would be multiplied by different parameters, including negative directions.
In this way, we study how the neurons interact within the channel.
Instead of maximizing the activation, you can also minimize the activation (which corresponds to maximizing the negative direction).
Interestingly, when you maximize the negative direction you get very different features for the same unit:

```{r pos-neg, fig.cap="Positive (left) and negative (right) activation of Inception V1 neuron 484 from layer mixed4d pre relu. While the neuron is maximally activated by wheels, something which seems to have eyes yields a negative activation. Code: https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/feature-visualization/negative_neurons.ipynb", out.width=800}
knitr::include_graphics("images/a484.png")
```

<!-- Why not use training data? -->
We can address this optimization problem in different ways.
First, why should we generate new images?
We could simply search through our training images and select those that maximize the activation.
This is a valid approach, but using training data has the problem that elements on the images can be correlated and we can't see what the neural network is really looking for.
If images that yield a high activation of a certain channel show a dog and a tennis ball, we don't know whether the neural network looks at the dog, the tennis ball or maybe at both.

<!-- Direct optimization --> 
The other approach is to generate new images, starting from random noise.
To obtain meaningful visualizations, there are usually constraints on the image, e.g. that only small changes are allowed.
To reduce noise in the feature visualization, you can apply jittering, rotation or scaling to the image before the optimization step.
Other regularization options include frequency penalization (e.g. reduce variance of neighboring pixels) or generating images with learned priors, e.g. with generative adversarial networks (GANs) [^synthesize] or denoising autoencoders [^plugandplay].

```{r activation-optim, fig.cap="Iterative optimization from random image to maximizing activation. Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/.", out.width=800}
knitr::include_graphics("images/activation-optim.png")
```
#### Connection to Adversarial Examples

There is a connection between feature visualization and [adversarial examples](#adversarial):
Both techniques maximize the activation of a neural network unit.
For adversarial examples, we look for the maximum activation of the neuron for the adversarial (= incorrect) class.
One difference is the image we start with:
For adversarial examples, it is the image for which we want to generate the adversarial image.
For feature visualization it is, depending on the approach, random noise.

#### Text and Tabular Data

The literature focuses on feature visualization for convolutional neural networks for image recognition.
Technically, there is nothing to stop you from finding the input that maximally activates a neuron of a fully connected neural network for tabular data or a recurrent neural network for text data.
You might not call it feature visualization any longer, since the "feature" would be a tabular data input or text.
For credit default prediction, the inputs might be the number of prior credits, number of mobile contracts, address and dozens of other features.
The learned feature of a neuron would then be a certain combination of the dozens of features.
For recurrent neural networks, it is a bit nicer to visualize what the network learned:
Karpathy et. al (2015)[^viz-rnn] showed that recurrent neural networks indeed have neurons that learn interpretable features.
They trained a character-level model, which predicts the next character in the sequence from the previous characters.
Once an opening brace "(" occurred, one of the neurons got highly activated, and got de-activated when the matching closing bracket ")" occurred.
Other neurons fired at the end of a line.
Some neurons fired in URLs.
The difference to the feature visualization for CNNs is that the examples were note found through optimization, but by studying neuron activations in the training data.


Some of the images seem to clearly show well-known concepts like dog snouts or buildings.
But how can we be sure?
A rigorous, objective method of linking human concepts to individual neural network units is called Network Dissection.
Spoiler alert: Network dissection requires additional datasets that are labeled with the concepts we are looking for.

### Network Dissection {#network-dissection}

Network Dissection by Bau & Zhou et al. (2017) [^dissect] is an approach to quantify the interpretability of a unit of a convolutional neural network by linking highly activated areas to human concepts (objects, parts, textures, colors, ...).

The [Feature Visualizations](#feature-visualization) indicated that the channels of a convolutional neural network learns to detect specific concepts.
But these visualizations alone do not definitely prove that a unit has learned a certain concept and we also don't have a measure for how well a unit detects e.g. sky scrapers.
Before we go into the details of Network Dissection, we have to talk about the big hypothesis that is behind that line of research.
The hypothesis is that units of a neural network (like convolutional channel) learn, at least to some degree, disentangled concepts.

#### The Question of Disentangled Features

Does a neural network learn disentangled features?
Disentangled means that the network can clearly separate different concepts:
Convolutional channel 394 might learn to detect sky scrapers, 121 channel detects dog snouts, channel 12 detects stripes at 30 degree angle and so on.
For a prediction we could simply scan all those channels and say how much each concept was activated for that image.
For a missclassification for example, we could use our human knowledge and cross-check with the learned concepts.
The chihuahua was missclassified as a muffin?
Ah! The concept for chocolate chips in dough was highly activated, along with the blue frosting neuron, because someone put this weird hat on the chihuahua before taking the photo.
We have to train our network with more ugly chihuhuas in costumes.
Disentangled features would mean that our network is really interpretable.
When we ask:
How interpretable is the neural network?
How interpretable is each unit?
Is a certain unit connected with a certain concept?
How strong is that connection.

The answer to all that questions is Network Dissection, which 

TODO: CONTINUE HERE


### Network Dissection Algorithm

Let's get back to Network Dissection.
To really analyse the concepts, they propose to label the concepts on a pixel level in the training data.
This dataset is called 'Broden' which stands for broadly and densely labeled data.
The idea is that the framework aligns maps those concepts to the hidden units.


<!-- How does Network Dissection work? -->

1. Identify visual concepts which are labeled then in the data
1. Measure the activation of each neuron to these concepts
1. Quantify the alignment between activation and concept

A neuron might respond to more than on concept.
Since the researchers are only interested in completely disentangled, they propose to measure how much each neuron responds to a single concept.

The Broden dataset is segmented to the pixel level mostly.
All in all, there are following concept counts:
468 scences, 585 objects, 234 parts, 32 materials, 47 textures and 11 colors.

Every convolutional unit in the CNN is cross-checked with all concepts in broden whether they are good at segmenting the concepts.

```{r fig.cap = "For a given input image and a trained network (fixed weights), we forward propagate the image up to the target layer, upscale the activations to match the original image size and compare the maximum activations with the ground truth pixel-wise segmentation."}
knitr::include_graphics("images/dissection-network.png")
```


Algorithm:

1. For each image x  in Broden:
  1. Collect Activation map of each convolutional unit k: $A_k(x)$
  1. Compute distribution of of unit activations $\alpha_k$ over image
  1. The top quantile level $T_k$ is determined so that the probability that an activation is bigger than $T_k$ is 0.005: $P(\alpha_k>T_k)=0.005)$ for every location of the activation map. So the 0.5% quantile
  1. Scale up the possibly lower-resolution activation map $A_k(x)$ to the same resolution as the annotation mask $L_c(x)$ of the Broden dataset. We call the result $S_k(x)$.
  1. To reduce the activation map to the high activations, we binarize it: It's either on or off, depending whether a location exceeds the threshold. New mask is $M_k(x)=S_k(x)\leq{}T_k(x)$.

In the end we have a list with one max. activation masks per convolutional layer, $K$ masks to be precise.
We also have for each concept in Broden a mask.
For each combination of max-activation k and concept mask c, we simply compute how strongly they overlap with the intersection over union score:

$$IoU_{k,c}=\frac{\sum|M_k(x)\bigcap{}L_c(x)|}{\sum|M_k(x)\bigcup{}L_c(x)|}$$

where $|\cdot|$ is the cardinality of a set.

```{r, fig.cap = "The Intersection over Union (IoU) is computed by comparing the human ground truth annotation and the top activated pixels."}
knitr::include_graphics("images/dissection-dog-exemplary.jpg")
```

A unit can detect multiple concepts.
And multiple units can detect the same concept.
$IoU_{k,c}$ can be interpreted as the accuracy with which  unit k can detect the concept c.
The threshold chosen by the authors is $IoU_{k,c}>0.04$. 
When this treshold is exceeded, the unit was said to detect concept c.


Example of a unit detecting dog as concept.
It is a unit from "inception_4e unit 750". It has an $IoU=0.203$ for the dog concept.

```{r, fig.cap = "inception_4e unit 750 detecting dogs with $IoU=0.203$. The cut out area are the pixels with a high activation for this unit. It is a surprisingly coherent patch."}
knitr::include_graphics("images/dissection-dogs.jpeg")
```


#### Experiments

The authors tested different network architectures (e.g. VGG, and Inception-V3).
All models were trained from scratch on ImageNet and some other datasets (Places205, Places 365).
Additionally they had some self-supervised training tasks, like predicting context, video frame order or tracking, detecting object-centric alignment, colorizing images, predicting corsschannel and predicting ambient sound from frames.
For the self-supervised tasks, the AlexNet architecture was used.

Evaluation was done by humans, through Amazon Mechanical Turk.
For each unit, each rater was shown 15 images.
On these images, the patches with the highest activations as masks were shown.
Then they had to say whether or not this patch can be described with a given phrase, answered simply by yes/no.
If the answer was yes, the unit was deemed interpretable.
Interpretable units are the ones for which the raters agreed with the ground-truth interpretations.
Based on the set of interpretable units.
First measured human consistency, i.e. how much raters agreed with the ground truth (which is also from humans).
The it was measured how much the raters agreed with interpretability of network dissection methods.



The findings through the experiments across architectures, tasks and training settings reveal very interesting insights about the interpretability of convolutional neural networks:


- Interpretability of CNNs is axis-dependent, i.e. using random directions of units (weighted averages) reduces interpretability.
  Interpretability is measured as the number of units where IoU is larger than the treshold.
  This was tested by rotating images and comparing the concepts.
- The methods detects, similar to [Feature Visualizations](#feature-visualization) that the network detects lower-level concepts at lower layers and higher-level concepts at higher layers.
- Different architectures have different interpretability.
  Also different tasks have different interpretability.
- Interpretability is independent of discriminative power, since the units can be transformed with orthogonal transformations whiel the discriminative power remains the same, but interpretability decreases. A unit is a channel of a whole convolutional layer (e.g. one of 256 channels in layer conv5 from AlexNet)
- The number of intepretable CNN units strongly depends on the task and the architecture
- Also dependent on training of course, only after some training iterations these concepts emerge.
- Even random initializations (training the model multiple times with different random seeds but all else equal) yields slight differences in number of interpretable units.
- Batch normalization seems to decrease interpretability
- 
- 

You can look at results here.
Example with Resnet:
http://netdissect.csail.mit.edu/dissect/resnet-152-torch-places365/

Note how many units there are detecting airplanes.

Source for Network Dissection
http://netdissect.csail.mit.edu/


TODO: Cover a bit about GAN Dissection https://gandissect.csail.mit.edu/



Advantages:

- Can detect concepts which are not used in your classification task. Because concepts are different data.
 
Disadvantages

- Concepts covered by many units
- Still many units to look at.



### Advantages

- Feature visualizations give **unique insight into the working of neural networks**, especially for image recognition.
Given the complexity and opacity of neural networks, feature visualization is an important step in analyzing and describing neural networks.
Through feature visualization, we learned that neural networks first learn simple edge and texture detectors and more abstract parts and objects in higher layers.
- Feature visualization is a great tool to **communicate in a non-technical way how neural networks work**.
-  Feature visualization can be **combined with feature attribution methods**, which explain which pixels were important for the classification.
The combination of both methods allows to explain an individual classification along with local visualization of the learned features that were involved in the classification.
See [The Building Blocks of Interpretability from distill.pub](https://distill.pub/2018/building-blocks/).
- Finally, feature visualizations make great desktop wallpapers and T-Shirts prints.

### Disadvantages

- **Many feature visualization images are not interpretable** at all, but contain some abstract features for which we have no words or mental concept.
  The display of feature visualizations along with training data can help.
 The images still might not reveal what the neural network reacted to and only show something like "maybe there has to be something yellow in the images".
- There are **too many units to look at**, even when "only" visualizing the channel activations.
  For Inception V1 there are already over 5000 channels from 9 convolutional layers.
If you also want to show the negative activations plus a few images from the training data that maximally or minimally activate the channel (let's say 4 positive, 4 negative images), then you must already display more than 50 000 images.
  And you have not even started to investigate random directions or created a diverse set of multiple images for the channel activations.
- **Illusion of Interpretability?**
  These images can convey the illusion that we understand what the neural network is doing.
  But do we really understand what is going on in the neural network?
  Even if we look at hundreds or thousands of feature visualizations, we cannot understand the neural network.
 The neural network remains to incomprehensible: The neurons interact in a complex way, positive and negative activations are unrelated, multiple neurons might learn very similar features and for many of the features we do not have equivalent human concept.
  We must not fall into the trap of believing we fully understand neural networks just because we believe we saw that neuron 349 in layer 7 is activated by daisies. 




### Software and Further Material

There is an open-source implementation of feature visualization called [Lucid](https://github.com/tensorflow/lucid).
You can conveniently try it in your browser by using the notebook links that are provided on the Lucid Github page.
No additional software is required.
Other implementations are [tf_cnnvis](https://github.com/InFoCusp/tf_cnnvis) for TensorFlow, [Keras Filters](https://github.com/jacobgil/keras-filter-visualization) for Keras and [DeepVis](https://github.com/yosinski/deep-visualization-toolbox) for Caffe.


If you want to dive a lot deeper into feature visualization, take a look at the distill.pub online journal, especially the feature visualization post by Olah et al. [^distill-fv], from which I used many of the images, and also about the building blocks of interpretability [^distill-blocks].


[^distill-fv]: Olah, et al., "Feature Visualization", Distill, 2017.

[^distill-blocks]: Olah, et al., "The Building Blocks of Interpretability", Distill, 2018.

[^plugandplay]: Nguyen, Anh, et al. "Plug & play generative networks: Conditional iterative generation of images in latent space." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.

[^synthesize]: Nguyen, Anh, et al. "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks." Advances in Neural Information Processing Systems. 2016.

[^viz-rnn]: Karpathy, Andrej, Justin Johnson, and Li Fei-Fei. "Visualizing and understanding recurrent networks." arXiv preprint arXiv:1506.02078 (2015).

[^imagenet]: Olga Russakovsky*, Jia Deng*, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei. (* = equal contribution) ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015


