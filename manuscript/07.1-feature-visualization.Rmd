## Feature Visualization {#feature-visualization}

Convolutional neural networks learn abstract features from the raw image pixels.
Methods for feature visualization aim to visualize these features by generating images that maximally activate units of a trained convolutional neural network.

<!-- Background: Why feature visualization -->
One of the greatest strengths of deep neural networks is that they automatically learn high-level features from the input features in the hidden layers.
This partially reduces the need for feature engineering.
Let's say you want to build an image classifier with a support vector machine.
Only the raw pixels are not the best input for training your SVM, so you create new features based on color, frequency domain, edge detectors and so on.
With convolutional neural networks, the image is fed into the network in its raw form (pixels), transformed multiple times, first through multiple convolutional layers and then through the fully connected layers and turned into a classification or prediction.
During training, the convolutional neural network learns new features in its layers, with increasing complexity.

```{r fig.cap = "Architecture of Inception V1 neural network. Each enumerated unit (3a to 5b) represents a layer with differently sized convolutions and pooling. Figure from Olah, et al. 2019 (CC-BY 4.0) https://distill.pub/2017/activation-atlas/.", out.width = 800, include = FALSE}
knitr::include_graphics("images/inceptionv1.svg")
```
```{r fig.cap = "Features learned by a convolutional neural network (Inception V1) trained on the ImageNet data. The features range from simple features in the lower convolutional layers (left) to more abstract features in the higher convolutional layers (right). Figure from Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/appendix/.", out.width = 800}
knitr::include_graphics("images/cnn-features.png")
```

- The first convolutional layer(s) learn features such as edges and simple textures.
- Later convolutional layers learn features such as different textures and patterns.
- The last convolutional layers learn features such as objects or parts of objects.
- The fully connected layers learn to connect the activations from the high-level features to the individual classes to be predicted.

Cool.
But how do we actually get those hallucinatory images?

<!-- Feature Visualization explained-->
The approach of making the learned features explicit is called **Feature Visualization**.
Feature visualization for a unit of a neural network is done by finding the input that maximizes the activation of that unit.
"Unit" refers either to individual neurons, entire feature maps, entire (convolutional) layers or the final class probability in classification (or the corresponding pre-softmax neuron, which is recommended).
Individual neurons are atomic units of the network, so we would get the most information by creating feature visualizations for each neuron.
But there is a problem: 
Neural networks often contain millions of neurons and looking at each neurons feature visualization would take to long.
A good choice of unit for feature visualization are the channels, also called activation maps of the convolution layers.
We can go one step further and visualize an entire convolutional layer.
Layers as a unit are used for Google's DeepDream, which repeatedly adds the visualized features of a layer to the original image, resulting in a dream-like version of the input.

```{r units, fig.cap="Features visualization can be done for different units. A) Convolution neuron, B) Convolution channel, C) Convolution layer, D) Neuron, E) Hidden layer, F) Class probability neuron (or corresponding pre-softmax neuron)", out.width=800}
knitr::include_graphics("images/units.jpg")
```


```{r trippy, fig.cap="Optimized images for Inception V1 (channels mixed3a, mixed4c, mixed4d and mixed5a). Images are maximized for a random direction of the activations. Figure from Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/.", out.width=800, include = FALSE}
knitr::include_graphics("images/trippy.png")
```

### Feature Visualization through Optimization

In mathematical terms, feature visualization is an optimization problem.
We assume that the weights of the neural network is fixed, which means that the network is fully trained.
We are looking for a new image $img^*$ that maximizes the (mean) activation of a unit, here a single neuron:

$$img^*=\arg\max_{img}h_{n,x,y,z}(img)$$

The function $h$ is the activation of a neuron, x the input into the network (image), x and y describe the spatial position of the neuron, n specifies the layer and z is the channel index.
For the mean activation of an entire channel z in layer n we maximize:

$$img^*=\arg\max_{img}\sum_{x,y}h_{n,x,y,z}(img)$$

In this formula, all neurons in channel z are equally weighted.
Alternatively, you can also maximize random directions, which means that the neurons would be multiplied by different parameters, including negative directions.
In this way, we study how the neurons interact within the channel.
Instead of maximizing the activation, you can also minimize the activation (which corresponds to maximizing the negative direction).
Interestingly, when you maximize the negative direction you get very different features for the same unit:

```{r pos-neg, fig.cap="Positive (left) and negative (right) activation of Inception V1 neuron 484 from layer mixed4d pre relu. While the neuron is maximally activated by wheels, something weird (which seems to have eyes) brings a negative activation. Code: https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/feature-visualization/negative_neurons.ipynb", out.width=800}
knitr::include_graphics("images/a484.png")
```

<!-- Why not use training data? -->
We can address this optimization problem in different ways.
First, why should we generate new images?
We could simple search through our training images and select those that maximize the activation.
This is a valid approach, but using training data has the problem that elements on the image can be correlated and we can't see what the neural network is really looking for.
If images that yield a high activation of a certain channel show a dog and a tennis ball, we don't know whether the neural network looks at the dog, the tennis ball or maybe at both.

<!-- Direct optimization --> 
The other approach is to generate a new image, starting from random noise.
To obtain meaningful visualizations, there are usually constraints on the image *img*, e.g. that only small changes are allowed.
To reduce noise in the feature visualization, you can apply jittering, rotation or scaling to the image before the optimization step.
These steps are also called transformation robustness.
Other regularization options include frequency penalization (e.g. reduce variance of neighboring pixels) or generate image with learned priors, e.g. with generative adversarial networks (GANs) [^synthesize] or denoising autoencoders [^plugandplay].

```{r activation-optim, fig.cap="Iterative optimization from random image to maximizing activation. Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/.", out.width=800}
knitr::include_graphics("images/activation-optim.png")
```


### Connection to Adversarial Examples

There is a connection between feature visualization and [adversarial examples](#adversarial):
Both techniques maximize the activation of a neural network unit.
For adversarial examples, we look for the maximum activation of the neuron for the adversarial (= incorrect) class.
One difference is the image we start with:
For adversarial examples, it is the image for which we want to generate the adversarial image.
For feature visualization it is, depending on the approach, random noise.



### Text and Tabular Data

The literature focuses on feature visualization for convolutional neural networks for image recognition.
Technically, there is nothing to stop you from finding the input that maximally activates a neuron of a fully connected neural network for tabular data or a recurrent neural network for text data.
You might not call it feature visualization any longer, since the "feature" would be a tabular data input or text.
For credit default prediction, the input might be the number of prior credits, number of mobile contracts, address and dozens of other features.
The learned feature a neuron would then be a certain combination of the dozens of features.
For recurrent neural networks, it is a bit nicer to visualize what the network learned:
In [^viz-rnn], the authors showed that recurrent neural networks indeed have neurons that learn interpretable features.
They trained a character-level model, i.e.  predict the next character in the sequence from the previous character.
Once an opening brace "(" occurred, one of the neurons got highly activated, and got de-activated when the matching ")" closing bracket occurred.
Other neurons fired at the end of a line.
Some neurons fired in URLs.
The difference to the feature visualization for CNNs is that the examples have not been maximized, but rather the activations of neurons were studied for given text. 

### Advantages

- Feature visualizations give **unique insight into the working of neural networks**, especially for image recognition.
Given the complexity and opacity of neural networks, feature visualization is an important step in analyzing and describing neural networks.
Through feature visualization, we learned that the neural network first learns simple edge and texture detectors and more abstract parts and objects in higher layers.
- Feature visualization is a great tool to **communicate in a non-technical way how neural networks work**.
The image provide a quick intuition how a series of mathematical operations can derive a class probability from raw input pixels.
-  Feature visualization can be **combined with feature attribution methods**, which explain which pixels were important for the classification.
The combination of both methods allows to explain an individual classification along with local visualization of the learned features that were involved in the classification.
See [The Building Blocks of Interpretability from distill.pub](https://distill.pub/2018/building-blocks/).
  To explain why a certain prediction was made, we can combine the regional attribution of image parts and the channels that are most activated by those parts to get a better understanding what the network is looking for.
- And finally, feature visualizations make great desktop wallpaper or prints for **T-Shirts**.

### Disadvantages

-**Illusion of interpretability?**
Many images look interpretable:
We think it shows a certain concept a tennis ball. 
But then again, we only interpret it to be a tennis ball.
The neural network might react to something different.
Showing images from the training data that also activate that unit helps, but not always.
But there are other issues, which makes it difficult to really understand what's going on in the network.
If we minimize the activation instead of maximizing, a completely different blend of concepts might occur.
Several neurons or layers might respond (activate) for the same patterns or images.
All the neurons interact, which is difficult to grasp, and feature visualization can be used to describe the interactions, but there are just too many interactions to look at.
All in all, there is a good amount of guessing and simplification involved in feature visualization, so we should not over-interpret these images.
- There are **too many units to look at**, even when "only" visualizing the channel activations.
  For Inception V1 there are already over 5000 channels from 9 conv layers.
If you also want to show the negative activations plus a few images from the training data that maximally or minimally activate the channel (let's say 4 positive, 4 negative images), then you must already display more than 50 000 images.
  And you have not even started to investigate random directions or created a diverse set of multiple images for the channel activations.
- Many feature visualization images are not interpretable at all, but contain some abstract concept for which we have no words or concept.
  The display of feature visualizations along with training data can help.
  But it still might not reveal what the neural network reacted to and only show something like "maybe there has to be something yellow in that image".
- **Limited usefulness for applied settings**.
Feature visualizations are not practical for daily work.
There are some difficulties to get them to work.
And also what do they tell you in the end?
Feature visualization gives you a rough sense what the network is doing in general.
Especially to study neural networks it is a great tool.
But I do not see how feature visualization would help in a concrete image classification task, such as a classifier that identifies faulty products on an assembly line.

### Software and Further Informations

There is an open-source implementation of feature visualization called [Lucid](https://github.com/tensorflow/lucid).
You can conveniently try it in your browser by using the notebook links that are provided on the Lucid Github page.
No additional software is required.

If you want to dive a lot deeper into feature visualization, take a look at the distill.pub online journal, especially the feature visualization post by Olah et al. [^distill-fv], from which I used many of the images, and also about the building blocks of interpretability [^distill-blocks].


[^distill-fv]: Olah, et al., "Feature Visualization", Distill, 2017.

[^distill-blocks]: Olah, et al., "The Building Blocks of Interpretability", Distill, 2018.

[^plugandplay]: Nguyen, Anh, et al. "Plug & play generative networks: Conditional iterative generation of images in latent space." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.

[^synthesize]: Nguyen, Anh, et al. "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks." Advances in Neural Information Processing Systems. 2016.

[^viz-rnn]: Karpathy, Andrej, Justin Johnson, and Li Fei-Fei. "Visualizing and understanding recurrent networks." arXiv preprint arXiv:1506.02078 (2015).

[^imagenet]: Olga Russakovsky*, Jia Deng*, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei. (* = equal contribution) ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015


