<!--
Some literature
- For feature attribution: http://blog.qure.ai/notes/deep-learning-visualization-gradient-based-methods
- http://blog.qure.ai/notes/visualizing_deep_learning
-->
## Feature Activation Maximization Visualization {#feature-visualization}

Neural feature visualization creates data inputs that maximally active different units of the neural network.
By that, feature visualization tries to show which (abstract) features the network learned.

<!-- Background: Why feature visualization -->
One of the biggest strengths of deep neural networks is that they automatically learn more abstract features from the input features in the hidden layers.
Thispartially reduces the need for feature engineering.
The integrated feature learning part is especially remarkable for image and text data.
Let's say you want to build an image classifier using a support vector machine.
The raw pixels are not the best input to your SVM, so you create new features based on color, frequency domain, edge detectors and so on and train your machine learning model with it.
With convolutional neural networks, the image is passed into the network in its raw form (pixels), transformed multiple times first through the convolutional layers and then the fully connected layers and the network spits out the predictions.
During training, the convolutional neural network learns new features in its layers.

```{r fig.cap = "Architecture of Inception V1 neural network. Each enumerated unit (3a to 5b) represents a layer with differently sized convolutions and pooling. Figure from Olah, et al. 2019 (CC-BY 4.0) https://distill.pub/2017/activation-atlas/.", out.width = 800, include = FALSE}
knitr::include_graphics("images/inceptionv1.svg")
```
```{r fig.cap = "Features learned by a convolutional neural network (Inception V1) trained on the ImageNet data. The features range from low-level in the lower convolutional layers (left) to high-level in the higher convolutional layers (right). Figure from Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/appendix/.", out.width = 800}
knitr::include_graphics("images/cnn-features.png")
```

- The first convolutional layer(s) learn low-level features like edges and simple textures.
- Later convolutional layers learn mid-level features like different textures and patterns.
- Higher up convolutional layers learn high-level features like objects or parts of objects.
- The fully connected layers learn to connect the activations from the high-level features to the individual classes to be predicted.

Cool.
But how do we actually get those hallucinatory images?

<!-- Feature Visualization explained-->
The approach of making the learned features explicit is called **Feature Visualization**.
Feature visualization for a unit of the neural network is done by finding the input that maximizes the activation of that unit.
"Unit" refers to either individual neurons, whole feature maps, whole (convolutional) layers or the final class probability in classification.
A good choice of unit for feature visualization are the channels, also called activiation maps of the convolution layers.

```{r units, fig.cap="Features visualization can be done for different units. A) Convolution neuron, B) Convolution channel, C) Convolution layer, D) Neuron, E) Hidden layer, F) Class probability neuron", out.width=800}
knitr::include_graphics("images/units.jpg")
```


```{r trippy, fig.cap="Optimized images for Inception V1 (channels mixed3a, mixed4c, mixed4d and mixed5a). Images are maximized for a random direction of the activations. Figure from Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/.", out.width=800, include = FALSE}
knitr::include_graphics("images/trippy.png")
```

Mathematically, feature visualization is an optimization problem.
We assume that the weights of the neural networks are fixed, meaning that the network is fully trained.
We are looking for a new image $img^*$ that maximize the (mean) activation of a unit, here a single neuron:

$$img^*=\arg\max_{img}h_{n,x,y,z}(img)$$

The function $h$ is the activation of a neuron, x the input to the network (e.g. an image), x and y describe the spatial position of the neuron, n enumerates the layers and z is the channel index.
For the mean activation of a whole channel z in layer n we maximize:

$$img^*=\arg\max_{img}\sum_{x,y}h_{n,x,y,z}(img)$$

This weighs the activation all neurons in channel z equally.
Alternatively, you could also maximize random directions, which means that the neurons would be differently weighted.
Instead of maximizing the activation, you can also minimize the activation (which is the same as maximizing the negative direction).
Interestingly, when you do that you get very different features for the same unit:

```{r``{r pos-neg, fig.cap="Positive (left) and negative (right) activation of Inception V1 neuron 484 from layer mixed4d pre relu. While the neuron is maximally activated by wheels, something weird (which seems to have eyes) brings a negative activation. Code: https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/feature-visualization/negative_neurons.ipynb", out.width=800}
knitr::include_graphics("images/a484.png")
```

We can tackle this optimization problem in different ways.
First, why should we generate new images, we could simple try out all the images in our training data and choose the ones that maximizes the activation.
That's a valid approach, but training data has the problem that elements on the image can be correlated and we don't get to see what the neural network is really looking for.
If all the images that have a high activation of a certain channel show a dog and a tennis ball, we don't know whether the neural network looks at the dog, the tennis ball or maybe at both.

The other approach is to generate a new image, starting from random noise.
To get meaningful results, there are usually constraints on the image *img*, e.g. that only small changes are allowed.
To reduce noise in the feature visualization, you can apply jittering, rotation or scaling to the image before the optimization step.

```{r``{r activation-optim, fig.cap="Iterative optimization from random image to maximizing activation. Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/.", out.width=800}
knitr::include_graphics("images/activation-optim.png")
```
More sophisticated approach involve generative adversarial networks that generate image that are coherent with the training data, but specifically optimizied for the analyzed unit.

- TODO: Cite GAN approach
- TODO: Cite one of the first approaches, or the standard way

<!-- Google DeepDream 
Google DeepDream takes an image, extracts the activation from one and adds it to the image.
Start with an image, feed it to a pretrained nn, choose a layer.
During the forward pass until the chosen layer, extract the gradients.
We want to maximize for the given image the activation of the chosen layer, so we add the gradient to the image.
A few techniques like Gaussian blurring are applied to improve the resulting images.
-->


**Connection to Adversarial Examples**

<!-- Connection to Adversarial Examples -->
There is a connection between feature visualization and [adversarial examples](#adversarial):
Both techniques maximize the activation of a neural network unit.
For adversarial examples we search the maximum activation of the neuron after the softmax for the adversarial (= incorrect) class.
A difference is the image we start with:
For adversarial examples, it's obviously the image for which we want to generate the adversarial image.
For feature visualization it's 

Also the optimization has an additional constraint: Don't change too much from the initial image.
<!--
**Why not visualize the weights?**

An alternative would be to simply visualize the weights, e.g. the weights that are learned in convolutional layer by CNNs.
This could also tell us whether it looks for edges or not.
But for more complex concepts it does not help us and it is more helpful to look at images that maximize some activation.
I just explained feature visualization via looking at the activation of units of the network for certain inputs.
The other option would be to visualize the weights directly.
Why don't we simply print out or visualize the learned weights?
Well, there are often millions of them, interacting in non-linear ways with each other.
It does not scale.
Also it is not on the same space as the feature input.
It is more difficult what the weights really mean and more intuitive to understand what input maximizes the activation of certain neurons in the neural network.
-->


<!-- How useful are they? -->
These images look intepretable, we may detect that one of them shows a certain concept, like a tennis ball. 
But then again, we interpret it to be a tennis ball, the same as we interpret art.
It could be something else, and actually you might see something different on the image as well.
A blend of concepts.

If we go the other direction and minimize activation, a completely different blend of concepts might occur.
There can be multiple neurons or layers that respond (activate) for the same patterns or images.
See for example how many images you have to look at for Google Net here.
And these are only for optimizing the channels, not the individual neurons.
https://distill.pub/2017/feature-visualization/appendix/
Keep in mind that images can be unstable and it is recommend to look at multiple images per unit that is looked at.

To not get these images with repetitive patterns (which are more like adversarial examples than real life images), many researchers applied some regularization or some constraint.
Three options for regularization frequency penalization, transformation robustness, learned priors (e.g. GANs).
Or, of course, you only search through available, real images.



**Feature Visualization for Text and Tabular Data**

The literature focuses mainly on images.
The concept of activation maximization works for text and tabular data as well, but is less visually interesting.
For text, you would get a text that maximially activates an RNN.

For tabular data, you would get a data point that maximally activates a (usually) fully connected neural network.
This data point then has some features, is probably going to be out of range of the data distribution.


If you want to dive a lot deeper into feature visualization, check out the distil.pub blog/journal, especially the feature visualization post by Olah et al. [^distill-fv] from which I got the images, and also about the building blocks of interpretability [^distill-blocks].

### Advantages
- Tool for communication: Someone who does not understand neural networks can look at those and at least get an intuition about the neural network learning more and more complex features.
- Gives unique insight into neural networks, especially for image recognition
- Make a T-Shirt from it or a dreamy background image.
- Really great to learn about feature visulization. makes clearer how nns work. For example it showed that CNNs look more on the texture, less on the shapes: I MAGE N ET - TRAINED CNN S ARE BIASED TOWARDS TEXTURE; INCREASING SHAPE BIAS IMPROVES  ACCURACY AND ROBUSTNESS
- Feature visualization are Great Desktop Wallpaper or for T-Shirts
- Can be combined with feature attribution to get a better tool, see here: https://distill.pub/2018/building-blocks/.
Feature attributions are explained here: TODO: LINK CHAPTER

### Disadvantages
- Restricted usefunless in day-to-day job
- Many units to look at
- For whole channels: Multiple directions in which to look (either all weighted the same, or different random directions)
- Also, you can negatively activate it.
- There is no local maxima, so running this multiple times, from different inputs can yield different results (TODO: Check)
- A bit like a horoscope, because there are multiple possible interpretation.
  Reminds me of school: Interpret the shit out of a snippet of text, trying to figure out what the author meant.
  And usually this explanation becomes its own thing, independent of the actual author intention.
  Same here, because the unit interacts with other unit, not only maximal activation is interesting.
- Feature Visualization: Not sure how useful it is
- feature visualization are not practical for daily work and difficult to get it to work.
- feature visualization: Maximum activation only small part of the space. 
  Negative activation exists as well, what about slightly postive activation? 
  We would have to describe the whole spectrum to describe that neuron.
- too many neurons to look at for feature visualization
- what does it tell you in the end? only sense of understanding, but final prediction is still a difficult interaction between all those neurons, and these images only show the maximal activation, but neurons might only be have activated for some specific prediction, and it is unclear if that image really helps you then
- feature visualizations are like understanding modern art, always trying to interpret what artist intended
- The thing that maximally activates a neuron or channel can be completely unrelated from the patterns that minimize it


Quite many disadvantages.
Which speaks for the difficulties with this technique.
But invaluable element for furthering our understanding how deep learning works.
Just don't interpret too much the individual images.

Better understand when and why they might fail.

- CNNs
 - learn mostly textures, shapes not so much
 - Nets learn similar things. 


### Software

You can use the open-source implementation [Lucid](https://github.com/tensorflow/lucid) to produce feature visualizations.
The cool thing is, you can try all the stuff within your browser, by using the notebook links that are provided on the Lucid github page.


[^imagenet]: Olga Russakovsky*, Jia Deng*, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei. (* = equal contribution) ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015


[^distill-fv]: Olah, et al., "Feature Visualization", Distill, 2017.

[^distill-blocks]: Olah, et al., "The Building Blocks of Interpretability", Distill, 2018.

[^visual-survey-frontier]: Hohman, Fred Matthew, et al. "Visual analytics in deep learning: An interrogative survey for the next frontiers." IEEE transactions on visualization and computer graphics (2018).
 
[^visual-survey]: Zhang, Quan-shi, and Song-Chun Zhu. "Visual interpretability for deep learning: a survey." Frontiers of Information Technology & Electronic Engineering 19.1 (2018): 27-39.


