```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```

## Linear Regression {#limo}

A linear regression model predicts the target as the weighted sum of the feature inputs.
The linearity of the learned relationship makes  interpretation easy.
Linear regression models have long been used by statisticians, computer scientists and other people who tackle quantitative problems.


Linear models can be used to model the dependence of a regression target (y) on some features (x).
The learned relationships are linear and can be written for a single instance i as follows:

$$y_{i}=\beta_{0}+\beta_{1}x_{i1}+\ldots+\beta_{p}x_{ip}+\epsilon_{i}$$

The predicted outcome of the i-th instance is a weighted sum of its p features.
The betas ($\beta_{j}$) represent the learned feature weights or coefficients.
The epsilon ($\epsilon_{i}$) is the error we still make, i.e. the difference between the prediction and the actual outcome.
These errors are assumed to follow a Gaussian distribution, which means that we make errors in both negative and positive directions and make many small errors and few large errors.

Various methods can be used to estimate the optimal weight vector $\hat{\boldsymbol{\beta}}$.
The ordinary least squares method is usually used to find the weights that minimize the squared differences between the actual and the estimated outcomes:

$$\hat{\boldsymbol{\beta}}=\arg\!\min_{\beta_0,\ldots,\beta_p}\sum_{i=1}^n\left(y_i-\left(\beta_0+\sum_{j=1}^p\beta_jx_{ij}\right)\right)^{2}$$

We will not discuss in detail how the optimal weights can be found, but if you are interested, you can read chapter 3.2 of the book "Elements of Statistical Learning" (Hastie, Tibshirani, and Friedman 2009)[^Hastie] or one of the other zillions of sources on linear regression models.

The biggest advantage of linear regression models is linearity:
It makes the estimation procedure simple and most importantly, these linear equations have an easy to understand interpretation on a modular level (i.e. the weights).
This is one of the main reasons why the linear model and all similar models are so widespread in academic fields such as medicine, sociology, psychology, and many other quantitative research fields.
For example, in the medical field,  it is not only important to predict the clinical outcome of a patient, but also to quantify the influence of the drug and at the same time take things like sex, age, and other features into account in an interpretable way.

Estimated weights come with confidence intervals.
A confidence interval is a range for the weight estimate that covers the 'true' parameter with a certain confidence.
For example, a 95% confidence interval for a weight of 2 could range from 1 to 3.
The interpretation of this interval would be:
If we repeated the estimation 100 times with newly sampled data, the confidence interval would include the true weight in 95 out of 100 cases, given that the linear regression model is the correct model for the data.

Whether the model is the "correct" model depends on whether the relationships in the data meet certain assumptions, which are:
Linearity, normality, homoscedasticity, independence, fixed features, and absence of multicollinearity.

- **Linearity**:
The linear regression model forces the prediction to be a linear combination of features, which is both its greatest strength and its greatest limitation.
Linearity leads to interpretable models:
Linear effects are easy to quantify and describe and additive, so it is easy to separate the effects.
If you suspect feature interactions or a nonlinear association of a feature with the target value, you can add interaction terms and use techniques like regression splines to estimate nonlinear effects.
- **Normality**:
It is assumed that the target outcome given the features follows a normal distribution.
If this assumption is violated, the estimated confidence intervals of the feature weights are invalid.
- **Homoscedasticity** (constant variance):
The variance of the error terms $\epsilon_{i}$ is assumed to be constant over the einter feature space.
Suppose you want to predict the value of a house given the living area in square meters.
You estimate a linear model that assumes that regardless of the size of the house, the error around the predicted response has the same variance.
This assumption is often violated in reality.
In the house example, it is plausible that the variance of error terms around the predicted price is higher for larger houses, since prices are higher and there is more room for price fluctuations.
Suppose the average error (difference between predicted and actual price) in your linear regression model is 50,000 Euros.
If you assume homoscedasticity, you assume that the average error of 50,000 is the same for houses that cost 1 million and for houses that  cost only 40,000, which is unreasonable as it would mean that we can expect some negative house prices.
- **Independence**:
It is assumed that each instance is independent of the next.
If you perform repeated measurements, such as multiple blood tests per patient, the data points are not independent.
For dependent data you need special linear regression models, such as mixed effect models or GEEs.
If you use the "normal" linear regression model, you might draw wrong conclusion from the model.
- **Fixed features**:
The input features are considered 'fixed'.
Fixed means that they have no statistical variance and are free of measurement errors.
This is a rather unrealistic assumption.
Without that assumption, however, you would have to fit very complex measurement error models that account for the measurement errors of your input features.
And usually you don't want to do that.
- **Absence of multicollinearity**: Basically you don't want features to be highly correlated, because this messes up the estimation of the weights.
In a situation where two features are highly correlated (something like correlation > 0.9) it will become problematic to estimate the weights, since the feature effects are additive and it becomes indeterminable to which of the correlated features to attribute the effects.

### Interpretation
The interpretation of a weight in the linear regression model depends on the type of the corresponding feature:

- Numerical feature: Increasing the numerical feature $x_{j}$ by one unit changes the estimated outcome by its weight $\beta_{j}$. 
An example of a numerical feature is the size of a house.
- Binary feature: A feature that takes one of two possible values for each instance.
An example is the feature "House comes with a garden".
One of the values counts as the reference category (in some programming languages encoded with 0), such as "No garden".
Changing the feature $x_{j}$ from the reference category to the other category changes the estimated outcome by $\beta_{j}$.
- Categorical feature with multiple categories:
A feature with a fixed number of possible values.
An example is the feature "Floor type", with possible categories "carpet", "laminate" and "parquet".
A solution to deal with many categories is the one-hot-encoding, meaning that each category has its own binary column.
For a categorical feature with l categories, you only need l-1 columns, because the l-th column would have redundant information (e.g. when columns 1 to l-1 all have value 0 for an instance, we know that the instance has category l).
The interpretation for each category is then the same as the interpretation for binary features.
Some languages, such as R, allow you to encode categorical features in various ways, [described here](#cat-code).
- Intercept $\beta_{0}$: 
The intercept is the feature weight for the constant feature, which is always 1 for all instances.
Most software packages automatically add this "1"-feature to estimate the intercept.
The interpretation is:
For an instance with all numerical feature values at zero and the categorical feature values at the reference category, the estimated outcome is the intercept weight $\beta_{0}$.
The interpretation of the intercept $\beta_{0}$ is usually not relevant because instances with all features values at zero often make no sense.
The interpretation is only meaningful when the features have been standardised (mean of zero, standard deviation of one).
Then the intercept $\beta_0$ reflects the predicted outcome of an instance where all features are at their mean value.

The interpretation of the features in the linear regression model can be automated by using following text templates.

**Interpretation of a Numerical Feature**

An increase of feature $x_{k}$ by one unit increases the expectation for y by $\beta_k$ units, given all other feature values remain fixed.

**Interpretation of a Categorical Feature**

Changing feature $x_{k}$ from the reference category to the other category increases the expectation for y by $\beta_{k}$, given all other features remain fixed.

Another important measurement for interpreting linear models is the R-squared measurement.
R-squared tells you how much of the total variance of your target outcome is explained by the model.
The higher R-squared, the better your model explains the data.
The formula for calculating R-squared is: 

$$R^2=1-SSE/SST$$

SSE is the squared sum of the error terms:

$$SSE=\sum_{i=1}^n(y_i-\hat{y}_i)^2$$ 

SST is the squared sum of the data variance:

$$SST=\sum_{i=1}^n(y_i-\bar{y})^2$$

The SSE tells you how much variance remains after fitting the linear model, which is measured by the squared differences between the predicted and actual target values.
SST is the total variance of the target outcome.
R-squared tells you how much of your variance can be explained by the linear model.
R-squared ranges between 0 for models where the model doens't explain the data at all and 1 for models that explain all of the variance in your data.

There is a catch, because R-squared increases with the number of features in the model, even if they don't contain any information about the target value at all.
Therefore, it is better to use the adjusted R-squared ($\bar{R}^2$), which accounts for the number of features used in the model.
Its calculation is 

$$\bar{R}^2=R^2-(1-R^2)\frac{p}{n-p-1}$$

where p is the number of features and n the number of instances.

It isn't meaningful to interpret a model with very low (adjusted) R-squared, because the model basically does not explain much of the variance, so any interpretation of the weights are is meaningful.


**Feature Importance**

The importance of a feature in a linear regression model can be measured by the absolute value of its t-statistic.
The t-statistic is the estimated weight scaled with its standard error.

$$t_{\hat{\beta}}=\frac{\hat{\beta}}{SE(\hat{\beta})}$$

Let's examine what this formula tells us:
The importance of a feature increases with increasing weight.
This makes sense.
The more variance the estimated weight has (= the less certain we are about the correct value), the less important the feature is.
This also makes sense.

###  Example

In this example we use the linear regression model to predict the [number of rented bikes](#bike-data) on a day, given weather and calendar information.
For the interpretation we examine the estimated regression weights.
The features consist of numerical and categorical features.
For each feature, the table shows the estimated weight, the standard error of the estimate, and the absolute value of the t-statistic (feature importance).

```{r linear_model}

data(bike)
X = bike[bike.features.of.interest]
y = bike[,'cnt']
dat = cbind(X, y)

mod = lm(y ~ ., data = dat, x = TRUE)
lm_summary = summary(mod)$coefficients

lm_summary_print = lm_summary
lm_summary_print[,'t value'] = abs(lm_summary_print[,'t value'])
rownames(lm_summary_print) = pretty_rownames(rownames(lm_summary_print))

kable(lm_summary_print[,c('Estimate', 'Std. Error', 't value')], digits = 1, col.names = c('Weight estimate', 'Std. Error', "abs(t)"))
```

Interpretation of a numerical feature (temperature):
An increase of the temperature by 1 degree Celsius increases the expected number of bicycles by `r sprintf('%.1f', lm_summary_print['temp', 'Estimate'])`, when all other features remain fixed.

Interpretation of a categorical feature ('weathersituation')):
The estimated number of bicycles is `r sprintf('%.1f', lm_summary_print['weathersitRAIN/SNOW/STORM', 'Estimate'])` lower when it is raining, snowing or stormy, compared to good weather - again assuming that all other features don't change.
When the weather is misty, the expected number of bicycles is `r sprintf('%.1f', lm_summary_print['weathersitMISTY', 'Estimate'])` lower compared to good weather, given all other features remain the same.

All the interpretations always come with the footnote that 'all other features remain the same'.
This is because of the nature of linear regression models:
The predicted target is a linear combination of the weighted features.
The estimated linear equation is a hyperplane in the feature/target space (a simple line in the case of a single feature).
The $\beta$-weights specify the slope (gradient) of the hyperplane in each direction.
The good side is that the additivity isolates the interpretation of an individual feature effect from all other features.
That's possible because all the feature effects (= weight times feature value) in the equation are combined with a plus. 
On the bad side of things, the interpretation ignores the joint distribution of the features.
Increasing one feature, but not changing another, can lead to unrealistic or at least unlikely data points.
For example increasing the number of rooms might be unrealistic without also increasing the size of an apartment.

### Visual Interpretation
Various visualizations make the linear regression model easy and quick to grasp for humans.

#### Weight Plot
The information of the weight table (weight and variance estimates) can be visualized in a weight plot.
The following plot shows the results from the previous linear regression model:

```{r linear-weights-plot, fig.cap="The weights are displayed as points and the 95 percent confidence intervals as lines. A 95 percent confidence interval means that if the linear model were estimated 100 times with similar data, in 95 out of 100 cases the confidence interval would cover the true weight. Assuming that the linear model assumptions hold."}
coef_plot(mod)
```

The weight plot shows that rainy/snowy/stormy weather has a strong negative effect on the predicted number of bikes.
The weight of the working day feature is close to zero and zero is included in the 95% interval, which means that the effect is not statistically significant.
Some confidence intervals are very short and the estimates are close to zero, yet the features effects were statistically significant.
Temperature is one such candidate.
The problem with the weight plot is that the features are measured on different scales.
While for the weather the estimated $\beta$ reflects the difference between good and rainy/storm/snowy weather, for temperature it only reflects an increase of 1 degree Celsius.
You can make the estimated weights more comparable by scaling the features to zero mean and unit standard deviation before fitting the linear model.

#### Effect Plot
The weights of the linear regression model can be more meaningfully analyzed when they are multiplied by the actual feature values.
The weights depend on the scale of the features and will be different if you have a feature that measures e.g. a person's height and you switch from meter to centimeter.
The weight will change, but the actual relationships in your data will not.
It is also important to know the distribution of your feature in the data, because if you have a very low variance, it means that almost all instances have similar contribution from this feature.
The effect plot can help you understand how much the combination of weight and feature contributes to the predictions in your data.
Start by calculating the effects, which is the weight per feature times the feature value of an instance:
$$\text{effect}_{i,j}=w_{j}x_{i,j}$$

The resulting effects can be visualized with boxplots.
A box in a boxplot contains the effect range for half of your data (25% to 75% effect quantiles).
The vertical line in the box is the median effect, i.e. 50% of the instances have a lower and the other half a higher effect on the prediction.
The horizontal lines extend to$\pm1.58\text{IQR}/\sqrt{n}$, with IQR being the inter quartile range ($q_{0.75}-q_{0.25}$).
The dots are outliers.
The categorical feature effects can be summarized in a single boxplot, compared to the weight plot, where each category has its own row.

```{r linear-effects, fig.cap="The feature effect plot shows the distribution of effects (= feature value times feature weight) across the dataset for each feature."}
effect_plot(mod, dat)
```

The largest contributions to the expected number of rented bikes comes from the temperature feature and the days feature, which captures the trend of bike rentals over time.
The temperature has a broad range of how much it contributes to the prediction.
The day trend feature goes from zero to large positive contribution, because the first day in the dataset (01.01.2011) has a very small trend effect and the estimated weight for this feature is positive (`r sprintf('%.2f', lm_summary_print['days_since_2011', 'Estimate'])`).
This means that the effect increases with each day and is highest for the last day in the dataset (31.12.2012).
Note that for effects with a negative weight, the instances with a positive effect are those that have a negative feature value.
For example, days with a high negative effect of windspeed are the stormy ones.

### Explain Individual Predictions
```{r linear-effects-single-preparation}
i = 6
effects = get_effects(mod, dat)
predictions = predict(mod)

effects_i = tidyr::gather(effects[i, ])
predictions_mean = mean(predictions)
# For proper indexing, names have to be removed
names(predictions) = NULL
pred_i = predictions[i]
```

How much has each feature of an instance contributed to the prediction?
This can be answered by computing the effects for this instance.
An interpretation of instance-specific effects only makes sense in comparison to the distribution of the effect for each feature.
We want to explain the prediction of the linear model for the `r i`-th instance from the bicycle dataset.
The instance has the following feature values.

```{r linear-effects-single-table}
df = data.frame(feature = colnames(bike), value = t(bike[i,]))
colnames(df) = c("feature", "value")
kable(df, col.names = c("Feature", "Value"), row.names = FALSE)
```

To obtain the feature effects of this instance, we have to multiply its feature values by the corresponding weights from the linear regression model.
For the value `r df["workingday", "value"]` of feature `r df["workingday", "feature"]`, the effect is, for example, `r lm_summary_print[paste(df["workingday", "feature"], df["workingday", "value"], sep = ""), "Estimate"]`.
For the value `r df["temp", "value"]` of feature `r df["temp", "feature"]`, the effect is `r as.numeric(as.character(df["temp", "value"])) * lm_summary_print[as.character(df["temp", "feature"]), "Estimate"]`.
We add these individual effects as dots to the effect plot, which shows us the distribution of the effects in the data.
This allows us to compare the individual effects with the distribution of effects in the data. 

```{r linear-effects-single, fig.cap="The effect plot for one instance shows the effect distribution and highlighting the effects of the instance of interest."}
i = 6
effects = get_effects(mod, dat)
predictions = predict(mod)

effects_i = tidyr::gather(effects[i, ])
predictions_mean = mean(predictions)
# For proper indexing, names have to be removed
names(predictions) = NULL
pred_i = predictions[i]

effect_plot(mod, dat) +
  geom_point(aes(x=key, y=value), color = 'red', data = effects_i, shape = 4, size=2) +
  ggtitle(sprintf('Predicted value for instance: %.0f\nAverage predicted value: %.0f\nActual value: %.0f', pred_i, predictions_mean, y[i]))
```

If we average the predictions for the training data instances, we get an average of `r round(predictions_mean, 0)` (). 
In comparison, the prediction of the `r i`-th instance is small, since only `r round(pred_i, 0)` bicycles are predicted.
The effect plot reveals the reason.
The boxplots show the distributions of the effects for all instances of the dataset, the red markers show the effects for the `r i`-th instance.
The `r i`-th instance has a low temperature effect because on this day the temperature was `r round(X[i, 'temp'],0)` degrees, which is low compared to most other days (and remember that the weight of the temperature feature is positive).
Also, the effect of the trend feature "days_since_2011" is small compared to the other data instances because this instance is from early 2011 (`r  X[i, 'days_since_2011']` days) and the trend feature also has a positive weight.




### Encoding of Categorical Features {#cat-code}
There are several ways to encode a categorical feature, and the choice influences the interpretation of the $\beta$-weights.

The standard in linear regression models is treatment coding, which is sufficient in most cases.
Using different encodings boils down to creating different matrices (=design matrix) from a single column with the categorical feature.
This section presents three different encodings, but there are many more.
The example used has six instances and a categorical feature with 3 categories.
For the first two instances, the feature takes category A;
for instances three and four, category B;
and for the last two instances category C.

**Treatment coding**: 

The $\beta$ per category is the estimated difference in y to the reference category.
The intercept of the linear model is the mean of the reference category (given all other features remain the same).
The first column of the design matrix is the intercept, which is always 1.
Column two indicates whether instance i is in category B, column three indicates whether it is in category C.
There is no need for a column for category A, because then the linear equation would be overspecified and no unique solution for the weights can be found.
It is sufficient to know that an instance is neither in category B or C.

Feature matrix: $$\begin{pmatrix}1&0&0\\1&0&0\\1&1&0\\1&1&0\\1&0&1\\1&0&1\\\end{pmatrix}$$

**Effect coding**: 

The $\beta$ per category is the estimated y-difference from the category to the overall mean (again, given all other features are zero or the reference category).
The first column is used to estimate the intercept.
The weight $\beta_{0}$ associated with the intercept represents the overall mean and $\beta_{1}$, the weight for column two, is the difference between the overall mean and category B.
The total effect of category B is $\beta_{0}+\beta_{1}$.
The interpretation for category C is equivalent.
For the reference category A, $-(\beta_{1}+\beta_{2})$ is the difference to the overall mean and $\beta_{0}-(\beta_{1}+\beta_{2})$ the overall effect.

Feature matrix: $$\begin{pmatrix}1&-1&-1\\1&-1&-1\\1&1&0\\1&1&0\\1&0&1\\1&0&1\\\end{pmatrix}$$

**Dummy coding**: 

The $\beta$ per category is the estimated mean value of y for each category (given all feature are at zero or reference category).
Note that the intercept was has been omitted here so that a unique solution can be found for the linear model weights.

Feature matrix: $$\begin{pmatrix}1&0&0\\1&0&0\\0&1&0\\0&1&0\\0&0&1\\0&0&1\\\end{pmatrix}$$

If you want to dive a little deeper into the different encodings of categorical features, checkout [this webpage](http://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/) and
[this blog post](http://heidiseibold.github.io/page7/).


### Do Linear Models Create Good Explanations?

Judging by the attributes that constitute a good explanation, as presented [in the Human-friendly Explanations chapter](#good-explanation), linear models don't create the best explanations.
They are contrastive, but the reference instance is a data point where all numerical features are zero and the categorical features are at their reference categories.
This is usually an artificial, meaningless instance that is unlikely to occur in your data or reality.
There is an exception:
If all numerical features are mean centered (feature minus mean of feature) and all categorical features are effect coded, the reference instance is the data point where all the features take on the mean feature value.
This might also be a non-existent data point, but it might be at least more likely or meaningful.
In this case, the $\beta$-values times the feature values (feature effects) explain the contribution to the predicted outcome contrastive to the "mean-instance".
Another aspect of a good explanation is selectivity, which can be achieved in linear models by using less features or by fitting sparse linear models.
But by default, linear models don't create selective explanations.
Linear models create truthful explanations, as long as the linear equation is an appropriate model for the relationship between features and outcome.
The more nonlinearities and interactions there are, the less accurate the linear model will be and the less truthful the explanations become.
Linearity makes the explanations more general and simpler.
The linear nature of the model, I believe, is the main factor why people like linear models for explaining relationships.


###  Sparse Linear Models {#sparse-linear}
The examples of the linear models that I have chosen all look nice and neat, don't they?
But in reality you might not have just a handful of features, but hundreds or thousands.
And your normal linear models?
Interpretability goes downhill.
You might even find yourself in a situation where there are more features than instances, and you can't fit a standard linear model at all.
The good news is that there are ways to introduce sparsity (= few features) into linear models.

#### Lasso {#lasso}

Lasso is an automatic and convenient way to introduce sparsity into the linear regression model.
Lasso stands for "least absolute shrinkage and selection operator" and, when applied for a linear model, performs feature selection and regularization of the selected feature weights.
Let's consider the minimization problem that the $\beta$s optimize:

$$min_{\boldsymbol{\beta}}\left(\frac{1}{n}\sum_{i=1}^n(y_i-x_i^T\boldsymbol{\beta})^2\right)$$

Lasso adds a term to this optimization problem:

$$min_{\boldsymbol{\beta}}\left(\frac{1}{n}\sum_{i=1}^n(y_i-x_i^T\boldsymbol{\beta})^2+\lambda||\boldsymbol{\beta}||_1\right)$$

The term $||\boldsymbol{\beta}||_1$, the L1-norm of the feature vector,  leads to a penalization of large  $\boldsymbol{\beta}$-values.
Since the L1-norm is used, many of the weights for the features receive an estimate of 0 and the others are shrunk.
The parameter lambda ($\lambda$) controls the strength of the regularizing effect and is usually tuned by cross-validation.
Especially when lambda ($\lambda$) is large, many weights become 0.

The feature weights can be visualized as a function of the penalty term lambda.
Each feature weight is represented by a curve in the following figure.

```{r lasso-path, fig.cap="With increasing penalty of the weights, fewer and fewer features receive a non-zero weight estimate. These curves are also called regularization paths."}
library("glmnet")
X.d = model.matrix(y ~ . -1, data = X)
l.mod = glmnet(X.d, y)
plot(l.mod,  xvar = "lambda")
```

What value should we choose for $\lambda$?
If you see the regularization parameter as a tuning parameter, then you can find the optimal  $\lambda$ that minimizes the model error with cross-validation.
You can also consider lambda ($\lambda$) as a parameter to control the interpretability of the model.
The larger the penalization, the fewer features are present in the model (because their weight is zero) and the better the model can be interpreted.

**Example with LASSO**

We will predict bicycle rentals using LASSO. 
We set the number of features we want to have in the model beforehand.
Let's first set the number to 2 features:

```{r lasso_effects}
extract.glmnet.effects = function(betas, best.index) {
  data.frame(beta = betas[, best.index])
}
n.features = apply(l.mod$beta, 2, function(x){sum(x!=0)})
kable(extract.glmnet.effects(l.mod$beta, max(which(n.features == 2))), col.names = "Weight", digits = 2)
```

The first two features with non-zero weights in the lasso path are temperature ("temp") and the time trend ("days_since_2011").

Now, let's select 5 features:

```{r lasso_effects2}
kable(extract.glmnet.effects(l.mod$beta, max(which(n.features == 5))), col.names = "Weight", digits = 2)
```


Note that the weights for "temp" and "days_since_2011" differ from the model with two features.
The reason for this is that by decreasing $\lambda$ even features that are already "in" the model are penalized less and may get a larger absolute weight.
The interpretation of the LASSO weights corresponds to the interpretation of the weights in the linear regression model.
You only need to pay attention to whether the features are standardized or not, because this affects the weights.
In this example, the features were standardized by the software, but the weights were automatically transformed back for us to match the original feature scales.

**Other Methods for Sparsity in Linear Models**

A wide spectrum of methods can be used to reduce the number of features in a linear model.

Methods that include a pre-processing step:

- Manually selected features:
You can always use expert knowledge to select or discard some features.
The big drawback is that it can't be automated and you might not be an expert.
- Use some measures to pre-select features:
An example is the correlation coefficient.
You only consider features that exceed a certain threshold of correlation between the feature and the target.
The disadvantage is that it only considers the features individually.
Some features might not show a correlation until the linear model has accounted for some other features.
Those ones you will miss with univariate selection methods.

Step-wise procedures:

- Forward selection:
Fit the linear model with one feature.
Do this with each feature.
Select the model that works best (e.g. determined by the highest R-squared).
Now again, for the remaining features, fit different versions of your model by adding each feature to your currently best model.
Select the one that performs best.
Continue until some criterion is reached, such as the maximum number of features in the model.
- Backward selection:
Similar to forward selection.
But instead of adding features, start with the model that contains all features and try out which feature you have to remove to get the highest performance increase.
Repeat this until some stopping criterion is reached.

I recommend using Lasso, because it can be automated, considers all features simultaneously, and can be controlled via $\lambda$.
It also works for the [logistic regression model](#logistic) for classification.




### Advantages

The modeling of the predictions as a **weighted sum** makes it transparent how predictions are produced.
And with LASSO we can ensure that the number of summands remains small.

Many people use linear regression models.
This means that in many places it's **accepted** for predictive modeling and doing inference.
There is a **high level of collective experience and expertise**, including teaching materials on linear regression models and   software implementations.
Linear regression can be found in R, Python, Java, Julia, Scala, Javascript, ...

Mathematically, it's straightforward to estimate the weights and you have a **guarantee to find optimal weights** (given all assumptions of the linear regression model are met by the data).

Together with the weights you get confidence intervals, tests, a very solid statistical theory. 
There are also many extensions of the linear regression model (see [chapter Linear Model 2.0 - GLMs, GAMs and more](#extend-lm)).


### Disadvantages

Linear models can only represent linear relationships, i.e. a weighted sum of the input features.
Each **nonlinearity or interaction has to be hand-crafted** and explicitly given to the model as an input feature.

Linear models are also often **not that good regarding predictive performance**, because the relationships that can be learned are so restricted and usually oversimplifies how complex reality is.

The interpretation of a weight **can be unintuitive** because it depends on all other features.
A feature with high positive correlation with the outcome y and another feature might get a negative weight in the linear model, because, given the other correlated feature, it is negatively correlated with y in the high-dimensional space.
Completely correlated features make it even impossible to find a unique solution for the linear equation.
An example:
You have a model to predict the value of a house and have features like number of rooms and area of the house.
House area and number of rooms are highly correlated: the bigger a house is, the more rooms it has. If you now take both features into a linear model, it might happen, that the area of the house is the better predictor and get's a large positive weight.
The number of rooms might end up getting a negative weight, because either, given that a house has the same size, increasing the number of rooms could make it less valuable or the linear equation becomes less stable, when the correlation is too strong.